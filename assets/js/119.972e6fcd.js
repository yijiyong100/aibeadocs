(window.webpackJsonp=window.webpackJsonp||[]).push([[119],{635:function(t,_,v){"use strict";v.r(_);var s=v(53),a=Object(s.a)({},(function(){var t=this,_=t.$createElement,v=t._self._c||_;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("div",{staticClass:"custom-block tip"},[v("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),v("p",[t._v("本文主要是介绍 神经网络-常见神经网络区别 。")])]),t._v(" "),v("p"),v("div",{staticClass:"table-of-contents"},[v("ul",[v("li",[v("a",{attrs:{href:"#三种常见的神经网络"}},[t._v("三种常见的神经网络")]),v("ul",[v("li",[v("a",{attrs:{href:"#_1、全连接神经网络-fnn"}},[t._v("1、全连接神经网络(FNN)")])]),v("li",[v("a",{attrs:{href:"#_2、卷积神经网络-cnn"}},[t._v("2、卷积神经网络（CNN）")])]),v("li",[v("a",{attrs:{href:"#_3、循环神经网络-rnn"}},[t._v("3、循环神经网络(RNN)")])]),v("li",[v("a",{attrs:{href:"#总结"}},[t._v("总结：")])])])]),v("li",[v("a",{attrs:{href:"#【-】"}},[t._v("【----------------------------】")])]),v("li",[v("a",{attrs:{href:"#卷积神经网络和深度神经网络的区别是什么"}},[t._v("卷积神经网络和深度神经网络的区别是什么？")])]),v("li",[v("a",{attrs:{href:"#一-神经网络的发展"}},[t._v("一.神经网络的发展")]),v("ul",[v("li",[v("a",{attrs:{href:"#_1-感知机"}},[t._v("1.感知机")])]),v("li",[v("a",{attrs:{href:"#_2-多层感知机"}},[t._v("2.多层感知机")])]),v("li",[v("a",{attrs:{href:"#_3-深度神经网络-dnn"}},[t._v("3.深度神经网络（DNN）")])]),v("li",[v("a",{attrs:{href:"#_4-卷积神经网络-cnn"}},[t._v("4.卷积神经网络(CNN)")])])])]),v("li",[v("a",{attrs:{href:"#二-dnn-深度神经网络"}},[t._v("二.DNN(深度神经网络)")])]),v("li",[v("a",{attrs:{href:"#三-cnn-卷积神经网络"}},[t._v("三.CNN(卷积神经网络)")]),v("ul",[v("li",[v("a",{attrs:{href:"#_1-什么是卷积神经网络"}},[t._v("1.什么是卷积神经网络")])]),v("li",[v("a",{attrs:{href:"#_2-卷积神经网络的基本概念"}},[t._v("2.卷积神经网络的基本概念")])]),v("li",[v("a",{attrs:{href:"#_3-卷积神经网络的构成"}},[t._v("3.卷积神经网络的构成")])]),v("li",[v("a",{attrs:{href:"#_4-卷积神经网络有2大特点"}},[t._v("4.卷积神经网络有2大特点")])]),v("li",[v("a",{attrs:{href:"#_5-卷积神经网络的擅长处理领域"}},[t._v("5.卷积神经网络的擅长处理领域")])]),v("li",[v("a",{attrs:{href:"#_6-卷积神经网络解决了什么问题"}},[t._v("6.卷积神经网络解决了什么问题？")])]),v("li",[v("a",{attrs:{href:"#_7-卷积神经网络的典型架构"}},[t._v("7.卷积神经网络的典型架构")])])])]),v("li",[v("a",{attrs:{href:"#参考文章"}},[t._v("参考文章")])])])]),v("p"),t._v(" "),v("h2",{attrs:{id:"三种常见的神经网络"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#三种常见的神经网络"}},[t._v("#")]),t._v(" 三种常见的神经网络")]),t._v(" "),v("p",[t._v("神经网络根据中间功能层的不同分为不同的神经网络。主要有以下三种：")]),t._v(" "),v("h3",{attrs:{id:"_1、全连接神经网络-fnn"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1、全连接神经网络-fnn"}},[t._v("#")]),t._v(" "),v("strong",[t._v("1、全连接神经网络(FNN)")])]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-1.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("特点：每一层是全连接层—即每一层的每个神经元与上一层所有神经元都有连接；")]),t._v(" "),v("p",[t._v("作用：")]),t._v(" "),v("ul",[v("li",[t._v("1、多个全连接层可以从不同角度提取特征；")]),t._v(" "),v("li",[t._v("2、全连接层作为输出层有分类和数值预测的功能；也经常用于卷积神经网络。")])]),t._v(" "),v("p",[t._v("缺点：权重多，计算量大。")]),t._v(" "),v("p",[t._v("场景：所有的神经网络均可以利用")]),t._v(" "),v("h3",{attrs:{id:"_2、卷积神经网络-cnn"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2、卷积神经网络-cnn"}},[t._v("#")]),t._v(" "),v("strong",[t._v("2、卷积神经网络（CNN）")])]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-2.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("特点：")]),t._v(" "),v("ul",[v("li",[t._v("卷积层：相当于滤镜，将图片进行分块，对每一块进行特征处理，从而提取特征。")]),t._v(" "),v("li",[t._v("池化层：通过对提取的高维特征进行降维。")]),t._v(" "),v("li",[t._v("全连接层：对空间排列的特征化成一维的向量。")])]),t._v(" "),v("p",[t._v("场景：人脸识别、图片识别")]),t._v(" "),v("h3",{attrs:{id:"_3、循环神经网络-rnn"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3、循环神经网络-rnn"}},[t._v("#")]),t._v(" "),v("strong",[t._v("3、循环神经网络(RNN)")])]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-3.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("特点：")]),t._v(" "),v("ul",[v("li",[t._v("1、中间层的输出作为输入和下一个样本数据一起作为输入，也叫循环层")]),t._v(" "),v("li",[t._v("2、具有记忆样本之间相关联系的能力")])]),t._v(" "),v("p",[t._v("场景：常用于文本填充、时间序列、语音识别等序列数据")]),t._v(" "),v("h3",{attrs:{id:"总结"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结：")]),t._v(" "),v("p",[t._v("每一种神经网络各有优点，在具体的场景中，根据不用的应用选择不同的网络，也可能会同时用到三种网络搭建更复杂的网络。")]),t._v(" "),v("h2",{attrs:{id:"【-】"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#【-】"}},[t._v("#")]),t._v(" 【----------------------------】")]),t._v(" "),v("h2",{attrs:{id:"卷积神经网络和深度神经网络的区别是什么"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#卷积神经网络和深度神经网络的区别是什么"}},[t._v("#")]),t._v(" 卷积神经网络和深度神经网络的区别是什么？")]),t._v(" "),v("p",[t._v("作者：飘哥\n链接：https://www.zhihu.com/question/29366638/answer/864113705")]),t._v(" "),v("p",[v("strong",[t._v("DNN是指深度神经网络，它是一个很广的概念，某种意义上CNN、RNN、GAN等都属于其范畴之内")]),t._v("。DNN**与CNN（卷积神经网络）的区别是DNN特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。**DNN是指包含多个隐层的神经网络，根据神经元的特点，可以分为MLP、CNNs、RNNs等，从神经元的角度来讲解，MLP是最朴素的DNN，CNNs是encode了空间相关性的DNN，RNNs是encode进了时间相关性的DNN。")]),t._v(" "),v("h2",{attrs:{id:"一-神经网络的发展"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#一-神经网络的发展"}},[t._v("#")]),t._v(" 一.神经网络的发展")]),t._v(" "),v("h3",{attrs:{id:"_1-感知机"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-感知机"}},[t._v("#")]),t._v(" "),v("strong",[t._v("1.感知机")])]),t._v(" "),v("p",[t._v("神经网络技术起源于上世纪五、六十年代，当时叫感知机（perceptron），拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。早期感知机的推动者是Rosenblatt。但是，Rosenblatt的单层感知机有一个严重得不能再严重的问题，对于计算稍微复杂的函数其计算力显得无能为力。")]),t._v(" "),v("h3",{attrs:{id:"_2-多层感知机"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-多层感知机"}},[t._v("#")]),t._v(" "),v("strong",[t._v("2.多层感知机")])]),t._v(" "),v("p",[t._v("随着数学的发展，这个缺点直到上世纪八十年代才被Rumelhart、Williams、Hinton、LeCun等人发明的多层感知机（multilayer perceptron)克服。多层感知机，顾名思义，就是有多个隐含层的感知机。")]),t._v(" "),v("p",[t._v("多层感知机可以摆脱早期离散传输函数的束缚，使用sigmoid或tanh等连续函数模拟神经元对激励的响应，在训练算法上则使用Werbos发明的反向传播BP算法。对，这就是我们现在所说的神经网络( NN)！多层感知机解决了之前无法模拟异或逻辑的缺陷，同时更多的层数也让网络更能够刻画现实世界中的复杂情形。多层感知机给我们带来的启示是，神经网络的层数直接决定了它对现实的刻画能力——利用每层更少的神经元拟合更加复杂的函数。")]),t._v(" "),v("p",[t._v("即便大牛们早就预料到神经网络需要变得更深，但是有一个梦魇总是萦绕左右。随着神经网络层数的加深，优化函数越来越容易陷入局部最优解，并且这个“陷阱”越来越偏离真正的全局最优。利用有限数据训练的深层网络，性能还不如较浅层网络。同时，另一个不可忽略的问题是随着网络层数增加，“梯度消失”现象更加严重。具体来说，我们常常使用 sigmoid 作为神经元的输入输出函数。对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。")]),t._v(" "),v("h3",{attrs:{id:"_3-深度神经网络-dnn"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-深度神经网络-dnn"}},[t._v("#")]),t._v(" "),v("strong",[t._v("3.深度神经网络（DNN）")])]),t._v(" "),v("p",[t._v("2006年，Hinton利用预训练方法缓解了局部最优解问题，将隐含层推动到了7层(参考论文：Hinton G E, Salakhutdinov R R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313(5786):504-507.)，神经网络真正意义上有了“深度”，由此揭开了深度学习的热潮。这里的“深度”并没有固定的定义——在语音识别中4层网络就能够被认为是“较深的”，而在图像识别中20层以上的网络屡见不鲜。为了克服梯度消失，ReLU、maxout等传输函数代替了 sigmoid，形成了如今 DNN 的基本形式。单从结构上来说，全连接的DNN和上图的多层感知机是没有任何区别的。值得一提的是，今年出现的高速公路网络（highway network）和深度残差学习（deep residual learning）进一步避免了梯度弥散问题，网络层数达到了前所未有的一百多层（深度残差学习：152层）")]),t._v(" "),v("h3",{attrs:{id:"_4-卷积神经网络-cnn"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-卷积神经网络-cnn"}},[t._v("#")]),t._v(" "),v("strong",[t._v("4.卷积神经网络(CNN)")])]),t._v(" "),v("p",[t._v("我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。此时我们可以祭出题主所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是通过“"),v("strong",[t._v("卷积核")]),t._v("”作为中介。同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。")]),t._v(" "),v("p",[t._v("对于图像，"),v("strong",[t._v("如果没有卷积操作，学习的参数量是灾难级的")]),t._v("。CNN之所以用于图像识别，正是由于CNN模型限制了参数的个数并挖掘了局部结构的这个特点。顺着同样的思路，利用语音语谱结构中的局部信息，CNN照样能应用在语音识别中。在普通的全连接网络或CNN中，每层神经元的信号只能向上一层传播，样本的处理在各个时刻独立，因此又被称为"),v("strong",[t._v("前向神经网络")]),t._v("(Feed-forward Neural Networks)。")]),t._v(" "),v("p",[v("strong",[t._v("神经网络计算方法与传统方法的区别？")])]),t._v(" "),v("p",[t._v("神经网络具备:")]),t._v(" "),v("ul",[v("li",[t._v("并行")]),t._v(" "),v("li",[t._v("容错")]),t._v(" "),v("li",[t._v("硬件实现")]),t._v(" "),v("li",[t._v("自我学习\n以上是神经网络计算方法与传统方法的区别所在。")])]),t._v(" "),v("p",[v("strong",[t._v("多层神经网络包含哪几层？")])]),t._v(" "),v("p",[t._v("传统意义上的多层神经网络包含 "),v("strong",[t._v("三层")]),t._v("：")]),t._v(" "),v("ul",[v("li",[t._v("输入层")]),t._v(" "),v("li",[t._v("隐藏层")]),t._v(" "),v("li",[t._v("输出层\n其中 "),v("strong",[t._v("隐藏层的层数根据需要而定")]),t._v("，没有明确的理论推导来说明到底多少层合适，多层神经网络做的步骤是：特征映射到值，特征是人工挑选。")])]),t._v(" "),v("h2",{attrs:{id:"二-dnn-深度神经网络"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#二-dnn-深度神经网络"}},[t._v("#")]),t._v(" 二.DNN(深度神经网络)")]),t._v(" "),v("p",[t._v("传统的人工神经网络（ANN）由三部分组成：输入层，隐藏层，输出层，这三部分各占一层。而深度神经网络的“"),v("strong",[t._v("深度")]),t._v("”二字表示它的隐藏层"),v("strong",[t._v("大于2层")]),t._v("，这使它有了更深的"),v("strong",[t._v("抽象")]),t._v("和"),v("strong",[t._v("降维能力")]),t._v("。")]),t._v(" "),v("h2",{attrs:{id:"三-cnn-卷积神经网络"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#三-cnn-卷积神经网络"}},[t._v("#")]),t._v(" 三.CNN(卷积神经网络)")]),t._v(" "),v("p",[t._v("对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被大量应用于计算机视觉、自然语言处理等领域 。")]),t._v(" "),v("h3",{attrs:{id:"_1-什么是卷积神经网络"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_1-什么是卷积神经网络"}},[t._v("#")]),t._v(" "),v("strong",[t._v("1.什么是卷积神经网络")])]),t._v(" "),v("p",[t._v("卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的 "),v("strong",[t._v("前馈神经网络")]),t._v("（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一 。由于卷积神经网络能够进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）” 。\n《百度百科》")]),t._v(" "),v("h3",{attrs:{id:"_2-卷积神经网络的基本概念"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_2-卷积神经网络的基本概念"}},[t._v("#")]),t._v(" "),v("strong",[t._v("2.卷积神经网络的基本概念")])]),t._v(" "),v("ul",[v("li",[t._v("局部感受野（local receptive fields）")]),t._v(" "),v("li",[t._v("共享权重（shared weights）")]),t._v(" "),v("li",[t._v("池化（pooling）")])]),t._v(" "),v("h3",{attrs:{id:"_3-卷积神经网络的构成"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_3-卷积神经网络的构成"}},[t._v("#")]),t._v(" "),v("strong",[t._v("3.卷积神经网络的构成")])]),t._v(" "),v("p",[t._v("典型的卷积神经网络由3部分构成：")]),t._v(" "),v("ul",[v("li",[t._v("卷积层")]),t._v(" "),v("li",[t._v("池化层")]),t._v(" "),v("li",[t._v("全连接层")])]),t._v(" "),v("h4",{attrs:{id:"卷积层"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#卷积层"}},[t._v("#")]),t._v(" "),v("strong",[t._v("卷积层")])]),t._v(" "),v("p",[t._v("负责提取图像中的"),v("strong",[t._v("局部特征")]),t._v("；"),v("strong",[t._v("池化层")]),t._v("用来大幅降低参数量级("),v("strong",[t._v("降维")]),t._v(")；"),v("strong",[t._v("全连接层")]),t._v("类似传统神经网络的部分，用来"),v("strong",[t._v("输出想要的结果")]),t._v("。")]),t._v(" "),v("h4",{attrs:{id:"a-卷积-提取特征"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#a-卷积-提取特征"}},[t._v("#")]),t._v(" "),v("strong",[t._v("A.卷积——提取特征")])]),t._v(" "),v("p",[t._v("卷积层的运算过程如下图，用一个卷积核扫完整张图片：")]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-4.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("这个过程我们可以理解为我们使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。")]),t._v(" "),v("p",[t._v("在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：")]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-5.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("总结：卷积层的通过卷积核的过滤提取出图片中"),v("strong",[t._v("局部")]),t._v("的特征，跟上面提到的人类视觉的特征提取类似。")]),t._v(" "),v("h4",{attrs:{id:"b-池化层"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#b-池化层"}},[t._v("#")]),t._v(" "),v("strong",[t._v("B.池化层")])]),t._v(" "),v("p",[t._v("（下采样）——数据降维，避免过拟合")]),t._v(" "),v("p",[t._v("池化层简单说就是下采样，他可以大大降低数据的维度。其过程如下：")]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-6.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("上图中，我们可以看到，原始图片是20×20的，我们对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。")]),t._v(" "),v("p",[t._v("之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。")]),t._v(" "),v("p",[v("strong",[t._v("总结：池化层相比卷积层可以更有效的降低数据维度，这么做不但可以大大减少运算量，还可以有效的避免过拟合。")])]),t._v(" "),v("h4",{attrs:{id:"c-全连接层-输出结果"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#c-全连接层-输出结果"}},[t._v("#")]),t._v(" "),v("strong",[t._v("C.全连接层——输出结果")])]),t._v(" "),v("p",[t._v("这个部分就是最后一步了，经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。")]),t._v(" "),v("p",[t._v("经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。")]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-7.png"),alt:"wxmp"}}),t._v(" "),v("p",[t._v("典型的 CNN 并非只是上面提到的3层结构，而是多层结构，例如 LeNet-5 的结构就如下图所示：")]),t._v(" "),v("p",[v("strong",[t._v("卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层")])]),t._v(" "),v("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/ai/nnintro/diff-8.png"),alt:"wxmp"}}),t._v(" "),v("h3",{attrs:{id:"_4-卷积神经网络有2大特点"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_4-卷积神经网络有2大特点"}},[t._v("#")]),t._v(" "),v("strong",[t._v("4.卷积神经网络有2大特点")])]),t._v(" "),v("ul",[v("li",[t._v("能够有效的将大数据量的图片降维成小数据量")]),t._v(" "),v("li",[t._v("能够有效的保留图片特征，符合图片处理的原则")])]),t._v(" "),v("h3",{attrs:{id:"_5-卷积神经网络的擅长处理领域"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_5-卷积神经网络的擅长处理领域"}},[t._v("#")]),t._v(" "),v("strong",[t._v("5.卷积神经网络的擅长处理领域")])]),t._v(" "),v("p",[t._v("卷积神经网络 – 卷积神经网络最擅长的就是图片的处理")]),t._v(" "),v("h3",{attrs:{id:"_6-卷积神经网络解决了什么问题"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_6-卷积神经网络解决了什么问题"}},[t._v("#")]),t._v(" "),v("strong",[t._v("6.卷积神经网络解决了什么问题？")])]),t._v(" "),v("p",[t._v("在卷积神经网络出现之前，图像对于人工智能来说是一个难题，有2个原因：")]),t._v(" "),v("ul",[v("li",[t._v("图像需要处理的数据量太大，导致成本很高，效率很低")]),t._v(" "),v("li",[t._v("图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高")])]),t._v(" "),v("p",[v("strong",[t._v("A.需要处理的数据量太大")])]),t._v(" "),v("p",[t._v("图像是由像素构成的，每个像素又是由颜色构成的。现在随随便便一张图片都是 1000×1000 像素以上的， 每个像素都有RGB 3个参数来表示颜色信息。假如我们处理一张 1000×1000 像素的图片，我们就需要处理3百万个参数！")]),t._v(" "),v("blockquote",[v("p",[v("strong",[t._v("1000×1000×3=3,000,000")])])]),t._v(" "),v("p",[t._v("这么大量的数据处理起来是非常消耗资源的，而且这只是一张不算太大的图片！")]),t._v(" "),v("p",[v("strong",[t._v("卷积神经网络 – CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。")])]),t._v(" "),v("p",[v("strong",[t._v("更重要的是：我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。")])]),t._v(" "),v("p",[v("strong",[t._v("B.保留图像特征")])]),t._v(" "),v("p",[t._v("假如一张图像中有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，"),v("strong",[t._v("图像的内容（本质）并没有发生变化，只是位置发生了变化")]),t._v("。")]),t._v(" "),v("p",[t._v("所以当我们移动图像中的物体，用传统的方式的得出来的参数会差异很大！这是不符合图像处理的要求的。")]),t._v(" "),v("p",[v("strong",[t._v("而 CNN 解决了这个问题，他用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。")])]),t._v(" "),v("h3",{attrs:{id:"_7-卷积神经网络的典型架构"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#_7-卷积神经网络的典型架构"}},[t._v("#")]),t._v(" "),v("strong",[t._v("7.卷积神经网络的典型架构")])]),t._v(" "),v("ul",[v("li",[t._v("LeNet-5")]),t._v(" "),v("li",[t._v("AlexNet")]),t._v(" "),v("li",[t._v("VGG-16")]),t._v(" "),v("li",[t._v("Inception-V1")]),t._v(" "),v("li",[t._v("Inception-V3")]),t._v(" "),v("li",[t._v("RESNET-50")]),t._v(" "),v("li",[t._v("Xception")]),t._v(" "),v("li",[t._v("Inception-V4")]),t._v(" "),v("li",[t._v("Inception-ResNets")]),t._v(" "),v("li",[t._v("ResNeXt-50到")])]),t._v(" "),v("h2",{attrs:{id:"参考文章"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),v("ul",[v("li",[t._v("https://blog.csdn.net/dyna_lidan/article/details/82462145")]),t._v(" "),v("li",[t._v("https://blog.csdn.net/weixin_38278993/article/details/103290407")])])])}),[],!1,null,null,null);_.default=a.exports}}]);