(window.webpackJsonp=window.webpackJsonp||[]).push([[265],{780:function(a,s,t){"use strict";t.r(s);var n=t(53),e=Object(n.a)({},(function(){var a=this,s=a.$createElement,t=a._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"custom-block-title"},[a._v("TIP")]),a._v(" "),t("p",[a._v("本文主要是介绍 Spark-实时购物分析案例 。")])]),a._v(" "),t("p"),t("div",{staticClass:"table-of-contents"},[t("ul",[t("li",[t("a",{attrs:{href:"#大数据spark-kafka实时数据分析案例"}},[a._v("大数据Spark+Kafka实时数据分析案例")])]),t("li",[t("a",{attrs:{href:"#一、实验环境准备"}},[a._v("一、实验环境准备")]),t("ul",[t("li",[t("a",{attrs:{href:"#实验系统和软件要求"}},[a._v("实验系统和软件要求")])]),t("li",[t("a",{attrs:{href:"#系统和软件的安装"}},[a._v("系统和软件的安装")])]),t("li",[t("a",{attrs:{href:"#python安装"}},[a._v("Python安装")])]),t("li",[t("a",{attrs:{href:"#python依赖库"}},[a._v("Python依赖库")])])])]),t("li",[t("a",{attrs:{href:"#二、数据处理和python操作kafka"}},[a._v("二、数据处理和Python操作Kafka")]),t("ul",[t("li",[t("a",{attrs:{href:"#数据预处理"}},[a._v("数据预处理")])]),t("li",[t("a",{attrs:{href:"#python操作kafka"}},[a._v("Python操作Kafka")])])])]),t("li",[t("a",{attrs:{href:"#三、spark-streaming实时处理数据"}},[a._v("三、Spark Streaming实时处理数据")]),t("ul",[t("li",[t("a",{attrs:{href:"#spark准备工作"}},[a._v("Spark准备工作")])]),t("li",[t("a",{attrs:{href:"#建立spark项目"}},[a._v("建立Spark项目")])]),t("li",[t("a",{attrs:{href:"#运行项目"}},[a._v("运行项目")])]),t("li",[t("a",{attrs:{href:"#测试程序"}},[a._v("测试程序")])])])]),t("li",[t("a",{attrs:{href:"#四、结果展示"}},[a._v("四、结果展示")]),t("ul",[t("li",[t("a",{attrs:{href:"#flask-socketio实时推送数据"}},[a._v("Flask-SocketIO实时推送数据")])])])]),t("li",[t("a",{attrs:{href:"#参考文章"}},[a._v("参考文章")])])])]),t("p"),a._v(" "),t("h2",{attrs:{id:"大数据spark-kafka实时数据分析案例"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大数据spark-kafka实时数据分析案例"}},[a._v("#")]),a._v(" 大数据Spark+Kafka实时数据分析案例")]),a._v(" "),t("p",[a._v("本案例利用Spark+Kafka实时分析男女生每秒购物人数，利用Spark Streaming实时处理用户购物日志，然后利用websocket将数据实时推送给浏览器，最后浏览器将接收到的数据实时展现，案例的整体框架图如下：")]),a._v(" "),t("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/da/rtaspark/userbuycase-1.png"),alt:"wxmp"}}),a._v(" "),t("p",[a._v("下面分析详细分析下上述步骤：")]),a._v(" "),t("ul",[t("li",[t("ol",[t("li",[a._v("应用程序将购物日志发送给Kafka，topic为”sex”，因为这里只是统计购物男女生人数，所以只需要发送购物日志中性别属性即可。这里采用模拟的方式发送购物日志，即读取购物日志数据，每间隔相同的时间发送给Kafka。")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"2"}},[t("li",[a._v("接着利用Spark Streaming从Kafka主题”sex”读取并处理消息。这里按滑动窗口的大小按顺序读取数据，例如可以按每5秒作为窗口大小读取一次数据，然后再处理数据。")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"3"}},[t("li",[a._v("Spark将处理后的数据发送给Kafka，topic为”result”。")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"4"}},[t("li",[a._v("然后利用Flask搭建一个web应用程序，接收Kafka主题为”result”的消息。")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"5"}},[t("li",[a._v("利用Flask-SocketIO将数据实时推送给客户端。")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"6"}},[t("li",[a._v("客户端浏览器利用js框架socketio实时接收数据，然后利用js可视化库hightlights.js库动态展示。")])])])]),a._v(" "),t("p",[a._v("至此，本案例的整体架构已介绍完毕。")]),a._v(" "),t("h2",{attrs:{id:"一、实验环境准备"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#一、实验环境准备"}},[a._v("#")]),a._v(" 一、实验环境准备")]),a._v(" "),t("h3",{attrs:{id:"实验系统和软件要求"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#实验系统和软件要求"}},[a._v("#")]),a._v(" 实验系统和软件要求")]),a._v(" "),t("blockquote",[t("p",[a._v("Ubuntu: 16.04\nSpark: 2.1.0\nScala: 2.11.8\nkafka: 0.8.2.2\nPython: 3.x(3.0以上版本)\nFlask: 0.12.1\nFlask-SocketIO: 2.8.6\nkafka-python： 1.3.3")])]),a._v(" "),t("h3",{attrs:{id:"系统和软件的安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#系统和软件的安装"}},[a._v("#")]),a._v(" 系统和软件的安装")]),a._v(" "),t("h4",{attrs:{id:"spark安装-前续文档已经安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark安装-前续文档已经安装"}},[a._v("#")]),a._v(" Spark安装("),t("a",{attrs:{href:"http://www.cnblogs.com/freebird92/p/8886535.html",target:"_blank",rel:"noopener noreferrer"}},[a._v("前续文档已经安装"),t("OutboundLink")],1),a._v(")")]),a._v(" "),t("h4",{attrs:{id:"kafka安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#kafka安装"}},[a._v("#")]),a._v(" Kafka安装")]),a._v(" "),t("p",[a._v("Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。下面介绍有关Kafka的简单安装和使用, 简单介绍参考"),t("a",{attrs:{href:"http://www.cnblogs.com/freebird92/p/8933670.html",target:"_blank",rel:"noopener noreferrer"}},[a._v("KAFKA简介"),t("OutboundLink")],1),a._v(", 想全面了解Kafka,请访问"),t("a",{attrs:{href:"http://kafka.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[a._v("Kafka"),t("OutboundLink")],1),a._v("的官方博客。")]),a._v(" "),t("p",[a._v("我选择的是"),t("a",{attrs:{href:"http://archive.apache.org/dist/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz",target:"_blank",rel:"noopener noreferrer"}},[a._v("kafka_2.11-0.10.1.0.tgz"),t("OutboundLink")],1),a._v("(注意，此处版本号，在后面spark使用时是有要求的,见"),t("a",{attrs:{href:"http://spark.apache.org/docs/latest/streaming-kafka-integration.html",target:"_blank",rel:"noopener noreferrer"}},[a._v("集成指南"),t("OutboundLink")],1),a._v(")版本。")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("sudo")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("tar")]),a._v(" -zxf kafka_2.11-0.10.1.0.tgz -C /usr/local\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("sudo")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("mv")]),a._v("  kafka_2.11-0.10.1.0/ ./kafka\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("sudo")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("chown")]),a._v(" -R hadoop ./kafka\n")])])]),t("p",[a._v("接下来在Ubuntu系统环境下测试简单的实例。Mac系统请自己按照安装的位置，切换到相应的指令。按顺序执行如下命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/kafka   "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("## 进入kafka所在的目录")]),a._v("\nbin/zookeeper-server-start.sh config/zookeeper.properties\n")])])]),t("p",[a._v("命令执行后不会返回Shell命令输入状态,zookeeper就会按照默认的配置文件启动服务,请千万不要关闭当前终端.启动新的终端，输入如下命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/kafka\nbin/kafka-server-start.sh config/server.properties\n")])])]),t("p",[a._v("kafka服务端就启动了,请千万不要关闭当前终端。启动另外一个终端,输入如下命令:")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/kafka\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" --partitions "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" --topic dblab\n")])])]),t("p",[a._v("topic是发布消息发布的category,以单节点的配置创建了一个叫dblab的topic.可以用list列出所有创建的topics,来查看刚才创建的主题是否存在。")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("bin/kafka-topics.sh --list --zookeeper localhost:2181  \n")])])]),t("p",[a._v("可以在结果中查看到dblab这个topic存在。接下来用producer生产点数据：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("bin/kafka-console-producer.sh --broker-list localhost:9092 --topic dblab\n")])])]),t("p",[a._v("并尝试输入如下信息：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("hello hadoop\nhello xmu\nhadoop world\n")])])]),t("p",[a._v("然后再次开启新的终端或者直接按CTRL+C退出。然后使用consumer来接收数据,输入如下命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/kafka\nbin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic dblab --from-beginning  \n")])])]),t("p",[a._v("便可以看到刚才产生的三条信息。说明kafka安装成功。")]),a._v(" "),t("h3",{attrs:{id:"python安装"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#python安装"}},[a._v("#")]),a._v(" Python安装")]),a._v(" "),t("p",[a._v("Ubuntu16.04系统自带Python2.7和Python3.5，本案例直接使用Ubuntu16.04自带Python3.5；")]),a._v(" "),t("h3",{attrs:{id:"python依赖库"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#python依赖库"}},[a._v("#")]),a._v(" Python依赖库")]),a._v(" "),t("p",[a._v("案例主要使用了两个Python库，Flask和Flask-SocketIO，这两个库的安装非常简单，请启动进入Ubuntu系统，打开一个命令行终端。")]),a._v(" "),t("p",[a._v("Python之所以强大，其中一个原因是其丰富的第三方库。pip则是python第三方库的包管理工具。Python3对应的包管理工具是pip3。因此，需要首先在Ubuntu系统中安装pip3，命令如下：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("sudo")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("apt-get")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("install")]),a._v(" python3-pip\n")])])]),t("p",[a._v("安装完pip3以后，可以使用如下Shell命令完成Flask和Flask-SocketIO这两个Python第三方库的安装以及与Kafka相关的Python库的安装：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("pip3 "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("install")]),a._v(" flask\npip3 "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("install")]),a._v(" flask-socketio\npip3 "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("install")]),a._v(" kafka-python\n")])])]),t("p",[a._v("这些安装好的库在我们的程序文件的开头可以直接用来引用。比如下面的例子。")]),a._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[a._v("from flask "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Flask")]),a._v("\nfrom flask_socketio "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("SocketIO")]),a._v("\nfrom kafka "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaConsumer")]),a._v("\n")])])]),t("ul",[t("li",[a._v("from import 跟直接import的区别举个例子来说明。")]),a._v(" "),t("li",[a._v("import socket的话,要用socket.AF_INET,因为AF_INET这个值在socket的名称空间下。")]),a._v(" "),t("li",[a._v("from socket import* 是把socket下的所有名字引入当前名称空间。")])]),a._v(" "),t("h2",{attrs:{id:"二、数据处理和python操作kafka"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#二、数据处理和python操作kafka"}},[a._v("#")]),a._v(" 二、数据处理和Python操作Kafka")]),a._v(" "),t("p",[a._v("本案例采用的数据集压缩包为data_format.zip点击这里"),t("a",{attrs:{href:"https://pan.baidu.com/s/1cs02Nc",target:"_blank",rel:"noopener noreferrer"}},[a._v("下载data_format.zip"),t("OutboundLink")],1),a._v("数据集，该数据集压缩包是淘宝2015年双11前6个月(包含双11)的交易数据(交易数据有偏移，但是不影响实验的结果)，里面包含3个文件，分别是用户行为日志文件user_log.csv 、回头客训练集train.csv 、回头客测试集test.csv. 在这个案例中只是用user_log.csv这个文件，下面列出文件user_log.csv的数据格式定义：\n用户行为日志user_log.csv，日志中的字段定义如下：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(". user_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 买家id\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v(". item_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 商品id\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("3")]),a._v(". cat_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 商品类别id\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("4")]),a._v(". merchant_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 卖家id\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("5")]),a._v(". brand_id "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 品牌id\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("6")]),a._v(". month "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 交易时间:月\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("7")]),a._v(". day "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 交易事件:日\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("8")]),a._v(". action "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 行为,取值范围"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("0,1")]),a._v(",2,3"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v(",0表示点击，1表示加入购物车，2表示购买，3表示关注商品\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("9")]),a._v(". age_range "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 买家年龄分段：1表示年龄"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("18,2")]),a._v("表示年龄在"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("18,24")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("，3表示年龄在"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("25,29")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("，4表示年龄在"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("30,34")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("，5表示年龄在"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("35,39")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("，6表示年龄在"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("40,49")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("，7和8表示年龄"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("50,0")]),a._v("和NULL则表示未知\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("10")]),a._v(". gender "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 性别:0表示女性，1表示男性，2和NULL表示未知\n- "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("11")]),a._v(". province"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("|")]),a._v(" 收获地址省份\n")])])]),t("p",[a._v("数据具体格式如下：")]),a._v(" "),t("blockquote",[t("p",[a._v("user_id,item_id,cat_id,merchant_id,brand_id,month,day,action,age_range,gender,province\n328862,323294,833,2882,2661,08,29,0,0,1,内蒙古\n328862,844400,1271,2882,2661,08,29,0,1,1,山西\n328862,575153,1271,2882,2661,08,29,0,2,1,山西\n328862,996875,1271,2882,2661,08,29,0,1,1,内蒙古\n328862,1086186,1271,1253,1049,08,29,0,0,2,浙江")])]),a._v(" "),t("p",[a._v("这个案例实时统计每秒中男女生购物人数，因此针对每条购物日志，我们只需要获取gender即可，然后发送给Kafka，接下来Spark Streaming再接收gender进行处理。")]),a._v(" "),t("h3",{attrs:{id:"数据预处理"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据预处理"}},[a._v("#")]),a._v(" 数据预处理")]),a._v(" "),t("p",[a._v("接着可以写如下Python代码，文件名为producer.py：(具体的工程文件结构参照步骤一)")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("mkdir")]),a._v(" -p ~/kafka-exp/scripts\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" ~/kafka-exp/scripts\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("vim")]),a._v(" producer.py\n")])])]),t("p",[a._v("添加如入内容：")]),a._v(" "),t("div",{staticClass:"language-ini extra-class"},[t("pre",{pre:!0,attrs:{class:"language-ini"}},[t("code",[t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("## coding: utf-8")]),a._v("\nimport csv\nimport time\nfrom kafka import KafkaProducer\n \n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("## 实例化一个KafkaProducer示例，用于向Kafka投递消息")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token constant"}},[a._v("producer")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(" KafkaProducer(bootstrap_servers='localhost:9092')")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("## 打开数据文件")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token constant"}},[a._v("csvfile")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' open("../data/user_log.csv","r")')]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("## 生成一个可用于读取csv文件的reader")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token constant"}},[a._v("reader")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(" csv.reader(csvfile)")]),a._v("\n \nfor line in reader:\n"),t("span",{pre:!0,attrs:{class:"token constant"}},[a._v("    gender")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(" line[9] ## 性别在每行日志代码的第9个元素")]),a._v("\n    if gender "),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(" 'gender':")]),a._v("\n        continue ## 去除第一行表头\n    time.sleep(0.1) ## 每隔0.1秒发送一行数据\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("    ## 发送数据，topic为'sex'")]),a._v("\n    producer.send('sex',line[9].encode('utf8'))\n")])])]),t("p",[a._v("上述代码很简单，首先是先实例化一个Kafka生产者。然后读取用户日志文件，每次读取一行，接着每隔0.1秒发送给Kafka，这样1秒发送10条购物日志。这里发送给Kafka的topic为’sex’。")]),a._v(" "),t("h3",{attrs:{id:"python操作kafka"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#python操作kafka"}},[a._v("#")]),a._v(" Python操作Kafka")]),a._v(" "),t("p",[a._v("我们可以写一个KafkaConsumer测试数据是否投递成功，代码如下，文件名为consumer.py")]),a._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("from")]),a._v(" kafka "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" KafkaConsumer\n \nconsumer "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" KafkaConsumer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'sex'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" msg "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" consumer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(":")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("msg"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("decode"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'utf8'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),t("p",[a._v("在开启上述KafkaProducer和KafkaConsumer之前，需要先开启Kafka。然后再开两个终端，分别用作发布消息与消费消息，执行命令如下：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" ~/kafka-exp/scripts\npython3 producer.py "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#启动生产者发送消息给Kafaka")]),a._v("\n")])])]),t("p",[a._v("打开另外一个命令行 终端窗口，消费消息，执行如下命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" ~/kafka-exp/scripts  \npython3 consumer.py "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#启动消费者从Kafaka接收消息")]),a._v("\n")])])]),t("p",[a._v("运行上面这条命令以后，这时，你会看到屏幕上会输出一行又一行的数字，类似下面的样子：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v(".\n")])])]),t("h2",{attrs:{id:"三、spark-streaming实时处理数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#三、spark-streaming实时处理数据"}},[a._v("#")]),a._v(" 三、Spark Streaming实时处理数据")]),a._v(" "),t("p",[a._v("本案例在于实时统计每秒中男女生购物人数，而Spark Streaming接收的数据为1,1,0,2…，其中0代表女性，1代表男性，所以对于2或者null值，则不考虑。其实通过分析，可以发现这个就是典型的wordcount问题，而且是基于Spark流计算。女生的数量，即为0的个数，男生的数量，即为1的个数。")]),a._v(" "),t("p",[a._v("因此利用Spark Streaming接口reduceByKeyAndWindow，设置窗口大小为1，滑动步长为1，这样统计出的0和1的个数即为每秒男生女生的人数。")]),a._v(" "),t("h3",{attrs:{id:"spark准备工作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark准备工作"}},[a._v("#")]),a._v(" Spark准备工作")]),a._v(" "),t("p",[a._v("Kafka和Flume等高级输入源，需要依赖独立的库（jar文件）。按照我们前面安装好的Spark版本，这些jar包都不在里面，为了证明这一点，我们现在可以测试一下。请打开一个新的终端，然后启动spark-shell：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/spark/spark-2.3.0-bin-hadoop2.7\n./bin/spark-shell\n")])])]),t("p",[a._v("启动成功后，在spark-shell中执行下面import语句：")]),a._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[a._v("scala"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka010"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),a._v("_\n"),t("span",{pre:!0,attrs:{class:"token generics"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("<")]),a._v("console"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(">")])]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("25")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),a._v(" error"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),a._v(" object kafka is not a member of "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("package")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming")]),a._v("\n         "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka010"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),a._v("_\n                                           "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("^")]),a._v("\n")])])]),t("p",[a._v("你可以看到，马上会报错，因为找不到相关的jar包。然后我们退出spark-shell。\n根据"),t("a",{attrs:{href:"http://spark.apache.org/docs/latest/streaming-programming-guide.html",target:"_blank",rel:"noopener noreferrer"}},[a._v("Spark官网"),t("OutboundLink")],1),a._v("的说明，对于Spark2.3.0版本，如果要使用Kafka，则需要下载"),t("a",{attrs:{href:"http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.3.0/spark-streaming-kafka-0-10_2.11-2.3.0.jar",target:"_blank",rel:"noopener noreferrer"}},[a._v("spark-streaming-kafka-0-10_2.11"),t("OutboundLink")],1),a._v("相关jar包。\n现在请在Linux系统中，打开一个火狐浏览器，请点击这里访问Spark官网，里面有提供"),t("a",{attrs:{href:"http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.3.0/spark-streaming-kafka-0-10_2.11-2.3.0.jar",target:"_blank",rel:"noopener noreferrer"}},[a._v("spark-streaming-kafka-0-10_2.11-2.3.0.jar"),t("OutboundLink")],1),a._v("文件的下载，其中，2.11表示scala的版本，2.3.0表示Spark版本号。下载后的文件会被默认保存在当前Linux登录用户的下载目录下，本教程统一使用hadoop用户名登录Linux系统，所以，我们就把这个文件复制到Spark目录的jars目录下。请新打开一个终端，输入下面命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("mkdir")]),a._v(" /usr/local/spark/spark-2.3.0-bin-hadoop2.7/jars/kafka\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("cp")]),a._v(" ./spark-streaming-kafka-0-10_2.11-2.3.0.jar /usr/local/spark/spark-2.3.0-bin-hadoop2.7/jars/kafka\n")])])]),t("p",[a._v("下面还要继续把Kafka安装目录的libs目录下的所有jar文件复制到“/usr/local/spark/jars/kafka”目录下，请在终端中执行下面命令：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/kafka/libs\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("ls")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("cp")]),a._v(" ./* /usr/local/spark/spark-2.3.0-bin-hadoop2.7/jars/kafka\n")])])]),t("h3",{attrs:{id:"建立spark项目"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#建立spark项目"}},[a._v("#")]),a._v(" 建立Spark项目")]),a._v(" "),t("p",[a._v("之前有很多教程都有说明如何创建Spark项目，这里再次说明。首先在/usr/local/spark/mycode新建项目主目录kafka,然后在kafka目录下新建scala文件存放目录以及scala工程文件")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("mkdir")]),a._v(" -p /usr/local/spark/mycode/kafka/src/main/scala\n")])])]),t("p",[a._v("接着在src/main/scala文件下创建两个文件，一个是用于设置日志，一个是项目工程主文件，设置日志文件为StreamingExamples.scala")]),a._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("package")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("examples"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("internal"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Logging")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("log4j"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Level")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Logger")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/** Utility functions for Spark Streaming examples. */")]),a._v("\nobject "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StreamingExamples")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("extends")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Logging")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("/** Set reasonable logging levels for streaming if the user has not configured log4j. */")]),a._v("\n  def "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("setStreamingLogLevels")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n    val log4jInitialized "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Logger")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("getRootLogger"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("getAllAppenders"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("hasMoreElements\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("if")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("!")]),a._v("log4jInitialized"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// We first log something to initialize Spark's default logging, then we override the")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// logging level.")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("logInfo")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Setting log level to [WARN] for streaming example."')]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('" To override add a custom log4j.properties to the classpath."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Logger")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("getRootLogger"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("setLevel")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Level")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("WARN"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n")])])]),t("p",[a._v("这个文件不做过多解释，因为这只是一个辅助文件，下面着重介绍工程主文件，文件名为KafkaTest.scala")]),a._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("package")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("examples"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming")]),a._v("\n\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("java"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("util"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("HashMap")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("clients"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("producer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaProducer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerRecord")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("clients"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("consumer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerConfig")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("common"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("serialization"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StringDeserializer")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("json4s"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),a._v("_\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("json4s"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("jackson"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Serialization")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("json4s"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("jackson"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Serialization")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("write\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("SparkConf")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),a._v("_\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Interval")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token namespace"}},[a._v("org"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("apache"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("spark"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("streaming"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("kafka010"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")])]),a._v("_\n\nobject "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaWordCount")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n  implicit val formats "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("DefaultFormats")]),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("//数据格式化时需要")]),a._v("\n  def "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("main")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("args"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Array")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Unit")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("if")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("length "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("<")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("3")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("System")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("err"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("println")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"Usage: KafkaWordCount <brokers> <groupId> <topics>"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("System")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("exit")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StreamingExamples")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("setStreamingLogLevels")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n    val "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Array")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("brokers"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" groupId"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" topics"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" args\n    val sparkConf "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("SparkConf")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("setAppName")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"KafkaWordCount"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    val ssc "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StreamingContext")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("sparkConf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Seconds")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    ssc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("checkpoint")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"checkpoint"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n    val topicsSet "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" topics"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("split")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('","')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("toSet\n    val kafkaParams "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Map")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Object")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("BOOTSTRAP_SERVERS_CONFIG "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("->")]),a._v(" brokers"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("GROUP_ID_CONFIG "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("->")]),a._v(" groupId"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("KEY_DESERIALIZER_CLASS_CONFIG "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("->")]),a._v(" classOf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StringDeserializer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("VALUE_DESERIALIZER_CLASS_CONFIG "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("->")]),a._v(" classOf"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("StringDeserializer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n    val messages "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaUtils")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("createDirectStream"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("\n      ssc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("LocationStrategies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("PreferConsistent")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ConsumerStrategies"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("Subscribe")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("topicsSet"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" kafkaParams"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// Get the lines, split them into words, count the words and print")]),a._v("\n    val lines "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" messages"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("map")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    val words "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" lines"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("flatMap")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("split")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('" "')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("//将输入的每行用空格分割成一个个word")]),a._v("\n\n\n    "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// 对每一秒的输入数据进行reduce，然后将reduce后的数据发送给Kafka")]),a._v("\n    val wordCounts "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" words"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("map")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("x "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("x"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1L")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n        "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("reduceByKeyAndWindow")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("-")]),a._v("_"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Seconds")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Seconds")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("foreachRDD")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("rdd "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n          "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("if")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("rdd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("count "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("!=")]),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("0")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("{")]),a._v("\n               val props "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("HashMap")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Object")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               props"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("put")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("BOOTSTRAP_SERVERS_CONFIG"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"127.0.0.1:9092"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               props"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("put")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("VALUE_SERIALIZER_CLASS_CONFIG"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"org.apache.kafka.common.serialization.StringSerializer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               props"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("put")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerConfig")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("KEY_SERIALIZER_CLASS_CONFIG"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"org.apache.kafka.common.serialization.StringSerializer"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// 实例化一个Kafka生产者")]),a._v("\n               val producer "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaProducer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("props"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// rdd.colect即将rdd中数据转化为数组，然后write函数将rdd内容转化为json格式")]),a._v("\n               val str "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("write")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("rdd"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("collect"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v('// 封装成Kafka消息，topic为"result"')]),a._v("\n               val message "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("new")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("ProducerRecord")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("String")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"result"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("null")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),a._v(" str"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n               "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("// 给Kafka发送消息")]),a._v("\n               producer"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("send")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("message"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n          "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n      "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    ssc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("start")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n    ssc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("awaitTermination")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n  "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("}")]),a._v("\n")])])]),t("p",[a._v("上述代码注释已经也很清楚了，下面在简要说明下：")]),a._v(" "),t("ul",[t("li",[t("ol",[t("li",[a._v("首先按每秒的频率读取Kafka消息；")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"2"}},[t("li",[a._v("然后对每秒的数据执行wordcount算法，统计出0的个数，1的个数，2的个数；")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"3"}},[t("li",[a._v("最后将上述结果封装成json发送给Kafka。")])])])]),a._v(" "),t("p",[a._v("另外，需要注意，上面代码中有一行如下代码：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("ssc.checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"."')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v(" \n")])])]),t("p",[a._v("这行代码表示把检查点文件写入分布式文件系统HDFS，所以一定要事先启动Hadoop。如果没有启动Hadoop，则后面运行时会出现“拒绝连接”的错误提示。如果你还没有启动Hadoop，则可以现在在Ubuntu终端中，使用如下Shell命令启动Hadoop：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[a._v("cd")]),a._v(" /usr/local/hadoop  "),t("span",{pre:!0,attrs:{class:"token comment"}},[a._v("#这是hadoop的安装目录")]),a._v("\n./sbin/start-dfs.sh\n")])])]),t("p",[a._v("另外，如果不想把检查点写入HDFS，而是直接把检查点写入本地磁盘文件（这样就不用启动Hadoop），则可以对ssc.checkpoint()方法中的文件路径进行指定,比如下面这个例子：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("ssc.checkpoint"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"file:///usr/local/spark/mycode/kafka/checkpoint"')]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),t("blockquote",[t("blockquote",[t("p",[a._v("checkpoint的意思就是建立检查点,类似于快照,例如在spark计算里面 计算流程DAG特别长,服务器需要将整个DAG计算完成得出结果,但是如果在这很长的计算流程中突然中间算出的数据丢失了,spark又会根据RDD的依赖关系从头到尾计算一遍,这样子就很费性能,当然我们可以将中间的计算结果通过cache或者persist放到内存或者磁盘中,但是这样也不能保证数据完全不会丢失,存储的这个内存出问题了或者磁盘坏了,也会导致spark从头再根据RDD计算一遍,所以就有了checkpoint,其中checkpoint的作用就是将DAG中比较重要的中间数据做一个检查点将结果存储到一个高可用的地方(通常这个地方就是HDFS里面)")])])]),a._v(" "),t("h3",{attrs:{id:"运行项目"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#运行项目"}},[a._v("#")]),a._v(" 运行项目")]),a._v(" "),t("p",[a._v("编写好程序之后，下面介绍下如何打包运行程序。在/usr/local/spark/mycode/kafka目录下新建文件simple.sbt，输入如下内容：")]),a._v(" "),t("div",{staticClass:"language-ini extra-class"},[t("pre",{pre:!0,attrs:{class:"language-ini"}},[t("code",[a._v("name :"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "Simple Project"')]),a._v("\nversion :"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "1.0"')]),a._v("\nscalaVersion :"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "2.11.8"')]),a._v("\nlibraryDependencies +"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "org.apache.spark" %% "spark-core" % "2.3.0"')]),a._v("\nlibraryDependencies +"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "org.apache.spark" % "spark-streaming_2.11" % "2.3.0"')]),a._v("\nlibraryDependencies +"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "org.apache.spark" % "spark-streaming-kafka-0-10_2.11" % "2.3.0"')]),a._v("\nlibraryDependencies +"),t("span",{pre:!0,attrs:{class:"token attr-value"}},[t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),a._v(' "org.json4s" %% "json4s-jackson" % "3.2.11"')]),a._v("\n")])])]),t("p",[a._v("然后，即可编译打包程序，输入如下命令")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v("/usr/local/sbt/sbt package\n")])])]),t("p",[a._v("打包成功之后，接下来编写运行脚本，在/usr/local/spark/mycode/kafka目录下新建startup.sh文件，输入如下内容：")]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[a._v(" /usr/local/spark/spark-2.3.0-bin-hadoop2.7/bin/spark-submit --driver-class-path /usr/local/spark/spark-2.3.0-bin-hadoop2.7/jars/*:/usr/local/spark/spark-2.3.0-bin-hadoop2.7/jars/kafka/* --class "),t("span",{pre:!0,attrs:{class:"token string"}},[a._v('"org.apache.spark.examples.streaming.KafkaWordCount"')]),a._v(" /usr/local/spark/mycode/kafka/target/scala-2.11/simple-project_2.11-1.0.jar "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("127.0")]),a._v(".0.1:9092 "),t("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),a._v(" sex \n")])])]),t("p",[a._v("其中最后四个为输入参数，含义如下")]),a._v(" "),t("ul",[t("li",[t("ol",[t("li",[a._v("127.0.0.1:9092为brokerer地址")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"2"}},[t("li",[a._v("1 为consumer group标签")])])]),a._v(" "),t("li",[t("ol",{attrs:{start:"3"}},[t("li",[a._v("sex为消费者接收的topic\n最后在/usr/local/spark/mycode/kafka目录下，运行如下命令即可执行刚编写好的Spark Streaming程序")])])])]),a._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[a._v("sh")]),a._v(" startup.sh\n")])])]),t("p",[a._v("程序运行成功之后，下面通过之前的KafkaProducer和KafkaConsumer来检测程序。")]),a._v(" "),t("h3",{attrs:{id:"测试程序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#测试程序"}},[a._v("#")]),a._v(" 测试程序")]),a._v(" "),t("p",[a._v("下面开启之前编写的KafkaProducer投递消息，然后将KafkaConsumer中接收的topic改为result，验证是否能接收topic为result的消息，更改之后的KafkaConsumer为")]),a._v(" "),t("div",{staticClass:"language-java extra-class"},[t("pre",{pre:!0,attrs:{class:"language-java"}},[t("code",[a._v("from kafka "),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("import")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaConsumer")]),a._v("\n \nconsumer "),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),t("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("KafkaConsumer")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'result'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n"),t("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("for")]),a._v(" msg in consumer"),t("span",{pre:!0,attrs:{class:"token operator"}},[a._v(":")]),a._v("\n    "),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("print")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("msg"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("value"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),t("span",{pre:!0,attrs:{class:"token function"}},[a._v("decode")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[a._v("'utf8'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),t("p",[a._v("在同时开启Spark Streaming项目，KafkaProducer以及KafkaConsumer之后，可以在KafkaConsumer运行窗口，出现以下类似数据：")]),a._v(" "),t("blockquote",[t("p",[a._v('[{"0":1},{"2":3},{"1":6}]\n[{"0":5},{"2":2},{"1":3}]\n[{"0":3},{"2":3},{"1":4}]\n.......')])]),a._v(" "),t("h2",{attrs:{id:"四、结果展示"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#四、结果展示"}},[a._v("#")]),a._v(" 四、结果展示")]),a._v(" "),t("p",[a._v("接下来做的事是，利用Flask-SocketIO实时推送数据，socket.io.js实时获取数据，highlights.js展示数据。")]),a._v(" "),t("h3",{attrs:{id:"flask-socketio实时推送数据"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#flask-socketio实时推送数据"}},[a._v("#")]),a._v(" Flask-SocketIO实时推送数据")]),a._v(" "),t("p",[a._v("将介绍如何利用Flask-SocketIO将结果实时推送到浏览器。\n下载"),t("a",{attrs:{href:"https://pan.baidu.com/s/1c3oqzXQ",target:"_blank",rel:"noopener noreferrer"}},[a._v("代码"),t("OutboundLink")],1),a._v("，用python3.5 运行 app.py即可:")]),a._v(" "),t("div",{staticClass:"language-py extra-class"},[t("pre",{pre:!0,attrs:{class:"language-py"}},[t("code",[a._v("python app"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),a._v("py\n")])])]),t("h2",{attrs:{id:"参考文章"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[a._v("#")]),a._v(" 参考文章")]),a._v(" "),t("ul",[t("li",[a._v("https://www.cnblogs.com/zhaojinyan/p/9360873.html")])])])}),[],!1,null,null,null);s.default=e.exports}}]);