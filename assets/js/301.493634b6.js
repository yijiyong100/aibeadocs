(window.webpackJsonp=window.webpackJsonp||[]).push([[301],{816:function(s,t,a){"use strict";a.r(t);var r=a(53),e=Object(r.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[s._v("TIP")]),s._v(" "),a("p",[s._v("本文主要是介绍 MySQL慢查询和索引优化 。")])]),s._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#_1、背景"}},[s._v("1、背景")])]),a("li",[a("a",{attrs:{href:"#_2、一个慢查询引发的思考"}},[s._v("2、一个慢查询引发的思考")])]),a("li",[a("a",{attrs:{href:"#_3、mysql索引原理"}},[s._v("3、MySQL索引原理")]),a("ul",[a("li",[a("a",{attrs:{href:"#_3-1-索引目的"}},[s._v("3.1. 索引目的")])]),a("li",[a("a",{attrs:{href:"#_3-2-索引原理"}},[s._v("3.2. 索引原理")])]),a("li",[a("a",{attrs:{href:"#_3-3-磁盘io与预读"}},[s._v("3.3. 磁盘IO与预读")])]),a("li",[a("a",{attrs:{href:"#_3-4-索引的数据结构"}},[s._v("3.4. 索引的数据结构")])]),a("li",[a("a",{attrs:{href:"#_3-5-详解b-树"}},[s._v("3.5. 详解b+树")])]),a("li",[a("a",{attrs:{href:"#_3-6-b-树的查找过程"}},[s._v("3.6. b+树的查找过程")])]),a("li",[a("a",{attrs:{href:"#_3-7-b-树性质"}},[s._v("3.7. b+树性质")])])])]),a("li",[a("a",{attrs:{href:"#_4、慢查询优化"}},[s._v("4、慢查询优化")]),a("ul",[a("li",[a("a",{attrs:{href:"#_4-1-建索引的几大原则"}},[s._v("4.1. 建索引的几大原则")])]),a("li",[a("a",{attrs:{href:"#_4-2-回到开始的慢查询"}},[s._v("4.2. 回到开始的慢查询")])]),a("li",[a("a",{attrs:{href:"#_4-3-查询优化神器-explain命令"}},[s._v("4.3. 查询优化神器 - explain命令")])]),a("li",[a("a",{attrs:{href:"#_4-4-慢查询优化基本步骤"}},[s._v("4.4. 慢查询优化基本步骤")])]),a("li",[a("a",{attrs:{href:"#_4-5-几个慢查询案例"}},[s._v("4.5. 几个慢查询案例")])]),a("li",[a("a",{attrs:{href:"#_4-6-复杂语句写法"}},[s._v("4.6. 复杂语句写法")])]),a("li",[a("a",{attrs:{href:"#_4-7-明确应用场景"}},[s._v("4.7. 明确应用场景")])]),a("li",[a("a",{attrs:{href:"#_4-8-无法优化的语句"}},[s._v("4.8. 无法优化的语句")])])])]),a("li",[a("a",{attrs:{href:"#写在后面的话"}},[s._v("写在后面的话")])]),a("li",[a("a",{attrs:{href:"#参考文献"}},[s._v("参考文献：")])]),a("li",[a("a",{attrs:{href:"#参考文章"}},[s._v("参考文章")])])])]),a("p"),s._v(" "),a("p",[s._v("MySQL索引原理及慢查询优化")]),s._v(" "),a("blockquote",[a("p",[s._v("本文转载自"),a("a",{attrs:{href:"https://tech.meituan.com/2014/06/30/mysql-index.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("MySQL索引原理及慢查询优化"),a("OutboundLink")],1)])]),s._v(" "),a("h2",{attrs:{id:"_1、背景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、背景"}},[s._v("#")]),s._v(" 1、背景")]),s._v(" "),a("p",[s._v("MySQL凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓“好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如“精通MySQL”、“SQL语句优化”、“了解数据库原理”等要求。我们知道一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。")]),s._v(" "),a("p",[s._v("本人从2013年7月份起，一直在美团核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。")]),s._v(" "),a("h2",{attrs:{id:"_2、一个慢查询引发的思考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、一个慢查询引发的思考"}},[s._v("#")]),s._v(" 2、一个慢查询引发的思考")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   task \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" operator_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20839")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" operate_time"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1371169729")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" operate_time"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1371174603")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。")]),s._v(" "),a("p",[s._v("并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”。")]),s._v(" "),a("p",[s._v("我很惊讶，问道：“为什么需要每个字段都加上索引？”")]),s._v(" "),a("p",[s._v("“把查询的字段都加上索引会更快”，工程师信心满满。")]),s._v(" "),a("p",[s._v("“这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。”")]),s._v(" "),a("p",[s._v("“联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。")]),s._v(" "),a("p",[s._v("多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。")]),s._v(" "),a("h2",{attrs:{id:"_3、mysql索引原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、mysql索引原理"}},[s._v("#")]),s._v(" 3、MySQL索引原理")]),s._v(" "),a("h3",{attrs:{id:"_3-1-索引目的"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-索引目的"}},[s._v("#")]),s._v(" 3.1. 索引目的")]),s._v(" "),a("p",[s._v("索引的目的在于提高查询效率，可以类比字典，如果要查“mysql”这个单词，我们肯定需要定位到m字母，然后从下往下找到y字母，再找到剩下的sql。如果没有索引，那么你可能需要把所有单词看一遍才能找到你想要的，如果我想找到m开头的单词呢？或者ze开头的单词呢？是不是觉得如果没有索引，这个事情根本无法完成？")]),s._v(" "),a("h3",{attrs:{id:"_3-2-索引原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-索引原理"}},[s._v("#")]),s._v(" 3.2. 索引原理")]),s._v(" "),a("p",[s._v("除了词典，生活中随处可见索引的例子，如火车站的车次表、图书的目录等。它们的原理都是一样的，通过不断的缩小想要获得数据的范围来筛选出最终想要的结果，同时把随机的事件变成顺序的事件，也就是我们总是通过同一种查找方式来锁定数据。")]),s._v(" "),a("p",[s._v("数据库也是一样，但显然要复杂许多，因为不仅面临着等值查询，还有范围查询(>、<、between、in)、模糊查询(like)、并集查询(or)等等。数据库应该选择怎么样的方式来应对所有的问题呢？我们回想字典的例子，能不能把数据分成段，然后分段查询呢？最简单的如果1000条数据，1到100分成第一段，101到200分成第二段，201到300分成第三段……这样查第250条数据，只要找第三段就可以了，一下子去除了90%的无效数据。但如果是1千万的记录呢，分成几段比较好？稍有算法基础的同学会想到搜索树，其平均复杂度是lgN，具有不错的查询性能。但这里我们忽略了一个关键的问题，复杂度模型是基于每次相同的操作成本来考虑的，数据库实现比较复杂，数据保存在磁盘上，而为了提高性能，每次又可以把部分数据读入内存来计算，因为我们知道访问磁盘的成本大概是访问内存的十万倍左右，所以简单的搜索树难以满足复杂的应用场景。")]),s._v(" "),a("h3",{attrs:{id:"_3-3-磁盘io与预读"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-磁盘io与预读"}},[s._v("#")]),s._v(" 3.3. 磁盘IO与预读")]),s._v(" "),a("p",[s._v("前面提到了访问磁盘，那么这里先简单介绍一下磁盘IO和预读，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分，寻道时间指的是磁臂移动到指定磁道所需要的时间，主流磁盘一般在5ms以下；旋转延迟就是我们经常听说的磁盘转速，比如一个磁盘7200转，表示每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1/120/2 = 4.17ms；传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右，听起来还挺不错的，但要知道一台500 -MIPS的机器每秒可以执行5亿条指令，因为指令依靠的是电的性质，换句话说执行一次IO的时间可以执行40万条指令，数据库动辄十万百万乃至千万级数据，每次9毫秒的时间，显然是个灾难。下图是计算机硬件延迟的对比图，供大家参考：")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/db/mysqlopt1/slowopt-1.png"),alt:"wxmp"}}),s._v(" "),a("p",[s._v("various-system-software-hardware-latencies")]),s._v(" "),a("p",[s._v("考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。")]),s._v(" "),a("h3",{attrs:{id:"_3-4-索引的数据结构"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-索引的数据结构"}},[s._v("#")]),s._v(" 3.4. 索引的数据结构")]),s._v(" "),a("p",[s._v("前面讲了生活中索引的例子，索引的基本原理，数据库的复杂性，又讲了操作系统的相关知识，目的就是让大家了解，任何一种数据结构都不是凭空产生的，一定会有它的背景和使用场景，我们现在总结一下，我们需要这种数据结构能够做些什么，其实很简单，那就是：每次查找数据时把磁盘IO次数控制在一个很小的数量级，最好是常数数量级。那么我们就想到如果一个高度可控的多路搜索树是否能满足需求呢？就这样，b+树应运而生。")]),s._v(" "),a("h3",{attrs:{id:"_3-5-详解b-树"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-详解b-树"}},[s._v("#")]),s._v(" 3.5. 详解b+树")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/db/mysqlopt1/slowopt-2.jpg"),alt:"wxmp"}}),s._v(" "),a("p",[s._v("b+树")]),s._v(" "),a("p",[s._v("如上图，是一颗b+树，关于b+树的定义可以参见"),a("a",{attrs:{href:"http://zh.wikipedia.org/wiki/B%2B%E6%A0%91",target:"_blank",rel:"noopener noreferrer"}},[s._v("B+树"),a("OutboundLink")],1),s._v("，这里只说一些重点，浅蓝色的块我们称之为一个磁盘块，可以看到每个磁盘块包含几个数据项（深蓝色所示）和指针（黄色所示），如磁盘块1包含数据项17和35，包含指针P1、P2、P3，P1表示小于17的磁盘块，P2表示在17和35之间的磁盘块，P3表示大于35的磁盘块。真实的数据存在于叶子节点即3、5、9、10、13、15、28、29、36、60、75、79、90、99。非叶子节点只不存储真实的数据，只存储指引搜索方向的数据项，如17、35并不真实存在于数据表中。")]),s._v(" "),a("h3",{attrs:{id:"_3-6-b-树的查找过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-b-树的查找过程"}},[s._v("#")]),s._v(" 3.6. b+树的查找过程")]),s._v(" "),a("p",[s._v("如图所示，如果要查找数据项29，那么首先会把磁盘块1由磁盘加载到内存，此时发生一次IO，在内存中用二分查找确定29在17和35之间，锁定磁盘块1的P2指针，内存时间因为非常短（相比磁盘的IO）可以忽略不计，通过磁盘块1的P2指针的磁盘地址把磁盘块3由磁盘加载到内存，发生第二次IO，29在26和30之间，锁定磁盘块3的P2指针，通过指针加载磁盘块8到内存，发生第三次IO，同时内存中做二分查找找到29，结束查询，总计三次IO。真实的情况是，3层的b+树可以表示上百万的数据，如果上百万的数据查找只需要三次IO，性能提高将是巨大的，如果没有索引，每个数据项都要发生一次IO，那么总共需要百万次的IO，显然成本非常非常高。")]),s._v(" "),a("h3",{attrs:{id:"_3-7-b-树性质"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-b-树性质"}},[s._v("#")]),s._v(" 3.7. b+树性质")]),s._v(" "),a("p",[s._v("1.通过上面的分析，我们知道IO次数取决于b+数的高度h，假设当前数据表的数据为N，每个磁盘块的数据项的数量是m，则有h=㏒(m+1)N，当数据量N一定的情况下，m越大，h越小；而m = 磁盘块的大小 / 数据项的大小，磁盘块的大小也就是一个数据页的大小，是固定的，如果数据项占的空间越小，数据项的数量越多，树的高度越低。这就是为什么每个数据项，即索引字段要尽量的小，比如int占4字节，要比bigint8字节少一半。这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。")]),s._v(" "),a("p",[s._v("2.当b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+数是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道下一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。")]),s._v(" "),a("h2",{attrs:{id:"_4、慢查询优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、慢查询优化"}},[s._v("#")]),s._v(" 4、慢查询优化")]),s._v(" "),a("p",[s._v("关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则：")]),s._v(" "),a("h3",{attrs:{id:"_4-1-建索引的几大原则"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-建索引的几大原则"}},[s._v("#")]),s._v(" 4.1. 建索引的几大原则")]),s._v(" "),a("p",[s._v("1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。")]),s._v(" "),a("p",[s._v("2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。")]),s._v(" "),a("p",[s._v("3.尽量选择区分度高的列作为索引，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。")]),s._v(" "),a("p",[s._v("4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。")]),s._v(" "),a("p",[s._v("5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。")]),s._v(" "),a("h3",{attrs:{id:"_4-2-回到开始的慢查询"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-回到开始的慢查询"}},[s._v("#")]),s._v(" 4.2. 回到开始的慢查询")]),s._v(" "),a("p",[s._v("根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析；比如还有如下查询：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" task "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("12")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" task "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则")]),s._v(" "),a("h3",{attrs:{id:"_4-3-查询优化神器-explain命令"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-查询优化神器-explain命令"}},[s._v("#")]),s._v(" 4.3. 查询优化神器 - explain命令")]),s._v(" "),a("p",[s._v("关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网"),a("a",{attrs:{href:"http://dev.mysql.com/doc/refman/5.5/en/explain-output.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("explain-output"),a("OutboundLink")],1),s._v("，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。")]),s._v(" "),a("h3",{attrs:{id:"_4-4-慢查询优化基本步骤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-慢查询优化基本步骤"}},[s._v("#")]),s._v(" 4.4. 慢查询优化基本步骤")]),s._v(" "),a("p",[s._v("0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE")]),s._v(" "),a("p",[s._v("1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高")]),s._v(" "),a("p",[s._v("2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）")]),s._v(" "),a("p",[s._v("3.order by limit 形式的sql语句让排序的表优先查")]),s._v(" "),a("p",[s._v("4.了解业务方使用场景")]),s._v(" "),a("p",[s._v("5.加索引时参照建索引的几大原则")]),s._v(" "),a("p",[s._v("6.观察结果，不符合预期继续从0分析")]),s._v(" "),a("h3",{attrs:{id:"_4-5-几个慢查询案例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-5-几个慢查询案例"}},[s._v("#")]),s._v(" 4.5. 几个慢查询案例")]),s._v(" "),a("p",[s._v("下面几个例子详细解释了如何分析和优化慢查询。")]),s._v(" "),a("h3",{attrs:{id:"_4-6-复杂语句写法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-6-复杂语句写法"}},[s._v("#")]),s._v(" 4.6. 复杂语句写法")]),s._v(" "),a("p",[s._v("很多情况下，我们写SQL只是为了实现功能，这只是第一步，不同的语句书写方式对于效率往往有本质的差别，这要求我们对mysql的执行计划和索引原则有非常清楚的认识，请看下面的语句：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("distinct")]),s._v(" cert"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp_id \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   cm_log cl \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n         emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" emp_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n         emp_cert"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" cert_id \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n         employee emp \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("left")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n         emp_certificate emp_cert \n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" emp_cert"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp_id \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n         emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_deleted"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" cert \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n         cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_table"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Employee'")]),s._v(" \n         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cert"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp_id\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("or")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n         cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_table"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'EmpCertificate'")]),s._v(" \n         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cert"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cert_id\n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-07 15:03:00'")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-08 16:00:00'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("0.先运行一下，53条记录 1.87秒，又没有用聚合语句，比较慢")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("53")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.87")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("1.explain")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" select_type "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" possible_keys                   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("key")]),s._v("                   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" key_len "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Extra                          "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cl         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" range "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cm_log_cls_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_last_upd_date     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("379")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("temporary")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("derived2"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ALL")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                            "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("63727")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" buffer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" DERIVED     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ALL")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                            "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("13317")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("                    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" DERIVED     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp_cert   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp_certificate_empid           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp_certificate_empid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituanorg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("index")]),s._v("                    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+------------+-------+---------------------------------+-----------------------+---------+-------------------+-------+--------------------------------+")]),s._v("\n")])])]),a("p",[s._v("简述一下执行计划，首先mysql根据idx_last_upd_date索引扫描cm_log表获得379条记录；然后查表扫描了63727条记录，分为两部分，derived表示构造表，也就是不存在的表，可以简单理解成是一个语句形成的结果集，后面的数字表示语句的ID。derived2表示的是ID = 2的查询构造了虚拟表，并且返回了63727条记录。我们再来看看ID = 2的语句究竟做了写什么返回了这么大量的数据，首先全表扫描employee表13317条记录，然后根据索引emp_certificate_empid关联emp_certificate表，rows = 1表示，每个关联都只锁定了一条记录，效率比较高。获得后，再和cm_log的379条记录根据规则关联。从执行过程上可以看出返回了太多的数据，返回的数据绝大部分cm_log都用不到，因为cm_log只锁定了379条记录。")]),s._v(" "),a("p",[s._v("如何优化呢？可以看到我们在运行完后还是要和cm_log做join,那么我们能不能之前和cm_log做join呢？仔细分析语句不难发现，其基本思想是如果cm_log的ref_table是EmpCertificate就关联emp_certificate表，如果ref_table是Employee就关联employee表，我们完全可以拆成两部分，并用union连接起来，注意这里用union，而不用union all是因为原语句有“distinct”来得到唯一的记录，而union恰好具备了这种功能。如果原语句中没有distinct不需要去重，我们就可以直接使用union all了，因为使用union需要去重的动作，会影响SQL性能。")]),s._v(" "),a("p",[s._v("优化过的语句如下：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   cm_log cl \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   employee emp \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_table "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'Employee'")]),s._v(" \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-07 15:03:00'")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-08 16:00:00'")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_deleted "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("union")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   cm_log cl \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   emp_certificate ec \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_table "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'EmpCertificate'")]),s._v(" \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   employee emp \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" ec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp_id  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-07 15:03:00'")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_upd_date"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2013-11-08 16:00:00'")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" emp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_deleted "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("\n")])])]),a("p",[s._v("4.不需要了解业务场景，只需要改造的语句和改造之前的语句保持结果一致")]),s._v(" "),a("p",[s._v("5.现有索引可以满足，不需要建索引")]),s._v(" "),a("p",[s._v("6.用改造后的语句实验一下，只需要10ms 降低了近200倍！")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" select_type  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" possible_keys                   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("key")]),s._v("               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" key_len "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref                   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Extra       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cl         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" range  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cm_log_cls_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("379")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" eq_ref "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("                         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituanorg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("UNION")]),s._v("        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cl         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" range  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cm_log_cls_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_last_upd_date "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("379")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("UNION")]),s._v("        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ec         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" eq_ref "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("emp_certificate_empid   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituanorg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cl"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ref_oid "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("UNION")]),s._v("        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" emp        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" eq_ref "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("                         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituanorg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("emp_id  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("UNION")]),s._v(" RESULT "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("union1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ALL")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                            "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("                  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+--------------+------------+--------+---------------------------------+-------------------+---------+-----------------------+------+-------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("53")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.01")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("h3",{attrs:{id:"_4-7-明确应用场景"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-7-明确应用场景"}},[s._v("#")]),s._v(" 4.7. 明确应用场景")]),s._v(" "),a("p",[s._v("举这个例子的目的在于颠覆我们对列的区分度的认知，一般上我们认为区分度越高的列，越容易锁定更少的记录，但在一些特殊的情况下，这种理论是有局限性的。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   stage_poi sp \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   sp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("accurate_result"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n      sp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sync_status"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("or")]),s._v(" sp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sync_status"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("or")]),s._v(" sp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sync_status"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("0.先看看运行多长时间,951条数据6.22秒，真的很慢。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("951")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("6.22")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("1.先explain，rows达到了361万，type = ALL表明是全表扫描。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+-------+------+---------------+------+---------+------+---------+-------------+| id | select_type | table | type | possible_keys | key  | key_len | ref  | rows    | Extra       |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+|  1 | SIMPLE      | sp    | ALL  | NULL          | NULL | NULL    | NULL | 3613155 | Using where |+----+-------------+-------+------+---------------+------+---------+------+---------+-------------+")]),s._v("\n")])])]),a("p",[s._v("2.所有字段都应用查询返回记录数，因为是单表查询 0已经做过了951条。")]),s._v(" "),a("p",[s._v("3.让explain的rows 尽量逼近951。")]),s._v(" "),a("p",[s._v("看一下accurate_result = 1的记录数：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("accurate_result "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" stage_poi  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" accurate_result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----------+-----------------+| count(*) | accurate_result |+----------+-----------------+|     1023 |              -1 ||  2114655 |               0 ||   972815 |               1 |+----------+-----------------+")]),s._v("\n")])])]),a("p",[s._v("我们看到accurate_result这个字段的区分度非常低，整个表只有-1,0,1三个值，加上索引也无法锁定特别少量的数据。")]),s._v(" "),a("p",[s._v("再看一下sync_status字段的情况：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("sync_status "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" stage_poi  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" sync_status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----------+-------------+| count(*) | sync_status |+----------+-------------+|     3080 |           0 ||  3085413 |           3 |+----------+-------------+")]),s._v("\n")])])]),a("p",[s._v("同样的区分度也很低，根据理论，也不适合建立索引。")]),s._v(" "),a("p",[s._v("问题分析到这，好像得出了这个表无法优化的结论，两个列的区分度都很低，即便加上索引也只能适应这种情况，很难做普遍性的优化，比如当sync_status 0、3分布的很平均，那么锁定记录也是百万级别的。")]),s._v(" "),a("p",[s._v("4.找业务方去沟通，看看使用场景。业务方是这么来使用这个SQL语句的，每隔五分钟会扫描符合条件的数据，处理完成后把sync_status这个字段变成1,五分钟符合条件的记录数并不会太多，1000个左右。了解了业务方的使用场景后，优化这个SQL就变得简单了，因为业务方保证了数据的不平衡，如果加上索引可以过滤掉绝大部分不需要的数据。")]),s._v(" "),a("p",[s._v("5.根据建立索引规则，使用如下语句建立索引")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("alter")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" stage_poi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("add")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("index")]),s._v(" idx_acc_status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("accurate_result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("sync_status"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("6.观察预期结果,发现只需要200ms，快了30多倍。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("952")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.20")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("我们再来回顾一下分析问题的过程，单表查询相对来说比较好优化，大部分时候只需要把where条件里面的字段依照规则加上索引就好，如果只是这种“无脑”优化的话，显然一些区分度非常低的列，不应该加索引的列也会被加上索引，这样会对插入、更新性能造成严重的影响，同时也有可能影响其它的查询语句。所以我们第4步调差SQL的使用场景非常关键，我们只有知道这个业务场景，才能更好地辅助我们更好的分析和优化查询语句。")]),s._v(" "),a("h3",{attrs:{id:"_4-8-无法优化的语句"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-8-无法优化的语句"}},[s._v("#")]),s._v(" 4.8. 无法优化的语句")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("position"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("phone"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("office_phone"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("feature_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("birthday"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("creator_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_keyperson"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("giveup_reason"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_source"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("created_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" created_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_modified"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" last_modified"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_modified_user_id  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("   contact c  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("   contact_branch cb       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("contact_id  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("   branch_user bu       "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("         "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("      org_emp_info oei          "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id          "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_left "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2875")]),s._v("          "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_right "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10802")]),s._v("          "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("org_category "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("      c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("created_time "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("desc")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("还是几个步骤。")]),s._v(" "),a("p",[s._v("0.先看语句运行多长时间，10条记录用了13秒，已经不可忍受。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("13.06")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("1.explain")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" select_type "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("table")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("type")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" possible_keys                       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("key")]),s._v("                     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" key_len "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref                      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Extra                                        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SIMPLE")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" oei   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_category_left_right"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_data_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_category_left_right "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" const                    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("8849")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("temporary")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" filesort "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SIMPLE")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" bu    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_userid_status           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_userid_status       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituancrm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_id   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("76")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("Using")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("index")]),s._v("                     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SIMPLE")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" cb    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" ref    "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_branch_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("idx_contact_branch_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" idx_branch_id           "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("4")]),s._v("       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituancrm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("                                              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SIMPLE")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" c     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" eq_ref "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("                             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v("                 "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("108")]),s._v("     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" meituancrm"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("contact_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("                                              "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----+-------------+-------+--------+-------------------------------------+-------------------------+---------+--------------------------+------+----------------------------------------------+")]),s._v("\n")])])]),a("p",[s._v("从执行计划上看，mysql先查org_emp_info表扫描8849记录，再用索引idx_userid_status关联branch_user表，再用索引idx_branch_id关联contact_branch表，最后主键关联contact表。")]),s._v(" "),a("p",[s._v("rows返回的都非常少，看不到有什么异常情况。我们在看一下语句，发现后面有order by + limit组合，会不会是排序量太大搞的？于是我们简化SQL，去掉后面的order by 和 limit，看看到底用了多少记录来排序。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   contact c  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   contact_branch cb \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("contact_id  \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n   branch_user bu \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id \n      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n         "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("  \n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n      org_emp_info oei \n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id \n         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_left "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2875")]),s._v(" \n         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_right "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10802")]),s._v(" \n         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("org_category "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("  \n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("778878")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("----------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("row")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5.19")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("发现排序之前居然锁定了778878条记录，如果针对70万的结果集排序，将是灾难性的，怪不得这么慢，那我们能不能换个思路，先根据contact的created_time排序，再来join会不会比较快呢？")]),s._v(" "),a("p",[s._v("于是改造成下面的语句，也可以用straight_join来优化：")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("select\n   c.id,\n   c.name,\n   c.position,\n   c.sex,\n   c.phone,\n   c.office_phone,\n   c.feature_info,\n   c.birthday,\n   c.creator_id,\n   c.is_keyperson,\n   c.giveup_reason,\n   c.status,\n   c.data_source,\n   from_unixtime(c.created_time) as created_time,\n   from_unixtime(c.last_modified) as last_modified,\n   c.last_modified_user_id   \nfrom\n   contact c  \nwhere\n   exists (\n      select\n         1 \n      from\n         contact_branch cb  \n      inner join\n         branch_user bu        \n            on  cb.branch_id = bu.branch_id        \n            and bu.status in (\n               1,\n            2)      \n         inner join\n            org_emp_info oei           \n               on  oei.data_id = bu.user_id           \n               and oei.node_left >= 2875           \n               and oei.node_right <= 10802           \n               and oei.org_category = - 1      \n         where\n            c.id = cb.contact_id    \n      )    \n   order by\n      c.created_time desc  limit 0 ,\n      10;\n")])])]),a("p",[s._v("验证一下效果 预计在1ms内，提升了13000多倍！")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("rows")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.00")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("本以为至此大工告成，但我们在前面的分析中漏了一个细节，先排序再join和先join再排序理论上开销是一样的，为何提升这么多是因为有一个limit！大致执行过程是：mysql先按索引排序得到前10条记录，然后再去join过滤，当发现不够10条的时候，再次去10条，再次join，这显然在内层join过滤的数据非常多的时候，将是灾难的，极端情况，内层一条数据都找不到，mysql还傻乎乎的每次取10条，几乎遍历了这个数据表！")]),s._v(" "),a("p",[s._v("用不同参数的SQL试验下：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n   sql_no_cache   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("position"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sex"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("phone"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("office_phone"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("feature_info"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("birthday"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("creator_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("is_keyperson"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("giveup_reason"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_source"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("created_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" created_time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   from_unixtime"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_modified"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" last_modified"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("last_modified_user_id    \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n   contact c   \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("exists")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v("\n         "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("        \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v("\n         contact_branch cb         \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n         branch_user bu                     \n            "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("branch_id                     \n            "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("status")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n               "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n            "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("                \n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("inner")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v("\n            org_emp_info oei                           \n               "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v("  oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("data_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" bu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id                           \n               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_left "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2875")]),s._v("                           \n               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("node_right "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2875")]),s._v("                           \n               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("and")]),s._v(" oei"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("org_category "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("                \n         "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("where")]),s._v("\n            c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" cb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("contact_id           \n      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("        \n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("order")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v("\n      c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("created_time "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("desc")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("limit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n      "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nEmpty "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("set")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v(" min "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("18.99")]),s._v(" sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("2 min 18.99 sec！比之前的情况还糟糕很多。由于mysql的nested loop机制，遇到这种情况，基本是无法优化的。这条语句最终也只能交给应用系统去优化自己的逻辑了。")]),s._v(" "),a("p",[s._v("通过这个例子我们可以看到，并不是所有语句都能优化，而往往我们优化时，由于SQL用例回归时落掉一些极端情况，会造成比原来还严重的后果。所以，第一：不要指望所有语句都能通过SQL优化，第二：不要过于自信，只针对具体case来优化，而忽略了更复杂的情况。")]),s._v(" "),a("p",[s._v("慢查询的案例就分析到这儿，以上只是一些比较典型的案例。我们在优化过程中遇到过超过1000行，涉及到16个表join的“垃圾SQL”，也遇到过线上线下数据库差异导致应用直接被慢查询拖死，也遇到过varchar等值比较没有写单引号，还遇到过笛卡尔积查询直接把从库搞死。再多的案例其实也只是一些经验的积累，如果我们熟悉查询优化器、索引的内部原理，那么分析这些案例就变得特别简单了。")]),s._v(" "),a("h2",{attrs:{id:"写在后面的话"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#写在后面的话"}},[s._v("#")]),s._v(" 写在后面的话")]),s._v(" "),a("p",[s._v("本文以一个慢查询案例引入了MySQL索引原理、优化慢查询的一些方法论;并针对遇到的典型案例做了详细的分析。其实做了这么长时间的语句优化后才发现，任何数据库层面的优化都抵不上应用系统的优化，同样是MySQL，可以用来支撑Google/FaceBook/Taobao应用，但可能连你的个人网站都撑不住。套用最近比较流行的话：“查询容易，优化不易，且写且珍惜！”")]),s._v(" "),a("h2",{attrs:{id:"参考文献"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文献"}},[s._v("#")]),s._v(" 参考文献：")]),s._v(" "),a("p",[s._v("1.《高性能MySQL》 2.《数据结构与算法分析》")]),s._v(" "),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[s._v("#")]),s._v(" 参考文章")]),s._v(" "),a("ul",[a("li",[s._v("https://www.cnblogs.com/yungyu16/p/12952351.html")])])])}),[],!1,null,null,null);t.default=e.exports}}]);