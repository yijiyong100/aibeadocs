(window.webpackJsonp=window.webpackJsonp||[]).push([[329],{844:function(s,t,a){"use strict";a.r(t);var e=a(53),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[s._v("TIP")]),s._v(" "),a("p",[s._v("本文主要是介绍 Flume-Sqoop和Flume案例。")])]),s._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#前言"}},[s._v("前言")])]),a("li",[a("a",{attrs:{href:"#_1-flume日志采集框架"}},[s._v("1 Flume日志采集框架")])]),a("li",[a("a",{attrs:{href:"#_1-1-flume介绍"}},[s._v("1.1 Flume介绍")]),a("ul",[a("li",[a("a",{attrs:{href:"#_1-1-1-概述"}},[s._v("1.1.1 概述")])]),a("li",[a("a",{attrs:{href:"#_1-1-2-运行机制"}},[s._v("1.1.2 运行机制")])]),a("li",[a("a",{attrs:{href:"#_1-1-3-flume采集系统结构图"}},[s._v("1.1.3 Flume采集系统结构图")])])])]),a("li",[a("a",{attrs:{href:"#_1-2-flume实战案例"}},[s._v("1.2 Flume实战案例")]),a("ul",[a("li",[a("a",{attrs:{href:"#_1-2-1-flume的安装部署"}},[s._v("1.2.1 Flume的安装部署")])]),a("li",[a("a",{attrs:{href:"#_1-2-2-采集案例"}},[s._v("1.2.2 采集案例")])])])]),a("li",[a("a",{attrs:{href:"#_1-3-更多source和sink组件"}},[s._v("1.3 更多source和sink组件")])]),a("li",[a("a",{attrs:{href:"#_2-sqoop数据迁移工具"}},[s._v("2 sqoop数据迁移工具")])]),a("li",[a("a",{attrs:{href:"#_2-1-概述"}},[s._v("2.1 概述")])]),a("li",[a("a",{attrs:{href:"#_2-2-工作机制"}},[s._v("2.2 工作机制")])]),a("li",[a("a",{attrs:{href:"#_2-3-sqoop实战及原理"}},[s._v("2.3 sqoop实战及原理")]),a("ul",[a("li",[a("a",{attrs:{href:"#_2-3-1-sqoop安装"}},[s._v("2.3.1 sqoop安装")])])])]),a("li",[a("a",{attrs:{href:"#_2-4-sqoop的数据导入"}},[s._v("2.4 Sqoop的数据导入")]),a("ul",[a("li",[a("a",{attrs:{href:"#_2-4-1-语法"}},[s._v("2.4.1 语法")])]),a("li",[a("a",{attrs:{href:"#_2-4-2-示例"}},[s._v("2.4.2 示例")])])])]),a("li",[a("a",{attrs:{href:"#_2-5-sqoop的数据导出"}},[s._v("2.5 Sqoop的数据导出")]),a("ul",[a("li",[a("a",{attrs:{href:"#语法"}},[s._v("语法")])]),a("li",[a("a",{attrs:{href:"#示例"}},[s._v("示例")])])])]),a("li",[a("a",{attrs:{href:"#更多综合案例参见"}},[s._v("更多综合案例参见:")])]),a("li",[a("a",{attrs:{href:"#参考文章"}},[s._v("参考文章")])])])]),a("p"),s._v(" "),a("h2",{attrs:{id:"前言"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[s._v("#")]),s._v(" 前言")]),s._v(" "),a("p",[s._v("在一个完整的离线大数据处理系统中，除了hdfs+mapreduce+hive组成分析系统的核心之外，还需要数据采集、结果数据导出、任务调度等不可或缺的辅助系统，而这些辅助工具在hadoop生态体系中都有便捷的开源框架，如图所示：")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102152258290-2110277143.png"),alt:"wxmp"}}),s._v(" "),a("h2",{attrs:{id:"_1-flume日志采集框架"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-flume日志采集框架"}},[s._v("#")]),s._v(" 1 Flume日志采集框架")]),s._v(" "),a("h2",{attrs:{id:"_1-1-flume介绍"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-flume介绍"}},[s._v("#")]),s._v(" 1.1 Flume介绍")]),s._v(" "),a("h3",{attrs:{id:"_1-1-1-概述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-1-概述"}},[s._v("#")]),s._v(" 1.1.1 概述")]),s._v(" "),a("ul",[a("li",[s._v("Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。")]),s._v(" "),a("li",[s._v("Flume可以采集文件，socket数据包、文件、文件夹、kafka等各种形式源数据，又可以将采集到的数据(下沉sink)输出到HDFS、hbase、hive、kafka等众多外部存储系统中")]),s._v(" "),a("li",[s._v("一般的采集需求，通过对flume的简单配置即可实现")]),s._v(" "),a("li",[s._v("Flume针对特殊场景也具备良好的自定义扩展能力")])]),s._v(" "),a("p",[s._v("因此，flume可以适用于大部分的日常数据采集场景。")]),s._v(" "),a("h3",{attrs:{id:"_1-1-2-运行机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-2-运行机制"}},[s._v("#")]),s._v(" 1.1.2 运行机制")]),s._v(" "),a("p",[s._v("1 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成。\n2 每一个agent相当于一个数据传递员（Source 到 Channel 到 Sink之间传递数据的形式是Event事件，Event事件是一个数据流单元），内部有3个组件：")]),s._v(" "),a("p",[s._v("a) Source：采集组件，用于跟数据源对接，以获取数据")]),s._v(" "),a("p",[s._v("b) Sink：下沉组件，用于往下一级agent传递数据或者往最终存储系统传递数据")]),s._v(" "),a("p",[s._v("c) Channel：传输通道组件，用于从source将数据传递到sink")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102152557298-1375120858.png"),alt:"wxmp"}}),s._v(" "),a("h3",{attrs:{id:"_1-1-3-flume采集系统结构图"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-3-flume采集系统结构图"}},[s._v("#")]),s._v(" 1.1.3 Flume采集系统结构图")]),s._v(" "),a("p",[s._v("1 简单结构——单个agent采集数据")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102153004391-1928945539.png"),alt:"wxmp"}}),s._v(" "),a("p",[s._v("2 复杂结构——多级agent之间串联")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102153129430-1042627131.png"),alt:"wxmp"}}),s._v(" "),a("h2",{attrs:{id:"_1-2-flume实战案例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-flume实战案例"}},[s._v("#")]),s._v(" 1.2 Flume实战案例")]),s._v(" "),a("h3",{attrs:{id:"_1-2-1-flume的安装部署"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-1-flume的安装部署"}},[s._v("#")]),s._v(" 1.2.1 Flume的安装部署")]),s._v(" "),a("p",[s._v("1、Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境。上传安装包到数据源所在节点上")]),s._v(" "),a("p",[s._v("然后解压  tar -zxvf apache-flume-1.6.0-bin.tar.gz")]),s._v(" "),a("p",[s._v("然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME")]),s._v(" "),a("p",[s._v("2、根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)")]),s._v(" "),a("p",[s._v("3、指定采集方案配置文件，在相应的节点上启动flume agent")]),s._v(" "),a("p",[s._v("先用一个最简单的例子来测试一下程序环境是否正常")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102153344338-1889226770.png"),alt:"wxmp"}}),s._v(" "),a("p",[s._v("1、先在flume的conf目录下新建一个配置文件（采集方案）")]),s._v(" "),a("p",[s._v("vi  netcat-logger.properties")]),s._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 定义这个agent中各组件的名字")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sources")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" r1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sinks")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" k1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" c1")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 描述和配置source组件：r1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sources.r1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" netcat")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sources.r1.bind")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" localhost")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sources.r1.port")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 44444")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 描述和配置sink组件：k1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sinks.k1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" logger")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 描述和配置channel组件，此处使用是内存缓存的方式")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.channels.c1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" memory")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.channels.c1.capacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 1000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.channels.c1.transactionCapacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 100")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 描述和配置source  channel   sink之间的连接关系")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sources.r1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" c1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("a1.sinks.k1.channel")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" c1")]),s._v("\n")])])]),a("p",[s._v("2、启动agent去采集数据")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1  -Dflume.root.logger"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("INFO,console\n")])])]),a("p",[s._v("-c conf  指定flume自身的配置文件所在目录")]),s._v(" "),a("p",[s._v("-f conf/netcat-logger.con  指定我们所描述的采集方案")]),s._v(" "),a("p",[s._v("-n a1  指定我们这个agent的名字")]),s._v(" "),a("p",[s._v("3、测试")]),s._v(" "),a("p",[s._v("先要往agent的source所监听的端口上发送数据，让agent有数据可采。随便在一个能跟agent节点联网的机器上。")]),s._v(" "),a("p",[s._v("telnet anget-hostname  port  （telnet localhost 44444）")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102153643294-1151864309.png"),alt:"wxmp"}}),s._v(" "),a("h3",{attrs:{id:"_1-2-2-采集案例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-2-采集案例"}},[s._v("#")]),s._v(" 1.2.2 采集案例")]),s._v(" "),a("h5",{attrs:{id:"_1、采集日志目录中的文件到hdfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、采集日志目录中的文件到hdfs"}},[s._v("#")]),s._v(" 1、采集日志目录中的文件到HDFS")]),s._v(" "),a("p",[s._v("结构示意图：")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102153735943-119975266.png"),alt:"wxmp"}}),s._v(" "),a("p",[s._v("采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去")]),s._v(" "),a("p",[s._v("根据需求，首先定义以下3大要素")]),s._v(" "),a("p",[s._v("1 数据源组件，即source ——监控文件目录 :  spooldir")]),s._v(" "),a("p",[s._v("spooldir特性：")]),s._v(" "),a("p",[s._v("1、监视一个目录，只要目录中出现新文件，就会采集文件中的内容")]),s._v(" "),a("p",[s._v("2、采集完成的文件，会被agent自动添加一个后缀：COMPLETED")]),s._v(" "),a("p",[s._v("3、所监视的目录中不允许重复出现相同文件名的文件")]),s._v(" "),a("p",[s._v("1 下沉组件，即sink——HDFS文件系统  :  hdfs sink")]),s._v(" "),a("p",[s._v("2 通道组件，即channel——可用file channel 也可以用内存channel")]),s._v(" "),a("p",[s._v("配置文件编写：")]),s._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#定义三大组件的名称")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" source1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" sink1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 配置source组件")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" spooldir")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.spoolDir")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" /home/hadoop/logs/")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.fileHeader")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" false")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#配置拦截器")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" i1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors.i1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" host")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors.i1.hostHeader")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" hostname")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 配置sink组件")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" hdfs")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.path")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v("hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.filePrefix")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" access_log")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.maxOpenFiles")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 5000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.batchSize")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 100")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.fileType")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" DataStream")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.writeFormat")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v("Text")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollSize")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 102400")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollCount")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 1000000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollInterval")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 60")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#agent1.sinks.sink1.hdfs.round = true")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#agent1.sinks.sink1.hdfs.roundValue = 10")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#agent1.sinks.sink1.hdfs.roundUnit = minute")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.useLocalTimeStamp")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" true")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Use a channel which buffers events in memory")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" memory")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.keep-alive")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 120")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.capacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 500000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.transactionCapacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 600")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Bind the source and sink to the channel")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.channel")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n")])])]),a("p",[s._v("Channel参数解释：")]),s._v(" "),a("p",[s._v("capacity：默认该通道中最大的可以存储的event数量")]),s._v(" "),a("p",[s._v("trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量")]),s._v(" "),a("p",[s._v("keep-alive：event添加到通道中或者移出的允许时间")]),s._v(" "),a("p",[s._v("测试阶段，启动flume agent的命令：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/flume-ng  agent  -c  ./conf  -f ./dir-hdfs.conf -n  agent1 -Dflume.root.logger"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("DEBUG,console\n")])])]),a("p",[s._v("-D后面跟的是log4j的参数，用于测试观察")]),s._v(" "),a("p",[s._v("生产中，启动flume，应该把flume启动在后台：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[s._v("nohup")]),s._v(" bin/flume-ng  agent  -c  ./conf  -f ./dir-hdfs.conf -n  agent1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("1")]),s._v(">")]),s._v("/dev/null "),a("span",{pre:!0,attrs:{class:"token operator"}},[a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("2")]),s._v(">")]),a("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[s._v("&1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("&")]),s._v("\n")])])]),a("p",[s._v("2、采集文件到HDFS")]),s._v(" "),a("p",[s._v("采集需求：比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs")]),s._v(" "),a("p",[s._v("根据需求，首先定义以下3大要素")]),s._v(" "),a("p",[s._v("1 采集源，即source——监控文件内容更新 :  exec  ‘tail -F file’\n2 下沉目标，即sink——HDFS文件系统  :  hdfs sink\n3 Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel")]),s._v(" "),a("p",[s._v("配置文件编写：")]),s._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" source1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" sink1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Describe/configure tail -F source1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" exec")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.command")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" tail -F /home/hadoop/logs/access_log")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#configure host for source")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" i1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors.i1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" host")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.interceptors.i1.hostHeader")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" hostname")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Describe sink1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" hdfs")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#a1.sinks.k1.channel = c1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.path")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v("hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.filePrefix")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" access_log")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.maxOpenFiles")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 5000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.batchSize")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 100")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.fileType")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" DataStream")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.writeFormat")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v("Text")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollSize")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 102400")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollCount")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 1000000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.rollInterval")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 60")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.round")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" true")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.roundValue")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 10")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.roundUnit")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" minute")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.hdfs.useLocalTimeStamp")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" true")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Use a channel which buffers events in memory")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.type")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" memory")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.keep-alive")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 120")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.capacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 500000")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.channels.channel1.transactionCapacity")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" 600")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# Bind the source and sink to the channel")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sources.source1.channels")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[s._v("agent1.sinks.sink1.channel")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("=")]),s._v(" channel1")]),s._v("\n")])])]),a("p",[s._v("3、两个agent级联")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102154357179-1200598944.png"),alt:"wxmp"}}),s._v(" "),a("h2",{attrs:{id:"_1-3-更多source和sink组件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-更多source和sink组件"}},[s._v("#")]),s._v(" 1.3 更多source和sink组件")]),s._v(" "),a("p",[s._v("Flume支持众多的source和sink类型，详细手册可参考官方文档 http://flume.apache.org/FlumeUserGuide.html")]),s._v(" "),a("h2",{attrs:{id:"_2-sqoop数据迁移工具"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-sqoop数据迁移工具"}},[s._v("#")]),s._v(" 2 sqoop数据迁移工具")]),s._v(" "),a("h2",{attrs:{id:"_2-1-概述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-概述"}},[s._v("#")]),s._v(" 2.1 概述")]),s._v(" "),a("p",[s._v("sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。")]),s._v(" "),a("p",[s._v("导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；")]),s._v(" "),a("p",[s._v("导出数据：从Hadoop的文件系统中导出数据到关系数据库mysql等")]),s._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/flume/case1/1478220-20181102154726780-1161515173.png"),alt:"wxmp"}}),s._v(" "),a("h2",{attrs:{id:"_2-2-工作机制"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-工作机制"}},[s._v("#")]),s._v(" 2.2 工作机制")]),s._v(" "),a("p",[s._v("将导入或导出命令翻译成mapreduce程序来实现，在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。")]),s._v(" "),a("h2",{attrs:{id:"_2-3-sqoop实战及原理"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-sqoop实战及原理"}},[s._v("#")]),s._v(" 2.3 sqoop实战及原理")]),s._v(" "),a("h3",{attrs:{id:"_2-3-1-sqoop安装"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-1-sqoop安装"}},[s._v("#")]),s._v(" 2.3.1 sqoop安装")]),s._v(" "),a("p",[s._v("安装sqoop的前提是已经具备java和hadoop的环境")]),s._v(" "),a("h4",{attrs:{id:"_1、下载并解压"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1、下载并解压"}},[s._v("#")]),s._v(" 1、下载并解压")]),s._v(" "),a("p",[s._v("最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/")]),s._v(" "),a("h4",{attrs:{id:"_2、修改配置文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2、修改配置文件"}},[s._v("#")]),s._v(" 2、修改配置文件")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("\n　　　　cdcdSQOOP_HOME/conf\n\n　　　　$ "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("mv")]),s._v(" sqoop-env-template.sh sqoop-env.sh\n")])])]),a("p",[s._v("打开sqoop-env.sh并编辑下面几行：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_COMMON_HOME")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hadoop-2.6.1/ \n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_MAPRED_HOME")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hadoop-2.6.1/\n"),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HIVE_HOME")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hive-1.2.1\n")])])]),a("h4",{attrs:{id:"_3、加入mysql的jdbc驱动包"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3、加入mysql的jdbc驱动包"}},[s._v("#")]),s._v(" 3、加入mysql的jdbc驱动包")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("\n　　　　"),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("cp")]),s._v("  ~/app/hive/lib/mysql-connector-java-5.1.28.jar  "),a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/lib/\n")])])]),a("h4",{attrs:{id:"_4、验证启动"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4、验证启动"}},[s._v("#")]),s._v(" 4、验证启动")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("\n　　　　cdcdSQOOP_HOME/bin\n\n　　　　$ sqoop-version\n")])])]),a("p",[s._v("预期的输出：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("\n　　　　"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v("/12/17 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":52:32 INFO sqoop.Sqoop: Running Sqoop version: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6\n\n　　　　Sqoop "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6 "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("git")]),s._v(" commit "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" 5b34accaca7de251fc91161733f906af2eddbe83\n\n　　　　Compiled by abe on Fri Aug "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("11")]),s._v(":19:26 PDT "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2015")]),s._v("\n")])])]),a("p",[s._v("到这里，整个Sqoop安装工作完成。")]),s._v(" "),a("p",[s._v("验证sqoop到mysql业务库之间的连通性：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("\n　　　　bin/sqoop-list-databases --connect jdbc:mysql://localhost:3306 --username root --password root\n\n　　　　bin/sqoop-list-tables --connect jdbc:mysql://localhost:3306/userdb --username root --password root\n")])])]),a("h2",{attrs:{id:"_2-4-sqoop的数据导入"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-sqoop的数据导入"}},[s._v("#")]),s._v(" 2.4 Sqoop的数据导入")]),s._v(" "),a("p",[s._v("“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据）")]),s._v(" "),a("h3",{attrs:{id:"_2-4-1-语法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-1-语法"}},[s._v("#")]),s._v(" 2.4.1 语法")]),s._v(" "),a("p",[s._v("下面的语法用于将数据导入HDFS")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("$ sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("generic-args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("import-args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])])]),a("h3",{attrs:{id:"_2-4-2-示例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-2-示例"}},[s._v("#")]),s._v(" 2.4.2 示例")]),s._v(" "),a("h4",{attrs:{id:"表数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#表数据"}},[s._v("#")]),s._v(" 表数据")]),s._v(" "),a("p",[s._v("在mysql中有一个库userdb中三个表：emp, emp_add和emp_conn")]),s._v(" "),a("p",[s._v("表emp:")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("id")]),s._v(" "),a("th",[s._v("name")]),s._v(" "),a("th",[s._v("deg")]),s._v(" "),a("th",[s._v("salary")]),s._v(" "),a("th",[s._v("dept")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("1201")]),s._v(" "),a("td",[s._v("gopal")]),s._v(" "),a("td",[s._v("manager")]),s._v(" "),a("td",[s._v("50,000")]),s._v(" "),a("td",[s._v("TP")])]),s._v(" "),a("tr",[a("td",[s._v("1202")]),s._v(" "),a("td",[s._v("manisha")]),s._v(" "),a("td",[s._v("Proof reader")]),s._v(" "),a("td",[s._v("50,000")]),s._v(" "),a("td",[s._v("TP")])]),s._v(" "),a("tr",[a("td",[s._v("1203")]),s._v(" "),a("td",[s._v("khalil")]),s._v(" "),a("td",[s._v("php dev")]),s._v(" "),a("td",[s._v("30,000")]),s._v(" "),a("td",[s._v("AC")])]),s._v(" "),a("tr",[a("td",[s._v("1204")]),s._v(" "),a("td",[s._v("prasanth")]),s._v(" "),a("td",[s._v("php dev")]),s._v(" "),a("td",[s._v("30,000")]),s._v(" "),a("td",[s._v("AC")])]),s._v(" "),a("tr",[a("td",[s._v("1205")]),s._v(" "),a("td",[s._v("kranthi")]),s._v(" "),a("td",[s._v("admin")]),s._v(" "),a("td",[s._v("20,000")]),s._v(" "),a("td",[s._v("TP")])])])]),s._v(" "),a("p",[s._v("表emp_add:")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("id")]),s._v(" "),a("th",[s._v("hno")]),s._v(" "),a("th",[s._v("street")]),s._v(" "),a("th",[s._v("city")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("1201")]),s._v(" "),a("td",[s._v("288A")]),s._v(" "),a("td",[s._v("vgiri")]),s._v(" "),a("td",[s._v("jublee")])]),s._v(" "),a("tr",[a("td",[s._v("1202")]),s._v(" "),a("td",[s._v("108I")]),s._v(" "),a("td",[s._v("aoc")]),s._v(" "),a("td",[s._v("sec-bad")])]),s._v(" "),a("tr",[a("td",[s._v("1203")]),s._v(" "),a("td",[s._v("144Z")]),s._v(" "),a("td",[s._v("pgutta")]),s._v(" "),a("td",[s._v("hyd")])]),s._v(" "),a("tr",[a("td",[s._v("1204")]),s._v(" "),a("td",[s._v("78B")]),s._v(" "),a("td",[s._v("old city")]),s._v(" "),a("td",[s._v("sec-bad")])]),s._v(" "),a("tr",[a("td",[s._v("1205")]),s._v(" "),a("td",[s._v("720X")]),s._v(" "),a("td",[s._v("hitec")]),s._v(" "),a("td",[s._v("sec-bad")])])])]),s._v(" "),a("p",[s._v("表emp_conn:")]),s._v(" "),a("table",[a("thead",[a("tr",[a("th",[s._v("id")]),s._v(" "),a("th",[s._v("phno")]),s._v(" "),a("th",[s._v("email")])])]),s._v(" "),a("tbody",[a("tr",[a("td",[s._v("1201")]),s._v(" "),a("td",[s._v("2356742")]),s._v(" "),a("td",[s._v("gopal@tp.com")])]),s._v(" "),a("tr",[a("td",[s._v("1202")]),s._v(" "),a("td",[s._v("1661663")]),s._v(" "),a("td",[s._v("manisha@tp.com")])]),s._v(" "),a("tr",[a("td",[s._v("1203")]),s._v(" "),a("td",[s._v("8887776")]),s._v(" "),a("td",[s._v("khalil@ac.com")])]),s._v(" "),a("tr",[a("td",[s._v("1204")]),s._v(" "),a("td",[s._v("9988774")]),s._v(" "),a("td",[s._v("prasanth@ac.com")])]),s._v(" "),a("tr",[a("td",[s._v("1205")]),s._v(" "),a("td",[s._v("1231231")]),s._v(" "),a("td",[s._v("kranthi@tp.com")])])])]),s._v(" "),a("h4",{attrs:{id:"导入表表数据到hdfs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#导入表表数据到hdfs"}},[s._v("#")]),s._v(" 导入表表数据到HDFS")]),s._v(" "),a("p",[s._v("下面的命令用于从MySQL数据库服务器中的emp表导入HDFS")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("--connect jdbc:mysql://hdp-node-01:3306/test   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("--username root  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("--password root   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("--table emp   "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("--m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("  \n")])])]),a("p",[s._v("如果成功执行，那么会得到下面的输出")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":24:54 INFO sqoop.Sqoop: Running Sqoop version: "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".5\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":24:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\nINFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cebe706d23ebb1fd99c1f063ad51ebd7/emp.jar\n-----------------------------------------------------\nO mapreduce.Job: map "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("% reduce "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("%\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":28:08 INFO mapreduce.Job: map "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("% reduce "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("%\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":28:16 INFO mapreduce.Job: Job job_1419242001831_0001 completed successfully\n-----------------------------------------------------\n-----------------------------------------------------\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":28:17 INFO mapreduce.ImportJobBase: Transferred "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("145")]),s._v(" bytes "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("177.5849")]),s._v(" seconds "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8165")]),s._v(" bytes/sec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v("/12/22 "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("15")]),s._v(":28:17 INFO mapreduce.ImportJobBase: Retrieved "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v(" records.\n")])])]),a("p",[s._v("为了验证在HDFS导入的数据，请使用以下命令查看导入的数据")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("$ "),a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000\n")])])]),a("p",[s._v("emp表的数据和字段之间用逗号(,)表示。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v("1201, gopal,    manager, 50000, TP\n1202, manisha,  preader, 50000, TP\n1203, kalil,    php dev, 30000, AC\n1204, prasanth, php dev, 30000, AC\n1205, kranthi,  admin,   20000, TP\n")])])]),a("h4",{attrs:{id:"导入到hdfs指定目录"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#导入到hdfs指定目录"}},[s._v("#")]),s._v(" 导入到HDFS指定目录")]),s._v(" "),a("p",[s._v("在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。")]),s._v(" "),a("p",[s._v("以下是指定目标目录选项的Sqoop导入命令的语法。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("--target-dir "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("new or exist directory "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" HDFS"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),a("p",[s._v("下面的命令是用来导入emp_add表数据到'/queryresult'目录。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /queryresult "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--fields-terminated-by ‘"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("001’ "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp \n--split-by "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v("\n--m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),a("p",[s._v("注意：如果报错，说emp类找不到，则可以手动从sqoop生成的编译目录(/tmp/sqoop-root/compile)中，找到这个emp.class和emp.jar，拷贝到sqoop的lib目录下：")]),s._v(" "),a("p",[s._v("如果设置了 --m 1，则意味着只会启动一个maptask执行数据导入")]),s._v(" "),a("p",[s._v("如果不设置 --m 1，则默认为启动4个map task执行数据导入，则需要指定一个列来作为划分map task任务的依据")]),s._v(" "),a("p",[s._v("下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。")]),s._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[s._v(" $HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*\n")])])]),a("p",[s._v("它会用逗号（，）分隔emp_add表的数据和字段。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", 288A, vgiri,   jublee\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", 108I, aoc,     sec-bad\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", 144Z, pgutta,  hyd\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", 78B,  oldcity, sec-bad\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", 720C, hitech,  sec-bad\n")])])]),a("h4",{attrs:{id:"导入关系表到hive"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#导入关系表到hive"}},[s._v("#")]),s._v(" 导入关系表到HIVE")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import  --split-by "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v("  --m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),a("h4",{attrs:{id:"导入表数据子集"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#导入表数据子集"}},[s._v("#")]),s._v(" 导入表数据子集")]),s._v(" "),a("p",[s._v('我们可以导入表的使用Sqoop导入工具，"where"子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。')]),s._v(" "),a("p",[s._v("where子句的语法如下。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("--where "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("condition"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),a("p",[s._v("下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--where "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("\"city ='sec-bad'\"")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /wherequery "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp_add "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n --m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),a("p",[s._v("按需导入")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /wherequery2 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--query "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'select id,name,deg from emp WHERE id>1207 and "),a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$CONDITIONS")]),s._v("'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--split-by "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--fields-terminated-by "),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'"),a("span",{pre:!0,attrs:{class:"token entity",title:"\\t"}},[s._v("\\t")]),s._v("'")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("\n")])])]),a("p",[s._v("下面的命令用来验证数据从emp_add表导入/wherequery目录")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /wherequery/part-m-*\n")])])]),a("p",[s._v("它用逗 分隔 emp_add表数据和字段。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", 108I, aoc, sec-bad\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", 78B, oldcity, sec-bad\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", 720C, hitech, sec-bad\n")])])]),a("h4",{attrs:{id:"增量导入"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#增量导入"}},[s._v("#")]),s._v(" 增量导入")]),s._v(" "),a("p",[s._v("增量导入是仅导入新添加的表中的行的技术。")]),s._v(" "),a("p",[s._v("sqoop支持两种增量MySql导入到hive的模式，")]),s._v(" "),a("p",[s._v("一种是append，即通过指定一个递增的列，比如：")]),s._v(" "),a("p",[s._v("--incremental append  --check-column num_id --last-value 0")]),s._v(" "),a("p",[s._v("另种是可以根据时间戳，比如：")]),s._v(" "),a("p",[s._v("--incremental lastmodified --check-column created --last-value '2012-02-01 11:0:00'")]),s._v(" "),a("p",[s._v("就是只导入created 比'2012-02-01 11:0:00'更大的数据。")]),s._v(" "),a("p",[s._v("1 append模式")]),s._v(" "),a("p",[s._v("它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。")]),s._v(" "),a("p",[s._v("下面的语法用于Sqoop导入命令增量选项。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("--incremental "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n--check-column "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("column name"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n--last value "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("last check "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("column")]),s._v(" value"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),a("p",[s._v("假设新添加的数据转换成emp表如下：")]),s._v(" "),a("p",[s._v("1206, satish p, grp des, 20000, GR")]),s._v(" "),a("p",[s._v("下面的命令用于在EMP表执行增量导入。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp --m "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--incremental append "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--check-column "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--last-value "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1208")]),s._v("\n")])])]),a("p",[s._v("以下命令用于从emp表导入HDFS emp/ 目录的数据验证。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("$ "),a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /user/hadoop/emp/part-m-*\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", gopal,    manager, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", manisha,  preader, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", kalil,    php dev, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", prasanth, php dev, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", kranthi,  admin,   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p, grp des, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),a("p",[s._v("下面的命令是从表emp 用来查看修改或新添加的行")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /emp/part-m-*1\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p, grp des, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),a("h2",{attrs:{id:"_2-5-sqoop的数据导出"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-sqoop的数据导出"}},[s._v("#")]),s._v(" 2.5 Sqoop的数据导出")]),s._v(" "),a("p",[s._v("1 将数据从HDFS把文件导出到RDBMS数据库")]),s._v(" "),a("p",[s._v("导出前，目标表必须存在于目标数据库中。")]),s._v(" "),a("p",[s._v("1 默认操作是从将文件中的数据使用INSERT语句插入到表中\n2 更新模式下，是生成UPDATE语句更新表数据")]),s._v(" "),a("h3",{attrs:{id:"语法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#语法"}},[s._v("#")]),s._v(" 语法")]),s._v(" "),a("p",[s._v("以下是export命令语法。")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("$ sqoop "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("generic-args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("export-args"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])])]),a("h3",{attrs:{id:"示例"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#示例"}},[s._v("#")]),s._v(" 示例")]),s._v(" "),a("p",[s._v("数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", gopal,     manager, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", manisha,   preader, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", kalil,     php dev, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", prasanth,  php dev, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", kranthi,   admin,   "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", TP\n"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p,  grp des, "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),a("p",[s._v("1、首先需要手动创建mysql中的目标表")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[s._v("$ mysql\nmysql"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("USE")]),s._v(" db"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nmysql"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CREATE")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("TABLE")]),s._v(" employee "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v(" \n   id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("NOT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("PRIMARY")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("KEY")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n   name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("VARCHAR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" \n   deg "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("VARCHAR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   salary "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("INT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("\n   dept "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("VARCHAR")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("2、然后执行导出命令")]),s._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[s._v("bin/sqoop "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table employee "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--export-dir /user/hadoop/emp/\n")])])]),a("p",[s._v("3、验证表mysql命令行。")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[s._v("mysql"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" employee"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n如果给定的数据存储成功，那么可以找到数据在如下的employee表。\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("------+--------------+-------------+-------------------+--------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Id   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Name         "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Designation "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Salary            "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Dept   "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("------+--------------+-------------+-------------------+--------+")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" gopal        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" manager     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" manisha      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" preader     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" kalil        "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" php dev     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v("               "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" AC     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" prasanth     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" php dev     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" AC     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" kranthi      "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" admin       "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" satish p     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" grp des     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v("             "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" GR     "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("------+--------------+-------------+-------------------+--------+")]),s._v("\n")])])]),a("h2",{attrs:{id:"更多综合案例参见"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#更多综合案例参见"}},[s._v("#")]),s._v(" 更多综合案例参见:")]),s._v(" "),a("p",[a("a",{attrs:{href:"https://www.cnblogs.com/fenghuoliancheng/p/10674911.html",target:"_blank",rel:"noopener noreferrer"}},[s._v("Flume+Sqoop+Azkaban笔记"),a("OutboundLink")],1)]),s._v(" "),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[s._v("#")]),s._v(" 参考文章")]),s._v(" "),a("ul",[a("li",[s._v("https://www.cnblogs.com/zhangchao162/p/9896805.html")]),s._v(" "),a("li",[s._v("https://www.cnblogs.com/fenghuoliancheng/p/10674911.html")])])])}),[],!1,null,null,null);t.default=n.exports}}]);