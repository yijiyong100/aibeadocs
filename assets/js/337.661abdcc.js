(window.webpackJsonp=window.webpackJsonp||[]).push([[337],{852:function(s,a,t){"use strict";t.r(a);var e=t(53),r=Object(e.a)({},(function(){var s=this,a=s.$createElement,t=s._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[t("div",{staticClass:"custom-block tip"},[t("p",{staticClass:"custom-block-title"},[s._v("TIP")]),s._v(" "),t("p",[s._v("本文主要是介绍 Sqoop-基础知识 。")])]),s._v(" "),t("p"),t("div",{staticClass:"table-of-contents"},[t("ul",[t("li",[t("a",{attrs:{href:"#sqoop知识点总结-全"}},[s._v("sqoop知识点总结(全)")])]),t("li",[t("a",{attrs:{href:"#_1、概述"}},[s._v("1、概述")])]),t("li",[t("a",{attrs:{href:"#_2、工作机制"}},[s._v("2、工作机制")])]),t("li",[t("a",{attrs:{href:"#_3、简单安装及使用"}},[s._v("3、简单安装及使用")]),t("ul",[t("li",[t("a",{attrs:{href:"#_3-1、下载并解压"}},[s._v("3.1、下载并解压")])]),t("li",[t("a",{attrs:{href:"#_3-2、修改配置文件"}},[s._v("3.2、修改配置文件")])]),t("li",[t("a",{attrs:{href:"#_3-3、加入mysql的jdbc驱动包"}},[s._v("3.3、加入mysql的jdbc驱动包")])]),t("li",[t("a",{attrs:{href:"#_3-4、验证启动"}},[s._v("3.4、验证启动")])])])]),t("li",[t("a",{attrs:{href:"#_4、sqoop的数据导入"}},[s._v("4、sqoop的数据导入")]),t("ul",[t("li",[t("a",{attrs:{href:"#_4-1、语法"}},[s._v("4.1、语法")])]),t("li",[t("a",{attrs:{href:"#_4-2、示例"}},[s._v("4.2、示例")])])])]),t("li",[t("a",{attrs:{href:"#_5、sqoop的数据导出"}},[s._v("5、Sqoop的数据导出")]),t("ul",[t("li",[t("a",{attrs:{href:"#_5-1、将数据从hdfs文件导出到rdbms数据库"}},[s._v("5.1、将数据从HDFS文件导出到RDBMS数据库")])])])]),t("li",[t("a",{attrs:{href:"#参考文章"}},[s._v("参考文章")])])])]),t("p"),s._v(" "),t("h2",{attrs:{id:"sqoop知识点总结-全"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sqoop知识点总结-全"}},[s._v("#")]),s._v(" sqoop知识点总结(全)")]),s._v(" "),t("h2",{attrs:{id:"_1、概述"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1、概述"}},[s._v("#")]),s._v(" 1、概述")]),s._v(" "),t("p",[s._v("sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。")]),s._v(" "),t("p",[s._v("Sqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。")]),s._v(" "),t("p",[s._v("导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；")]),s._v(" "),t("p",[s._v("导出数据：从Hadoop的文件系统中导出数据到关系数据库mysql等。")]),s._v(" "),t("img",{staticClass:"zoom-custom-imgs",attrs:{src:s.$withBase("/assets/img/dc/sqoop/intro-1.png"),alt:"wxmp"}}),s._v(" "),t("h2",{attrs:{id:"_2、工作机制"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2、工作机制"}},[s._v("#")]),s._v(" 2、工作机制")]),s._v(" "),t("p",[s._v("将导入或导出命令翻译成mapreduce程序来实现。")]),s._v(" "),t("p",[s._v("在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。")]),s._v(" "),t("h2",{attrs:{id:"_3、简单安装及使用"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3、简单安装及使用"}},[s._v("#")]),s._v(" 3、简单安装及使用")]),s._v(" "),t("p",[s._v("安装sqoop的前提是已经具备java和hadoop的环境。")]),s._v(" "),t("h3",{attrs:{id:"_3-1、下载并解压"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1、下载并解压"}},[s._v("#")]),s._v(" 3.1、下载并解压")]),s._v(" "),t("p",[s._v("最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/")]),s._v(" "),t("h3",{attrs:{id:"_3-2、修改配置文件"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-2、修改配置文件"}},[s._v("#")]),s._v(" 3.2、修改配置文件")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("cd")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/conf\n$ "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("mv")]),s._v(" sqoop-env-template.sh sqoop-env.sh\n")])])]),t("p",[s._v("打开sqoop-env.sh并编辑下面几行：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_COMMON_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hadoop-2.6.1/\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HADOOP_MAPRED_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hadoop-2.6.1/\n"),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token assign-left variable"}},[s._v("HIVE_HOME")]),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("/home/hadoop/apps/hive-1.2.1\n")])])]),t("h3",{attrs:{id:"_3-3、加入mysql的jdbc驱动包"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-3、加入mysql的jdbc驱动包"}},[s._v("#")]),s._v(" 3.3、加入mysql的jdbc驱动包")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token function"}},[s._v("cp")]),s._v("  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/lib/\n")])])]),t("h3",{attrs:{id:"_3-4、验证启动"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-4、验证启动"}},[s._v("#")]),s._v(" 3.4、验证启动")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("cd")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$SQOOP_HOME")]),s._v("/bin\n$ sqoop-version\n")])])]),t("p",[s._v("预期的输出：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("18")]),s._v("/07/01 "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("14")]),s._v(":52:32 INFO sqoop.Sqoop: Running Sqoop version: "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6\nSqoop "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".6 "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("git")]),s._v(" commit "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" 5b34accaca7de251fc91161733f906af2eddbe83\nCompiled by abe on Fri Aug "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("11")]),s._v(":19:26 PDT "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2018")]),s._v("\n")])])]),t("p",[s._v("到这里，整个Sqoop安装工作完成。")]),s._v(" "),t("p",[s._v("验证sqoop到mysql业务库之间的连通性：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop-list-databases --connect jdbc:mysql://localhost:3306 --username root --password root\nbin/sqoop-list-tables --connect jdbc:mysql://localhost:3306/userdb --username root --password root\n")])])]),t("h2",{attrs:{id:"_4、sqoop的数据导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4、sqoop的数据导入"}},[s._v("#")]),s._v(" 4、sqoop的数据导入")]),s._v(" "),t("p",[s._v("导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据）")]),s._v(" "),t("h3",{attrs:{id:"_4-1、语法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-1、语法"}},[s._v("#")]),s._v(" 4.1、语法")]),s._v(" "),t("p",[s._v("下面的语法用于将数据导入HDFS：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("generic-args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("import-args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])])]),t("h3",{attrs:{id:"_4-2、示例"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2、示例"}},[s._v("#")]),s._v(" 4.2、示例")]),s._v(" "),t("p",[s._v("表数据在mysql中有一个库userdb中三个表：emp, emp_add和emp_conn")]),s._v(" "),t("p",[s._v("表emp:")]),s._v(" "),t("table",[t("thead",[t("tr",[t("th",[t("em",[t("strong",[s._v("*id*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*name*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*deg*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*salary*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*dept*")])])])])]),s._v(" "),t("tbody",[t("tr",[t("td",[s._v("1201")]),s._v(" "),t("td",[s._v("gopal")]),s._v(" "),t("td",[s._v("manager")]),s._v(" "),t("td",[s._v("50,000")]),s._v(" "),t("td",[s._v("TP")])]),s._v(" "),t("tr",[t("td",[s._v("1202")]),s._v(" "),t("td",[s._v("manisha")]),s._v(" "),t("td",[s._v("Proof reader")]),s._v(" "),t("td",[s._v("50,000")]),s._v(" "),t("td",[s._v("TP")])]),s._v(" "),t("tr",[t("td",[s._v("1203")]),s._v(" "),t("td",[s._v("khalil")]),s._v(" "),t("td",[s._v("php dev")]),s._v(" "),t("td",[s._v("30,000")]),s._v(" "),t("td",[s._v("AC")])]),s._v(" "),t("tr",[t("td",[s._v("1204")]),s._v(" "),t("td",[s._v("prasanth")]),s._v(" "),t("td",[s._v("php dev")]),s._v(" "),t("td",[s._v("30,000")]),s._v(" "),t("td",[s._v("AC")])]),s._v(" "),t("tr",[t("td",[s._v("1205")]),s._v(" "),t("td",[s._v("kranthi")]),s._v(" "),t("td",[s._v("admin")]),s._v(" "),t("td",[s._v("20,000")]),s._v(" "),t("td",[s._v("TP")])])])]),s._v(" "),t("p",[s._v("表emp_add:")]),s._v(" "),t("table",[t("thead",[t("tr",[t("th",[t("em",[t("strong",[s._v("*id*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*hno*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*street*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*city*")])])])])]),s._v(" "),t("tbody",[t("tr",[t("td",[s._v("1201")]),s._v(" "),t("td",[s._v("288A")]),s._v(" "),t("td",[s._v("vgiri")]),s._v(" "),t("td",[s._v("jublee")])]),s._v(" "),t("tr",[t("td",[s._v("1202")]),s._v(" "),t("td",[s._v("108I")]),s._v(" "),t("td",[s._v("aoc")]),s._v(" "),t("td",[s._v("sec-bad")])]),s._v(" "),t("tr",[t("td",[s._v("1203")]),s._v(" "),t("td",[s._v("144Z")]),s._v(" "),t("td",[s._v("pgutta")]),s._v(" "),t("td",[s._v("hyd")])]),s._v(" "),t("tr",[t("td",[s._v("1204")]),s._v(" "),t("td",[s._v("78B")]),s._v(" "),t("td",[s._v("old city")]),s._v(" "),t("td",[s._v("sec-bad")])]),s._v(" "),t("tr",[t("td",[s._v("1205")]),s._v(" "),t("td",[s._v("720X")]),s._v(" "),t("td",[s._v("hitec")]),s._v(" "),t("td",[s._v("sec-bad")])])])]),s._v(" "),t("p",[s._v("表emp_conn：")]),s._v(" "),t("table",[t("thead",[t("tr",[t("th",[t("em",[t("strong",[s._v("*id*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*phno*")])])]),s._v(" "),t("th",[t("em",[t("strong",[s._v("*email*")])])])])]),s._v(" "),t("tbody",[t("tr",[t("td",[s._v("1201")]),s._v(" "),t("td",[s._v("2356742")]),s._v(" "),t("td",[s._v("gopal@tp.com")])]),s._v(" "),t("tr",[t("td",[s._v("1202")]),s._v(" "),t("td",[s._v("1661663")]),s._v(" "),t("td",[s._v("manisha@tp.com")])]),s._v(" "),t("tr",[t("td",[s._v("1203")]),s._v(" "),t("td",[s._v("8887776")]),s._v(" "),t("td",[s._v("khalil@ac.com")])]),s._v(" "),t("tr",[t("td",[s._v("1204")]),s._v(" "),t("td",[s._v("9988774")]),s._v(" "),t("td",[s._v("prasanth@ac.com")])]),s._v(" "),t("tr",[t("td",[s._v("1205")]),s._v(" "),t("td",[s._v("1231231")]),s._v(" "),t("td",[s._v("kranthi@tp.com")])])])]),s._v(" "),t("h4",{attrs:{id:"_4-2-1、导入表数据到hdfs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-1、导入表数据到hdfs"}},[s._v("#")]),s._v(" 4.2.1、导入表数据到HDFS")]),s._v(" "),t("p",[s._v("下面的命令用于从MySQL数据库服务器中的emp表导入HDFS：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v("   "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test   "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root  "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root   "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp   "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("  \n")])])]),t("p",[s._v("如果成功执行，那么会得到下面的输出：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("INFO sqoop.Sqoop: Running Sqoop version: "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.4")]),s._v(".5\nINFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\nINFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cebe706d23ebb1fd99c1f063ad51ebd7/emp.jar\n-----------------------------------------------------\nO mapreduce.Job: map "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("% reduce "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("%\nINFO mapreduce.Job: map "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),s._v("% reduce "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v("%\nINFO mapreduce.Job: Job job_1419242001831_0001 completed successfully\n-----------------------------------------------------\n-----------------------------------------------------\nINFO mapreduce.ImportJobBase: Transferred "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("145")]),s._v(" bytes "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("177.5849")]),s._v(" seconds "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.8165")]),s._v(" bytes/sec"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nINFO mapreduce.ImportJobBase: Retrieved "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v(" records.\n")])])]),t("p",[s._v("为了验证在HDFS导入的数据，请使用以下命令查看导入的数据：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000\n")])])]),t("p",[s._v("emp表的数据和字段之间用逗号(,)表示：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", gopal,    manager, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", manisha,  preader, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", kalil,    php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", prasanth, php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", kranthi,  admin,   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", TP\n")])])]),t("h4",{attrs:{id:"_4-2-2、导入到hdfs指定目录"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-2、导入到hdfs指定目录"}},[s._v("#")]),s._v(" 4.2.2、导入到HDFS指定目录")]),s._v(" "),t("p",[s._v("在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。")]),s._v(" "),t("p",[s._v("以下是指定目标目录选项的Sqoop导入命令的语法：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("--target-dir "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("new or exist directory "),t("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" HDFS"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),t("p",[s._v("下面的命令是用来导入emp_add表数据到'/queryresult'目录：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /queryresult "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("指定目录\n--fields-terminated-by ‘"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("001’ "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("指定分隔符\n--table emp \n--split-by "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v("\n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),t("p",[s._v("注意：如果报错，说emp类找不到，则可以手动从sqoop生成的编译目录(/tmp/sqoop-root/compile)中，找到这个emp.class和emp.jar，拷贝到sqoop的lib目录下。\n如果设置了 --m 1，则意味着只会启动一个maptask执行数据导入；\n如果不设置 --m 1，则默认为启动4个map task执行数据导入，则需要指定一个列来作为划分map task任务的依据。")]),s._v(" "),t("p",[s._v("下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /queryresult/part-m-*\n")])])]),t("p",[s._v("它会用逗号（，）分隔emp_add表的数据和字段：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", 288A, vgiri,   jublee\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", 108I, aoc,     sec-bad\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", 144Z, pgutta,  hyd\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", 78B,  oldcity, sec-bad\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", 720C, hitech,  sec-bad\n")])])]),t("h4",{attrs:{id:"_4-2-3、导入关系表到hive"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-3、导入关系表到hive"}},[s._v("#")]),s._v(" 4.2.3、导入关系表到HIVE")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" \n--connect jdbc:mysql://hdp-node-01:3306/test \n--username root \n--password root \n--table emp \n--hive-import  "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("增加导入hive声明\n--split-by "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v("  \n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),t("h4",{attrs:{id:"_4-2-4、导入表数据子集"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-4、导入表数据子集"}},[s._v("#")]),s._v(" 4.2.4、导入表数据子集")]),s._v(" "),t("p",[s._v('我们可以使用Sqoop导入工具导入表的"where"子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。')]),s._v(" "),t("p",[s._v("where子句的语法如下：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("--where "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("condition"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),t("p",[s._v("下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为（sec-bad）：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--where "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("\"city ='sec-bad'\"")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("子集条件声明\n--target-dir /wherequery "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp_add "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n --m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("\n")])])]),t("p",[s._v("按需导入：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--target-dir /wherequery2 "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--query "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'select id,name,deg from emp WHERE id>1207 and "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$CONDITIONS")]),s._v("'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--split-by "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--fields-terminated-by "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'"),t("span",{pre:!0,attrs:{class:"token entity",title:"\\t"}},[s._v("\\t")]),s._v("'")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),s._v("\n")])])]),t("p",[s._v("下面的命令用来验证数据从emp_add表导入/wherequery目录：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /wherequery/part-m-*\n")])])]),t("p",[s._v("它用逗号（，）分隔 emp_add表数据和字段：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", 108I, aoc, sec-bad\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", 78B, oldcity, sec-bad\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", 720C, hitech, sec-bad\n")])])]),t("h4",{attrs:{id:"_4-2-5、增量导入"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-5、增量导入"}},[s._v("#")]),s._v(" 4.2.5、增量导入")]),s._v(" "),t("p",[s._v("增量导入是仅导入新添加的表中的行的技术。\nsqoop支持两种增量MySql导入到hive的模式，")]),s._v(" "),t("p",[s._v("一种是append，即通过指定一个递增的列，比如：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("--incremental append  --check-column num_id --last-value "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),s._v(" \n")])])]),t("p",[s._v("另种是可以根据时间戳，比如：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("--incremental lastmodified --check-column created --last-value "),t("span",{pre:!0,attrs:{class:"token string"}},[s._v("'2018-02-01 11:0:00'")]),s._v(" \n")])])]),t("p",[s._v("就是只导入created 比'2018-02-01 11:0:00'更大的数据。")]),s._v(" "),t("h5",{attrs:{id:"_4-2-5-1、append模式"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-5-1、append模式"}},[s._v("#")]),s._v(" 4.2.5.1、append模式")]),s._v(" "),t("p",[s._v("它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。")]),s._v(" "),t("p",[s._v("下面的语法用于Sqoop导入命令增量选项：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("--incremental "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("mode"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n--check-column "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("column name"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n--last value "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("<")]),s._v("last check "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("column")]),s._v(" value"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("\n")])])]),t("p",[s._v("假设新添加的数据转换成emp表如下：")]),s._v(" "),t("p",[s._v("1206, satish p, grp des, 20000, GR")]),s._v(" "),t("p",[s._v("下面的命令用于在EMP表执行增量导入：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("import")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table emp "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--m "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--incremental append "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--check-column "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--last-value "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1208")]),s._v("\n")])])]),t("p",[s._v("以下命令用于从emp表导入HDFS emp/ 目录的数据验证：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ "),t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /user/hadoop/emp/part-m-*\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", gopal,    manager, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", manisha,  preader, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", kalil,    php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", prasanth, php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", kranthi,  admin,   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p, grp des, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),t("p",[s._v("下面的命令是从表emp 用来查看修改或新添加的行：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token variable"}},[s._v("$HADOOP_HOME")]),s._v("/bin/hadoop fs -cat /emp/part-m-*1\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p, grp des, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),t("h2",{attrs:{id:"_5、sqoop的数据导出"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5、sqoop的数据导出"}},[s._v("#")]),s._v(" 5、Sqoop的数据导出")]),s._v(" "),t("h3",{attrs:{id:"_5-1、将数据从hdfs文件导出到rdbms数据库"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1、将数据从hdfs文件导出到rdbms数据库"}},[s._v("#")]),s._v(" 5.1、将数据从HDFS文件导出到RDBMS数据库")]),s._v(" "),t("ul",[t("li",[s._v("导出前，目标表必须存在于目标数据库中。")]),s._v(" "),t("li",[s._v("默认操作是将文件中的数据使用INSERT语句插入到表中")]),s._v(" "),t("li",[s._v("更新模式下，是生成UPDATE语句更新表数据")])]),s._v(" "),t("p",[s._v("语法")]),s._v(" "),t("p",[s._v("以下是export命令语法:")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ sqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("generic-args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("export-args"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n")])])]),t("p",[s._v("示例")]),s._v(" "),t("p",[s._v("数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(", gopal,     manager, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(", manisha,   preader, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(", kalil,     php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(", prasanth,  php dev, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v(", AC\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(", kranthi,   admin,   "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", TP\n"),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(", satish p,  grp des, "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v(", GR\n")])])]),t("h4",{attrs:{id:"_5-1-1、首先需要手动创建mysql中的目标表"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-1、首先需要手动创建mysql中的目标表"}},[s._v("#")]),s._v(" 5.1.1、首先需要手动创建mysql中的目标表")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("$ mysql\nmysql"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" USE db"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\nmysql"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v(" CREATE TABLE employee "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("\n   "),t("span",{pre:!0,attrs:{class:"token function"}},[s._v("id")]),s._v(" INT NOT NULL PRIMARY KEY,\n   name VARCHAR"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n   deg VARCHAR"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(",\n   salary INT,\n   dept VARCHAR"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("))")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("h4",{attrs:{id:"_5-1-2、然后执行导出命令"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-2、然后执行导出命令"}},[s._v("#")]),s._v(" 5.1.2、然后执行导出命令")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("bin/sqoop "),t("span",{pre:!0,attrs:{class:"token builtin class-name"}},[s._v("export")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--connect jdbc:mysql://hdp-node-01:3306/test "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--username root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--password root "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--table employee "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("\\")]),s._v("\n--export-dir /user/hadoop/emp/\n")])])]),t("h4",{attrs:{id:"_5-1-3、验证表mysql命令行。"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-3、验证表mysql命令行。"}},[s._v("#")]),s._v(" 5.1.3、验证表mysql命令行。")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("mysql"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v(">")]),s._v("select * from employee"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),t("p",[s._v("如果给定的数据存储成功，那么可以找到数据在如下的employee表:")]),s._v(" "),t("div",{staticClass:"language-shell extra-class"},[t("pre",{pre:!0,attrs:{class:"language-shell"}},[t("code",[s._v("+------+--------------+-------------+-------------------+--------+\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Id   "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Name         "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Designation "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Salary            "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" Dept   "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n+------+--------------+-------------+-------------------+--------+\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1201")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" gopal        "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" manager     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1202")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" manisha      "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" preader     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("50000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1203")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" kalil        "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" php dev     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" AC     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1204")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" prasanth     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" php dev     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("30000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" AC     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1205")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" kranthi      "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" admin       "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" TP     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n"),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("1206")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" satish p     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" grp des     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" "),t("span",{pre:!0,attrs:{class:"token number"}},[s._v("20000")]),s._v("             "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v(" GR     "),t("span",{pre:!0,attrs:{class:"token operator"}},[s._v("|")]),s._v("\n+------+--------------+-------------+-------------------+--------+\n")])])]),t("h2",{attrs:{id:"参考文章"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[s._v("#")]),s._v(" 参考文章")]),s._v(" "),t("ul",[t("li",[s._v("https://blog.csdn.net/qq_26803795/article/details/80905651")])])])}),[],!1,null,null,null);a.default=r.exports}}]);