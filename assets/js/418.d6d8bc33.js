(window.webpackJsonp=window.webpackJsonp||[]).push([[418],{933:function(a,t,s){"use strict";s.r(t);var r=s(53),e=Object(r.a)({},(function(){var a=this,t=a.$createElement,s=a._self._c||t;return s("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[a._v("TIP")]),a._v(" "),s("p",[a._v("本文主要是介绍 Spark-综合精华总结(二) 。")])]),a._v(" "),s("p"),s("div",{staticClass:"table-of-contents"},[s("ul",[s("li",[s("a",{attrs:{href:"#spark总结笔记"}},[a._v("spark总结笔记")])]),s("li",[s("a",{attrs:{href:"#_1-高效性"}},[a._v("1 高效性")])]),s("li",[s("a",{attrs:{href:"#_2-易用性"}},[a._v("2 易用性")])]),s("li",[s("a",{attrs:{href:"#_3-通用性"}},[a._v("3 通用性")])]),s("li",[s("a",{attrs:{href:"#_4-兼容性"}},[a._v("4 兼容性")])]),s("li",[s("a",{attrs:{href:"#二、spark基本概念"}},[a._v("二、Spark基本概念")])]),s("li",[s("a",{attrs:{href:"#三、spark架构设计"}},[a._v("三、Spark架构设计")])]),s("li",[s("a",{attrs:{href:"#四、spark运行流程"}},[a._v("四、Spark运行流程")])]),s("li",[s("a",{attrs:{href:"#五、spark部署模式"}},[a._v("五、Spark部署模式")])]),s("li",[s("a",{attrs:{href:"#六、rdd数据结构"}},[a._v("六、RDD数据结构")])]),s("li",[s("a",{attrs:{href:"#七、wordcount范例"}},[a._v("七、WordCount范例")])]),s("li",[s("a",{attrs:{href:"#参考文章"}},[a._v("参考文章")])])])]),s("p"),a._v(" "),s("h2",{attrs:{id:"spark总结笔记"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#spark总结笔记"}},[a._v("#")]),a._v(" spark总结笔记")]),a._v(" "),s("p",[a._v("一、Spark优势特点")]),a._v(" "),s("p",[a._v("作为大数据计算框架MapReduce的几人者，Spark具备一下优势特性。")]),a._v(" "),s("h2",{attrs:{id:"_1-高效性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-高效性"}},[a._v("#")]),a._v(" 1 高效性")]),a._v(" "),s("p",[a._v("不同于MapReduce将中间计算结果放入磁盘中，Spark采用内存存储中间计算结果，减少了迭代运算的磁盘IO，并通过并行计算DAG图的优化，减少了不同任务之间的依赖，降低了延迟等待时间。内存计算下，Spark 比 MapReduce 快100倍。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624093623252-1017651431.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"_2-易用性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-易用性"}},[a._v("#")]),a._v(" 2 易用性")]),a._v(" "),s("p",[a._v("不同于MapReduce仅支持Map和Reduce两种编程算子，Spark提供了超过80种不同的Transformation和Action算子，如map,reduce,filter,groupByKey,sortByKey,foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624093737554-678093832.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"_3-通用性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-通用性"}},[a._v("#")]),a._v(" 3 通用性")]),a._v(" "),s("p",[a._v("Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。")]),a._v(" "),s("p",[a._v("这些不同类型的处理都可以在同一个应用中无缝使用。这对于企业应用来说，就可使用一个平台来进行不同的工程实现，减少了人力开发和平台部署成本。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624093824509-1402966378.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"_4-兼容性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-兼容性"}},[a._v("#")]),a._v(" 4 兼容性")]),a._v(" "),s("p",[a._v("Spark能够跟很多开源工程兼容使用。如Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且Spark可以读取多种数据源，如HDFS、HBase、MySQL等。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624093939153-664855847.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"二、spark基本概念"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、spark基本概念"}},[a._v("#")]),a._v(" 二、Spark基本概念")]),a._v(" "),s("p",[a._v("RDD：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。")]),a._v(" "),s("p",[a._v("DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系。")]),a._v(" "),s("p",[a._v("Driver Program：控制程序，负责为Application构建DAG图。")]),a._v(" "),s("p",[a._v("Cluster Manager：集群资源管理中心，负责分配计算资源。")]),a._v(" "),s("p",[a._v("Worker Node：工作节点，负责完成具体计算。")]),a._v(" "),s("p",[a._v("Executor：是运行在工作节点（Worker Node）上的一个进程，负责运行Task，并为应用程序存储数据。")]),a._v(" "),s("p",[a._v("Application：用户编写的Spark应用程序，一个Application包含多个Job。")]),a._v(" "),s("p",[a._v("Job：作业，一个Job包含多个RDD及作用于相应RDD上的各种操作。")]),a._v(" "),s("p",[a._v("Stage：阶段，是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”。")]),a._v(" "),s("p",[a._v("Task：任务，运行在Executor上的工作单元，是Executor中的一个线程。")]),a._v(" "),s("p",[a._v("总结：Application由多个Job组成，Job由多个Stage组成，Stage由多个Task组成。Stage是作业调度的基本单位。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094013467-1984588464.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"三、spark架构设计"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、spark架构设计"}},[a._v("#")]),a._v(" 三、Spark架构设计")]),a._v(" "),s("p",[a._v("Spark集群由Driver, Cluster Manager（Standalone,Yarn 或 Mesos），以及Worker Node组成。对于每个Spark应用程序，Worker Node上存在一个Executor进程，Executor进程中包括多个Task线程。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094122073-283724479.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"四、spark运行流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、spark运行流程"}},[a._v("#")]),a._v(" 四、Spark运行流程")]),a._v(" "),s("ul",[s("li",[s("ol",[s("li",[a._v("Application首先被Driver构建DAG图并分解成Stage。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"2"}},[s("li",[a._v("然后Driver向Cluster Manager申请资源。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"3"}},[s("li",[a._v("Cluster Manager向某些Work Node发送征召信号。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"4"}},[s("li",[a._v("被征召的Work Node启动Executor进程响应征召，并向Driver申请任务。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"5"}},[s("li",[a._v("Driver分配Task给Work Node。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"6"}},[s("li",[a._v("Executor以Stage为单位执行Task，期间Driver进行监控。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"7"}},[s("li",[a._v("Driver收到Executor任务完成的信号后向Cluster Manager发送注销信号。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"8"}},[s("li",[a._v("Cluster Manager向Work Node发送释放资源信号。")])])]),a._v(" "),s("li",[s("ol",{attrs:{start:"9"}},[s("li",[a._v("Work Node对应Executor停止运行。")])])])]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094259108-553466786.jpg"),alt:"wxmp"}}),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094427206-1189326068.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"五、spark部署模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#五、spark部署模式"}},[a._v("#")]),a._v(" 五、Spark部署模式")]),a._v(" "),s("p",[a._v("Local：本地运行模式，非分布式。")]),a._v(" "),s("p",[a._v("Standalone：使用Spark自带集群管理器，部署后只能运行Spark任务。")]),a._v(" "),s("p",[a._v("Yarn：Haoop集群管理器，部署后可以同时运行MapReduce，Spark，Storm，Hbase等各种任务。")]),a._v(" "),s("p",[a._v("Mesos：与Yarn最大的不同是Mesos 的资源分配是二次的，Mesos负责分配一次，计算框架可以选择接受或者拒绝。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094516496-1300093129.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"六、rdd数据结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#六、rdd数据结构"}},[a._v("#")]),a._v(" 六、RDD数据结构")]),a._v(" "),s("p",[a._v("RDD全称Resilient Distributed Dataset，弹性分布式数据集，它是记录的只读分区集合，是Spark的基本数据结构。")]),a._v(" "),s("p",[a._v("RDD代表一个不可变、可分区、里面的元素可并行计算的集合。")]),a._v(" "),s("p",[a._v("一般有两种方式可以创建RDD，第一种是读取文件中的数据生成RDD，第二种则是通过将内存中的对象并行化得到RDD。")]),a._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[a._v('//通过读取文件生成RDDval  rdd = sc.textFile("hdfs://hans/data_warehouse/test/data")')]),a._v("\n \n"),s("span",{pre:!0,attrs:{class:"token comment"}},[a._v("//通过将内存中的对象并行化得到RDD")]),a._v("\nval num "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[a._v("Array")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nval rdd "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("parallelize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("num"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[a._v("//或者 val rdd = sc.makeRDD(num)")]),a._v("\n")])])]),s("p",[a._v("创建RDD之后，可以使用各种操作对RDD进行编程。")]),a._v(" "),s("p",[a._v("RDD的操作有两种类型，即Transformation操作和Action操作。转换操作是从已经存在的RDD创建一个新的RDD，而行动操作是在RDD上进行计算后返回结果到 Driver。")]),a._v(" "),s("p",[a._v("Transformation操作都具有 Lazy 特性，即 Spark 不会立刻进行实际的计算，只会记录执行的轨迹，只有触发Action操作的时候，它才会根据 DAG 图真正执行。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094803736-2082075739.jpg"),alt:"wxmp"}}),a._v(" "),s("p",[a._v("操作确定了RDD之间的依赖关系。")]),a._v(" "),s("p",[a._v("RDD之间的依赖关系有两种类型，即窄依赖和宽依赖。窄依赖时，父RDD的分区和子RDD的分区的关系是一对一或者多对一的关系。而宽依赖时，父RDD的分区和子RDD的分区是一对多或者多对多的关系。")]),a._v(" "),s("p",[a._v("宽依赖关系相关的操作一般具有shuffle过程，即通过一个Patitioner函数将父RDD中每个分区上key不同的记录分发到不同的子RDD分区。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624094917022-1772537749.jpg"),alt:"wxmp"}}),a._v(" "),s("p",[a._v("依赖关系确定了DAG切分成Stage的方式。")]),a._v(" "),s("p",[a._v("切割规则：从后往前，遇到宽依赖就切割Stage。")]),a._v(" "),s("p",[a._v("RDD之间的依赖关系形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分成相互依赖的多个stage，划分stage的依据就是RDD之间的宽窄依赖。遇到宽依赖就划分stage,每个stage包含一个或多个task任务。然后将这些task以taskSet的形式提交给TaskScheduler运行。")]),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624095053462-215412218.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"七、wordcount范例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#七、wordcount范例"}},[a._v("#")]),a._v(" 七、WordCount范例")]),a._v(" "),s("p",[a._v("只需要四行代码就可以完成WordCount词频统计。")]),a._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[a._v("val file "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" sc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("textFile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("``"),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('"hello.txt"')]),a._v("``"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nval word "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("flatMap")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("split")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("``"),s("span",{pre:!0,attrs:{class:"token string"}},[a._v('","')]),a._v("``"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nval wordOne "),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" word"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[a._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\nwordOne"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[a._v("reduceByKey")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("(")]),a._v("_"),s("span",{pre:!0,attrs:{class:"token operator"}},[a._v("+")]),a._v("_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v(")")]),a._v("\n")])])]),s("p"),a._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:a.$withBase("/assets/img/dp/spark/sum2/732403-20200624095224415-1089432908.jpg"),alt:"wxmp"}}),a._v(" "),s("h2",{attrs:{id:"参考文章"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[a._v("#")]),a._v(" 参考文章")]),a._v(" "),s("ul",[s("li",[a._v("https://www.cnblogs.com/sumuncle/p/13186007.html")])])])}),[],!1,null,null,null);t.default=e.exports}}]);