(window.webpackJsonp=window.webpackJsonp||[]).push([[420],{935:function(t,a,s){"use strict";s.r(a);var r=s(53),e=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),s("p",[t._v("本文主要是介绍 Spark-常用调优整理 。")])]),t._v(" "),s("p"),s("div",{staticClass:"table-of-contents"},[s("ul",[s("li",[s("a",{attrs:{href:"#前言"}},[t._v("前言")])]),s("li",[s("a",{attrs:{href:"#一、分配更多的资源"}},[t._v("一、分配更多的资源")]),s("ul",[s("li",[s("a",{attrs:{href:"#_1-增加executor数量"}},[t._v("1. 增加executor数量")])]),s("li",[s("a",{attrs:{href:"#_2-增加core数量"}},[t._v("2. 增加core数量")])]),s("li",[s("a",{attrs:{href:"#_3-增加每个executor的内存数量"}},[t._v("3. 增加每个executor的内存数量")])]),s("li",[s("a",{attrs:{href:"#_4-增加driver内存-影响很小"}},[t._v("4. 增加driver内存(影响很小)")])]),s("li",[s("a",{attrs:{href:"#小结"}},[t._v("小结")])])])]),s("li",[s("a",{attrs:{href:"#二、提高并行度"}},[t._v("二、提高并行度")])]),s("li",[s("a",{attrs:{href:"#三、rdd重用"}},[t._v("三、RDD重用")])]),s("li",[s("a",{attrs:{href:"#四、广播变量的使用"}},[t._v("四、广播变量的使用")])]),s("li",[s("a",{attrs:{href:"#五、避免采用shuffle算子"}},[t._v("五、避免采用shuffle算子")]),s("ul",[s("li",[s("a",{attrs:{href:"#案例"}},[t._v("案例")])])])]),s("li",[s("a",{attrs:{href:"#六、使用高性能的算子"}},[t._v("六、使用高性能的算子")]),s("ul",[s("li",[s("a",{attrs:{href:"#_1-使用reducebykey-aggregatebykey替代groupbykey"}},[t._v("1. 使用reduceByKey/aggregateByKey替代groupByKey")])]),s("li",[s("a",{attrs:{href:"#_2-使用mappartitions替代普通map"}},[t._v("2. 使用mapPartitions替代普通map")])]),s("li",[s("a",{attrs:{href:"#_3-使用foreachpartitions替代foreach"}},[t._v("3. 使用foreachPartitions替代foreach")])]),s("li",[s("a",{attrs:{href:"#_4-filter之后进行coalesce操作"}},[t._v("4. filter之后进行coalesce操作")])]),s("li",[s("a",{attrs:{href:"#_5-使用repartitionandsortwithinpartitions替代repartition与sort类操作"}},[t._v("5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作")])])])]),s("li",[s("a",{attrs:{href:"#七、采用kryo提高序列化性能"}},[t._v("七、采用Kryo提高序列化性能")]),s("ul",[s("li",[s("a",{attrs:{href:"#如何使用"}},[t._v("如何使用")])]),s("li",[s("a",{attrs:{href:"#使用场景-1"}},[t._v("使用场景 1")])]),s("li",[s("a",{attrs:{href:"#使用场景-2"}},[t._v("使用场景 2")])]),s("li",[s("a",{attrs:{href:"#使用方式"}},[t._v("使用方式")])])])]),s("li",[s("a",{attrs:{href:"#九、调节数据本地化等待时长"}},[t._v("九、调节数据本地化等待时长")])]),s("li",[s("a",{attrs:{href:"#十、内存模型划分"}},[t._v("十、内存模型划分")])]),s("li",[s("a",{attrs:{href:"#参考文章"}},[t._v("参考文章")])])])]),s("p"),t._v(" "),s("h2",{attrs:{id:"前言"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[t._v("#")]),t._v(" 前言")]),t._v(" "),s("p",[t._v("在写spark任务中，离不开调优的工作，因此将常用的操作记录下来，方便以后的调优工作；同时如果之后发现了其他的调优手段，也会将其记录进来")]),t._v(" "),s("p",[t._v("ps：")]),t._v(" "),s("ul",[s("li",[t._v("(1) 将八斗的spark调优手段也补充进来")]),t._v(" "),s("li",[t._v("(2) 抽时间将这些调优操作都亲自执行一遍，并记录感想")])]),t._v(" "),s("h2",{attrs:{id:"一、分配更多的资源"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#一、分配更多的资源"}},[t._v("#")]),t._v(" 一、分配更多的资源")]),t._v(" "),s("p",[t._v("原则：在能力许可的范围内，尽可能的分配更多的资源")]),t._v(" "),s("h3",{attrs:{id:"_1-增加executor数量"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-增加executor数量"}},[t._v("#")]),t._v(" 1. 增加executor数量")]),t._v(" "),s("p",[s("strong",[t._v("executor越多，spark任务并行能力越强")])]),t._v(" "),s("ul",[s("li",[t._v("executor为3，core为2，则同一时间可以同时执行6个task")]),t._v(" "),s("li",[t._v("executor为6，core为2，则同一时间可以同时执行12个task")])]),t._v(" "),s("p",[t._v("执行速度提升2倍")]),t._v(" "),s("h3",{attrs:{id:"_2-增加core数量"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-增加core数量"}},[t._v("#")]),t._v(" 2. 增加core数量")]),t._v(" "),s("p",[s("strong",[t._v("core越多，spark任务并行执行能力越强")])]),t._v(" "),s("ul",[s("li",[t._v("executor为3，core为2，则同一时间可以同时执行6个task")]),t._v(" "),s("li",[t._v("executor为3，core为4，则同一时间可以同时执行12个task")])]),t._v(" "),s("p",[t._v("执行速度提升2倍")]),t._v(" "),s("h3",{attrs:{id:"_3-增加每个executor的内存数量"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-增加每个executor的内存数量"}},[t._v("#")]),t._v(" 3. 增加每个executor的内存数量")]),t._v(" "),s("p",[t._v("好处：")]),t._v(" "),s("ul",[s("li",[t._v("可以cache更多的数据到内存，提高读性能")]),t._v(" "),s("li",[t._v("shuffle操作聚合时，若内存不足也会写到磁盘，影响性能")]),t._v(" "),s("li",[t._v("task执行过程会创建很多对象，若内存不足会频繁GC，影响性能")])]),t._v(" "),s("h3",{attrs:{id:"_4-增加driver内存-影响很小"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-增加driver内存-影响很小"}},[t._v("#")]),t._v(" 4. 增加driver内存(影响很小)")]),t._v(" "),s("p",[t._v("如果spark任务中有collect或者广播变量比较大时，可以适当调大该值避免OOM")]),t._v(" "),s("h3",{attrs:{id:"小结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#小结"}},[t._v("#")]),t._v(" 小结")]),t._v(" "),s("p",[t._v("可参考的值")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("计算出yarn集群的所有大小，比如一共"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),t._v("g内存，"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("个cpu\n此时最多可分配"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("个executor、每个executor的内存大小"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("g"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("每个executor使用的cpu个数为"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n")])])]),s("h2",{attrs:{id:"二、提高并行度"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#二、提高并行度"}},[t._v("#")]),t._v(" 二、提高并行度")]),t._v(" "),s("p",[t._v("各个Stage中task的数量(分区数)就代表了spark任务的并行度")]),t._v(" "),s("p",[t._v("最好将task的数量设置为总core数的2~3倍")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("## 这种设置方式只有在遇到shuffle过程才生效\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" 设置参数spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("defalut"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parallelism \n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SparkConf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.defalut.parallelism"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"500"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n## 这种设置方式会立马生效，但是会触发shuffle，需要进行衡量\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" rdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repartition 来重新分区，该方法会生成一个新的rdd，使其分区数变大\n\n## 可以适当增大，来提高sql并行度\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" 通过设置参数 spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("partitions"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),t._v("  默认为"),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("200")]),t._v("；\n")])])]),s("h2",{attrs:{id:"三、rdd重用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#三、rdd重用"}},[t._v("#")]),t._v(" 三、RDD重用")]),t._v(" "),s("p",[t._v("如果有些RDD比较常用，或者RDD的链路比较长，可以缓存共用的RDD")]),t._v(" "),s("ul",[s("li",[t._v("设置为仅内存模式性能最佳，但是最容易触发OOM")]),t._v(" "),s("li",[t._v("可以退而求次，选择仅内存但序列化，这种依然可能会触发OOM")]),t._v(" "),s("li",[t._v("此时可以在退一步，选择内存和磁盘但不序列化，这种不会触发OOM")])]),t._v(" "),s("h2",{attrs:{id:"四、广播变量的使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#四、广播变量的使用"}},[t._v("#")]),t._v(" 四、广播变量的使用")]),t._v(" "),s("p",[t._v("如果执行任务的过程中，依赖的外部中间数据比较大，或者执行任务的task数量比较大，可以考虑采用广播变量")]),t._v(" "),s("ul",[s("li",[t._v("采用广播变量后，数据不再加载dataNum "),s("em",[t._v("taskNum ,而是仅加载 dataNum")]),t._v(" ExecutorNum")]),t._v(" "),s("li",[t._v("减少内存使用量以及提高spark任务的性能")])]),t._v(" "),s("p",[t._v("如果每个task都加载一份占用大量内存，会直接导致")]),t._v(" "),s("ul",[s("li",[t._v("持久化到内存的RDD会被溢写到磁盘，无法在提升读性能")]),t._v(" "),s("li",[t._v("导致频繁的GC")]),t._v(" "),s("li",[t._v("涉及到大量的网络传输")])]),t._v(" "),s("h2",{attrs:{id:"五、避免采用shuffle算子"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#五、避免采用shuffle算子"}},[t._v("#")]),t._v(" 五、避免采用shuffle算子")]),t._v(" "),s("p",[t._v("shuffle不仅涉及到大量的网络传输，而且还涉及到了读写磁盘\n"),s("em",[t._v("因为聚合阶段数据会膨胀几倍，内存一般放不下")]),t._v("、")]),t._v(" "),s("p",[t._v("spark任务中，shuffle是最耗费性能的操作")]),t._v(" "),s("h3",{attrs:{id:"案例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#案例"}},[t._v("#")]),t._v(" 案例")]),t._v(" "),s("p",[s("strong",[t._v("采用广播变量取代join")]),t._v("\n如果两个RDD存在join操作，并且其中一个RDD比较小(仅几百M或者1,2G)\n可将小RDD作为广播变量，然后对大RDD进行操作，比对广播变量中的小RDD做map操作，满足条件才写到下游")]),t._v(" "),s("p",[s("strong",[t._v("做预聚合操作")]),t._v("\n采用reduceByKey或aggregateByKey取代groupByKey")]),t._v(" "),s("p",[t._v("因为前两者会在shuffle上游对数据先做一次预聚合，这个操作的好处有")]),t._v(" "),s("ul",[s("li",[t._v("大大减少shuffle阶段网络传输的数据")]),t._v(" "),s("li",[t._v("上游先做一次预聚合，会大大减小下游做聚合时的计算时间")])]),t._v(" "),s("h2",{attrs:{id:"六、使用高性能的算子"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#六、使用高性能的算子"}},[t._v("#")]),t._v(" 六、使用高性能的算子")]),t._v(" "),s("h3",{attrs:{id:"_1-使用reducebykey-aggregatebykey替代groupbykey"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-使用reducebykey-aggregatebykey替代groupbykey"}},[t._v("#")]),t._v(" 1. 使用reduceByKey/aggregateByKey替代groupByKey")]),t._v(" "),s("h3",{attrs:{id:"_2-使用mappartitions替代普通map"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-使用mappartitions替代普通map"}},[t._v("#")]),t._v(" 2. 使用mapPartitions替代普通map")]),t._v(" "),s("p",[t._v("虽然性能提高了，但是可能会抛OOM异常(因为一次处理一个partition的数据)")]),t._v(" "),s("h3",{attrs:{id:"_3-使用foreachpartitions替代foreach"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-使用foreachpartitions替代foreach"}},[t._v("#")]),t._v(" 3. 使用foreachPartitions替代foreach")]),t._v(" "),s("p",[t._v("这个阶段一般会依赖于外部介质，如果每处理一条数据都跟外部介质建立一次连接，会大大影响性能，而且如果是对一个partition建立一次连接，还可以进行批量的提交，大大提高吞吐量")]),t._v(" "),s("p",[t._v("实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上")]),t._v(" "),s("h3",{attrs:{id:"_4-filter之后进行coalesce操作"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-filter之后进行coalesce操作"}},[t._v("#")]),t._v(" 4. filter之后进行coalesce操作")]),t._v(" "),s("p",[t._v("一般filter会过滤30%以上的数据，因此可能存在数据倾斜的问题，个别小partition数据比较多，直接影响整体任务执行速度。在某种场景下，此时进行coalesce可以提高性能")]),t._v(" "),s("h3",{attrs:{id:"_5-使用repartitionandsortwithinpartitions替代repartition与sort类操作"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_5-使用repartitionandsortwithinpartitions替代repartition与sort类操作"}},[t._v("#")]),t._v(" 5. 使用repartitionAndSortWithinPartitions替代repartition与sort类操作")]),t._v(" "),s("p",[t._v("如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子("),s("strong",[t._v("官方推荐")]),t._v(")")]),t._v(" "),s("h2",{attrs:{id:"七、采用kryo提高序列化性能"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#七、采用kryo提高序列化性能"}},[t._v("#")]),t._v(" 七、采用Kryo提高序列化性能")]),t._v(" "),s("p",[t._v("Kryo比Java原生的序列化方式要快，且序列化之后的数据仅为Java的1/10")]),t._v(" "),s("p",[t._v("带来的好处有")]),t._v(" "),s("ul",[s("li",[t._v("优化读取外部数据性能\n序列化后的数据量变少，提高网络传输效率")]),t._v(" "),s("li",[t._v("缓存时采用序列化\n持久化的RDD占用内存更少，task执行过程中GC的频率也会下降")]),t._v(" "),s("li",[t._v("shuffle阶段性能提升\n网络传输的性能提升")])]),t._v(" "),s("h3",{attrs:{id:"如何使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#如何使用"}},[t._v("#")]),t._v(" 如何使用")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 创建SparkConf对象。")]),t._v("\nval conf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SparkConf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMaster")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setAppName")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 设置序列化器为KryoSerializer。")]),t._v("\nconf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.serializer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.spark.serializer.KryoSerializer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 注册要序列化的自定义类型。")]),t._v("\nconf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("registerKryoClasses")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Array")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("classOf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyClass1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" classOf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyClass2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("八、使用fastutil优化数据格式")]),t._v(" "),s("ul",[s("li",[t._v("fastutil能够提供更小的内存占用，更快的存取速度")]),t._v(" "),s("li",[t._v("可使用fastutil提供的集合类来替代自己平时使用的JDK的原生的Map、List、Set")])]),t._v(" "),s("h3",{attrs:{id:"使用场景-1"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用场景-1"}},[t._v("#")]),t._v(" 使用场景 1")]),t._v(" "),s("p",[t._v("使用Broadcast广播变量优化")]),t._v(" "),s("p",[t._v("使用Kryo序列化类库，提升序列化性能和效率")]),t._v(" "),s("p",[t._v("如果外部变量是某种比较大的集合，那么可以考虑使用fastutil改写外部变量")]),t._v(" "),s("p",[t._v("首先从源头上就减少内存的占用(fastutil)，通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用")]),t._v(" "),s("h3",{attrs:{id:"使用场景-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用场景-2"}},[t._v("#")]),t._v(" 使用场景 2")]),t._v(" "),s("p",[t._v("算子函数里使用了比较大的集合Map/List")]),t._v(" "),s("p",[t._v("如果在task计算逻辑里会创建比较大的Map，可能会占用较大的内存空间，而且涉及到消耗性能的遍历、存取等集合操作；那么此时，可以考虑将这些集合类型使用fastutil类库重写")]),t._v(" "),s("p",[t._v("使用了fastutil集合类以后，就可以在一定程度上，减少task创建出来的集合类型的内存占用。 避免executor内存频繁占满，频繁唤起GC，导致性能下降")]),t._v(" "),s("h3",{attrs:{id:"使用方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#使用方式"}},[t._v("#")]),t._v(" 使用方式")]),t._v(" "),s("div",{staticClass:"language-xml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-xml"}},[s("code",[t._v("第一步：在pom.xml中引用fastutil的包\n    "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("dependency")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("groupId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("fastutil"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("groupId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("artifactId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("fastutil"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("artifactId")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("version")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("5.0.9"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("version")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("dependency")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n    \n第二步：平时使用List （Integer）的替换成IntList即可。 \n    List"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("Integer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("的list对应的到fastutil就是IntList类型\n    \n    \n使用说明：\n基本都是类似于IntList的格式，前缀就是集合的元素类型； \n特殊的就是Map，Int2IntMap，代表了key-value映射的元素类型。\n")])])]),s("h2",{attrs:{id:"九、调节数据本地化等待时长"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#九、调节数据本地化等待时长"}},[t._v("#")]),t._v(" 九、调节数据本地化等待时长")]),t._v(" "),s("p",[t._v("在本地执行，看看任务是否都在进程本地执行，如果不是可以适当调高该值")]),t._v(" "),s("p",[t._v("（1）PROCESS_LOCAL：进程本地化")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("BlockManager")]),t._v("中；性能最好\n")])])]),s("p",[t._v("（2）NODE_LOCAL：节点本地化")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("代码和数据在同一个节点中；比如说数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是数据和task在一个节点上的不同executor中；数据需要在进程间进行传输；性能其次\n")])])]),s("p",[t._v("（3）RACK_LOCAL：机架本地化")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输； 性能比较差\n")])])]),s("p",[t._v("（4） ANY：无限制")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("数据和task可能在集群中的任何地方，而且不在一个机架中；性能最差\n")])])]),s("p",[t._v("spark.locality.wait，默认是3s")]),t._v(" "),s("p",[t._v("首先采用最佳的方式，等待3s后降级,还是不行，继续降级...,最后还是不行，只能够采用最差的。")]),t._v(" "),s("p",[t._v("在代码中设置：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("SparkConf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark.locality.wait"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"10"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"十、内存模型划分"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#十、内存模型划分"}},[t._v("#")]),t._v(" 十、内存模型划分")]),t._v(" "),s("p",[t._v("如果我们cache数据量比较大，或者是我们的广播变量比较大，")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("那我们就把spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("storage"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("memoryFraction这个值调大一点。\n但是如果我们代码里面没有广播变量，也没有cache，shuffle又比较多，那我们要把spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("memoryFraction 这值调大。\n")])])]),s("ul",[s("li",[t._v("java.lang.OutOfMemoryError")]),t._v(" "),s("li",[t._v("ExecutorLostFailure")]),t._v(" "),s("li",[t._v("Executor exit code 为143")]),t._v(" "),s("li",[t._v("executor lost")]),t._v(" "),s("li",[t._v("hearbeat time out")]),t._v(" "),s("li",[t._v("shuffle file lost")])]),t._v(" "),s("p",[t._v("如果遇到以上问题，很有可能就是内存除了问题，可以先尝试增加内存")]),t._v(" "),s("h2",{attrs:{id:"参考文章"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),s("ul",[s("li",[t._v("https://segmentfault.com/a/1190000022526479")])])])}),[],!1,null,null,null);a.default=e.exports}}]);