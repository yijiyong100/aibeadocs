(window.webpackJsonp=window.webpackJsonp||[]).push([[486],{1001:function(s,t,a){"use strict";a.r(t);var e=a(53),n=Object(e.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[s._v("TIP")]),s._v(" "),a("p",[s._v("本文主要是介绍 Hive-千亿级数据倾斜 。")])]),s._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#hive-千亿级数据倾斜"}},[s._v("Hive-千亿级数据倾斜")]),a("ul",[a("li",[a("a",{attrs:{href:"#数据倾斜问题剖析"}},[s._v("数据倾斜问题剖析")])]),a("li",[a("a",{attrs:{href:"#数据倾斜解决方案"}},[s._v("数据倾斜解决方案")])]),a("li",[a("a",{attrs:{href:"#_1-空值引发的数据倾斜"}},[s._v("1. 空值引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#_2-不同数据类型引发的数据倾斜"}},[s._v("2. 不同数据类型引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#_3-不可拆分大文件引发的数据倾斜"}},[s._v("3. 不可拆分大文件引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#_4-数据膨胀引发的数据倾斜"}},[s._v("4. 数据膨胀引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#_5-表连接时引发的数据倾斜"}},[s._v("5. 表连接时引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#_6-确实无法减少数据量引发的数据倾斜"}},[s._v("6. 确实无法减少数据量引发的数据倾斜")])]),a("li",[a("a",{attrs:{href:"#总结"}},[s._v("总结")])])])]),a("li",[a("a",{attrs:{href:"#参考文章"}},[s._v("参考文章")])])])]),a("p"),s._v(" "),a("h2",{attrs:{id:"hive-千亿级数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-千亿级数据倾斜"}},[s._v("#")]),s._v(" Hive-千亿级数据倾斜")]),s._v(" "),a("h3",{attrs:{id:"数据倾斜问题剖析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜问题剖析"}},[s._v("#")]),s._v(" 数据倾斜问题剖析")]),s._v(" "),a("p",[s._v("数据倾斜是分布式系统不可避免的问题，任何分布式系统都有几率发生数据倾斜，但有些小伙伴在平时工作中感知不是很明显，这里要注意本篇文章的标题—“千亿级数据”，"),a("strong",[s._v("为什么说千亿级")]),s._v("，因为如果一个任务的数据量只有几百万，它即使发生了数据倾斜，所有数据都跑到一台机器去执行，对于几百万的数据量，一台机器执行起来还是毫无压力的，这时数据倾斜对我们感知不大，只有数据达到一个量级时，一台机器应付不了这么多的数据，这时如果发生数据倾斜，那么最后就很难算出结果。")]),s._v(" "),a("p",[s._v("所以就需要我们对数据倾斜的问题进行优化，尽量避免或减轻数据倾斜带来的影响。")]),s._v(" "),a("blockquote",[a("p",[s._v("在解决数据倾斜问题之前，还要再提一句：没有瓶颈时谈论优化，都是自寻烦恼。")])]),s._v(" "),a("p",[s._v("大家想想，在map和reduce两个阶段中，最容易出现数据倾斜的就是reduce阶段，因为map到reduce会经过shuffle阶段，在shuffle中默认会按照key进行hash，"),a("strong",[s._v("如果相同的key过多，那么hash的结果就是大量相同的key进入到同一个reduce中")]),s._v("，导致数据倾斜。")]),s._v(" "),a("p",[s._v("那么有没有可能在map阶段就发生数据倾斜呢，是有这种可能的。")]),s._v(" "),a("p",[s._v("一个任务中，数据文件在进入map阶段之前会进行切分，默认是128M一个数据块，但是如果"),a("strong",[s._v("当对文件使用GZIP压缩等不支持文件分割操作的压缩方式")]),s._v("时，MR任务读取压缩后的文件时，是对它切分不了的，该压缩文件只会被一个任务所读取，如果有一个超大的不可切分的压缩文件被一个map读取时，就会发生map阶段的数据倾斜。")]),s._v(" "),a("p",[s._v("所以，从本质上来说，"),a("strong",[s._v("发生数据倾斜的原因有两种：一是任务中需要处理大量相同的key的数据。二是任务读取不可分割的大文件")]),s._v("。")]),s._v(" "),a("h3",{attrs:{id:"数据倾斜解决方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜解决方案"}},[s._v("#")]),s._v(" 数据倾斜解决方案")]),s._v(" "),a("p",[s._v("MapReduce和Spark中的数据倾斜解决方案原理都是类似的，以下讨论Hive使用MapReduce引擎引发的数据倾斜，Spark数据倾斜也可以此为参照。")]),s._v(" "),a("h3",{attrs:{id:"_1-空值引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-空值引发的数据倾斜"}},[s._v("#")]),s._v(" 1. 空值引发的数据倾斜")]),s._v(" "),a("p",[s._v("实际业务中有些大量的null值或者一些无意义的数据参与到计算作业中，表中有大量的null值，如果表之间进行join操作，就会有shuffle产生，这样所有的null值都会被分配到一个reduce中，必然产生数据倾斜。")]),s._v(" "),a("p",[s._v("之前有小伙伴问，如果A、B两表join操作，假如A表中需要join的字段为null，但是B表中需要join的字段不为null，这两个字段根本就join不上啊，为什么还会放到一个reduce中呢？")]),s._v(" "),a("p",[s._v("这里我们需要明确一个概念，数据放到同一个reduce中的原因不是因为字段能不能join上，而是因为shuffle阶段的hash操作，只要key的hash结果是一样的，它们就会被拉到同一个reduce中。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案")]),s._v("：")]),s._v(" "),a("p",[s._v("第一种：可以直接不让null值参与join操作，即不让null值有shuffle阶段")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log a\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("JOIN")]),s._v(" users b\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("IS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("NOT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("AND")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("UNION")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ALL")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WHERE")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("IS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("第二种：因为null值参与shuffle时的hash结果是一样的，那么我们可以给null值随机赋值，这样它们的hash结果就不一样，就会进到不同的reduce中：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log a\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("LEFT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("JOIN")]),s._v(" users b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("CASE")]),s._v(" \n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("WHEN")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("IS")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("THEN")]),s._v(" concat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'hive_'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n   "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ELSE")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("END")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("h3",{attrs:{id:"_2-不同数据类型引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-不同数据类型引发的数据倾斜"}},[s._v("#")]),s._v(" 2. 不同数据类型引发的数据倾斜")]),s._v(" "),a("p",[s._v("对于两个表join，表a中需要join的字段key为int，表b中key字段既有string类型也有int类型。当按照key进行两个表的join操作时，默认的Hash操作会按int型的id来进行分配，这样所有的string类型都被分配成同一个id，结果就是所有的string类型的字段进入到一个reduce中，引发数据倾斜。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案")]),s._v("：")]),s._v(" "),a("p",[s._v("如果key字段既有string类型也有int类型，默认的hash就都会按int类型来分配，那我们直接把int类型都转为string就好了，这样key字段都为string，hash时就按照string类型分配了：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" users a\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("LEFT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("JOIN")]),s._v(" logs b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("ON")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("usr_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" CAST"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("user_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("AS")]),s._v(" string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("h3",{attrs:{id:"_3-不可拆分大文件引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-不可拆分大文件引发的数据倾斜"}},[s._v("#")]),s._v(" 3. 不可拆分大文件引发的数据倾斜")]),s._v(" "),a("p",[s._v("当集群的数据量增长到一定规模，有些数据需要归档或者转储，这时候往往会对数据进行压缩；"),a("strong",[s._v("当对文件使用GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取")]),s._v("。如果该压缩文件很大，则处理该文件的Map需要花费的时间会远多于读取普通文件的Map时间，该Map任务会成为作业运行的瓶颈。这种情况也就是Map读取文件的数据倾斜。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案：")])]),s._v(" "),a("p",[s._v("这种数据倾斜问题没有什么好的解决方案，只能将使用GZIP压缩等不支持文件分割的文件转为bzip和zip等支持文件分割的压缩方式。")]),s._v(" "),a("p",[s._v("所以，"),a("strong",[s._v("我们在对文件进行压缩时，为避免因不可拆分大文件而引发数据读取的倾斜，在数据压缩的时候可以采用bzip2和Zip等支持文件分割的压缩算法")]),s._v("。")]),s._v(" "),a("h3",{attrs:{id:"_4-数据膨胀引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-数据膨胀引发的数据倾斜"}},[s._v("#")]),s._v(" 4. 数据膨胀引发的数据倾斜")]),s._v(" "),a("p",[s._v("在多维聚合计算时，如果进行分组聚合的字段过多，如下：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" a，b，c，count（"),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),s._v("）"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" log "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" a，b，c "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with rollup")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("blockquote",[a("p",[s._v("注：对于最后的"),a("code",[s._v("with rollup")]),s._v("关键字不知道大家用过没，with rollup是用来在分组统计数据的基础上再进行统计汇总，即用来得到group by的汇总信息。")])]),s._v(" "),a("p",[s._v("如果上面的log表的数据量很大，并且Map端的聚合不能很好地起到数据压缩的情况下，会导致Map端产出的数据急速膨胀，这种情况容易导致作业内存溢出的异常。如果log表含有数据倾斜key，会加剧Shuffle过程的数据倾斜。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案")]),s._v("：")]),s._v(" "),a("p",[s._v("可以拆分上面的sql，将"),a("code",[s._v("with rollup")]),s._v("拆分成如下几个sql：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("GROUP")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("BY")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("SELECT")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("NULL")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("FROM")]),s._v(" log"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("但是，上面这种方式不太好，因为现在是对3个字段进行分组聚合，那如果是5个或者10个字段呢，那么需要拆解的SQL语句会更多。")]),s._v(" "),a("p",[s._v("在Hive中可以通过参数 "),a("code",[s._v("hive.new.job.grouping.set.cardinality")]),s._v(" 配置的方式自动控制作业的拆解，该参数默认值是30。表示针对grouping sets/rollups/cubes这类多维聚合的操作，如果最后拆解的键组合大于该值，会启用新的任务去处理大于该值之外的组合。如果在处理数据时，某个分组聚合的列有较大的倾斜，可以适当调小该值。")]),s._v(" "),a("h3",{attrs:{id:"_5-表连接时引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-表连接时引发的数据倾斜"}},[s._v("#")]),s._v(" 5. 表连接时引发的数据倾斜")]),s._v(" "),a("p",[s._v("两表进行普通的repartition join时，如果表连接的键存在倾斜，那么在 Shuffle 阶段必然会引起数据倾斜。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案")]),s._v("：")]),s._v(" "),a("p",[s._v("通常做法是将倾斜的数据存到分布式缓存中，分发到各个 Map任务所在节点。在Map阶段完成join操作，即MapJoin，这避免了 Shuffle，从而避免了数据倾斜。")]),s._v(" "),a("blockquote",[a("p",[s._v("MapJoin是Hive的一种优化操作，"),a("strong",[s._v("其适用于小表JOIN大表的场景")]),s._v("，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率。")])]),s._v(" "),a("p",[s._v("在Hive 0.11版本之前，如果想在Map阶段完成join操作，必须使用MAPJOIN来标记显示地启动该优化操作，"),a("strong",[s._v("由于其需要将小表加载进内存所以要注意小表的大小")]),s._v("。")]),s._v(" "),a("p",[s._v("如将a表放到Map端内存中执行，在Hive 0.11版本之前需要这样写：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("/* +mapjoin(a) */")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("age \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("join")]),s._v(" b \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("on")]),s._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])])]),a("p",[s._v("如果想将多个表放到Map端内存中，只需在mapjoin()中写多个表名称即可，用逗号分隔，如将a表和c表放到Map端内存中，则 "),a("code",[s._v("/* +mapjoin(a,c) */")]),s._v(" 。")]),s._v(" "),a("p",[s._v("在Hive 0.11版本及之后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机：")]),s._v(" "),a("p",[a("code",[s._v("hive.auto.convert.join=true")]),s._v(" 默认值为true，自动开启MAPJOIN优化。")]),s._v(" "),a("p",[a("code",[s._v("hive.mapjoin.smalltable.filesize=2500000")]),s._v(" 默认值为2500000(25M)，通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中。")]),s._v(" "),a("p",[a("strong",[s._v("注意")]),s._v("：使用默认启动该优化的方式如果出现莫名其妙的BUG(比如MAPJOIN并不起作用)，就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化:")]),s._v(" "),a("p",[a("code",[s._v("hive.auto.convert.join=false")]),s._v(" (关闭自动MAPJOIN转换操作)")]),s._v(" "),a("p",[a("code",[s._v("hive.ignore.mapjoin.hint=false")]),s._v(" (不忽略MAPJOIN标记)")]),s._v(" "),a("p",[s._v("再提一句：将表放到Map端内存时，如果节点的内存很大，但还是出现内存溢出的情况，我们可以通过这个参数 "),a("code",[s._v("mapreduce.map.memory.mb")]),s._v(" 调节Map端内存的大小。")]),s._v(" "),a("h3",{attrs:{id:"_6-确实无法减少数据量引发的数据倾斜"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-确实无法减少数据量引发的数据倾斜"}},[s._v("#")]),s._v(" 6. 确实无法减少数据量引发的数据倾斜")]),s._v(" "),a("p",[s._v("在一些操作中，我们没有办法减少数据量，如在使用 collect_list 函数时：")]),s._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("select")]),s._v(" s_age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("collect_list"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("s_score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" list_score\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" student\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("group")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("by")]),s._v(" s_age\n")])])]),a("blockquote",[a("p",[s._v("collect_list：将分组中的某列转为一个数组返回。")])]),s._v(" "),a("p",[s._v("在上述sql中，s_age有数据倾斜，但如果数据量大到一定的数量，会导致处理倾斜的Reduce任务产生内存溢出的异常。")]),s._v(" "),a("blockquote",[a("p",[s._v("collect_list输出一个数组，中间结果会放到内存中，所以如果collect_list聚合太多数据，会导致内存溢出。")])]),s._v(" "),a("p",[s._v("有小伙伴说这是 group by 分组引起的数据倾斜，可以开启"),a("code",[s._v("hive.groupby.skewindata")]),s._v("参数来优化。我们接下来分析下：")]),s._v(" "),a("p",[s._v("开启该配置会将作业拆解成两个作业，第一个作业会尽可能将Map的数据平均分配到Reduce阶段，并在这个阶段实现数据的预聚合，以减少第二个作业处理的数据量；第二个作业在第一个作业处理的数据基础上进行结果的聚合。")]),s._v(" "),a("p",[a("code",[s._v("hive.groupby.skewindata")]),s._v("的核心作用在于生成的第一个作业能够有效减少数量。但是对于collect_list这类要求全量操作所有数据的中间结果的函数来说，明显起不到作用，反而因为引入新的作业增加了磁盘和网络I/O的负担，而导致性能变得更为低下。")]),s._v(" "),a("p",[a("strong",[s._v("解决方案")]),s._v("：")]),s._v(" "),a("p",[s._v("这类问题最直接的方式就是调整reduce所执行的内存大小。")]),s._v(" "),a("p",[s._v("调整reduce的内存大小使用"),a("code",[s._v("mapreduce.reduce.memory.mb")]),s._v("这个配置。")]),s._v(" "),a("h3",{attrs:{id:"总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[s._v("#")]),s._v(" 总结")]),s._v(" "),a("p",[s._v("通过上面的内容我们发现，"),a("strong",[s._v("shuffle阶段堪称性能的杀手")]),s._v("，为什么这么说，一方面shuffle阶段是最容易引起数据倾斜的；另一方面shuffle的过程中会产生大量的磁盘I/O、网络I/O 以及压缩、解压缩、序列化和反序列化等。这些操作都是严重影响性能的。")]),s._v(" "),a("p",[s._v("所以围绕shuffle和数据倾斜有很多的调优点：")]),s._v(" "),a("ul",[a("li",[s._v("Mapper 端的Buffer 设置为多大？Buffer 设置得大，可提升性能，减少磁盘I/O ，但 是对内存有要求，对GC 有压力；Buffer 设置得小，可能不占用那么多内存， 但是可能频繁的磁盘I/O 、频繁的网络I/O 。")])]),s._v(" "),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[s._v("#")]),s._v(" 参考文章")]),s._v(" "),a("ul",[a("li",[s._v("https://blog.csdn.net/weixin_44291548/article/details/119765072")])])])}),[],!1,null,null,null);t.default=n.exports}}]);