(window.webpackJsonp=window.webpackJsonp||[]).push([[490],{1005:function(t,s,a){"use strict";a.r(s);var e=a(53),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("本文主要是介绍 Hive优化-优化策略总结 。")])]),t._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#hive优化策略"}},[t._v("Hive优化策略")]),a("ul",[a("li",[a("a",{attrs:{href:"#hive优化目标"}},[t._v("hive优化目标")])]),a("li",[a("a",{attrs:{href:"#查看运行计划"}},[t._v("查看运行计划")])]),a("li",[a("a",{attrs:{href:"#hive运行过程"}},[t._v("HIVE运行过程")])]),a("li",[a("a",{attrs:{href:"#hive表优化"}},[t._v("hive表优化")])]),a("li",[a("a",{attrs:{href:"#hive-job优化"}},[t._v("Hive job优化")])]),a("li",[a("a",{attrs:{href:"#hive-sql语句优化"}},[t._v("Hive SQL语句优化")])]),a("li",[a("a",{attrs:{href:"#hive-map-reduce优化"}},[t._v("Hive Map/Reduce优化")])])])]),a("li",[a("a",{attrs:{href:"#参考文章"}},[t._v("参考文章")])])])]),a("p"),t._v(" "),a("h2",{attrs:{id:"hive优化策略"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive优化策略"}},[t._v("#")]),t._v(" Hive优化策略")]),t._v(" "),a("h3",{attrs:{id:"hive优化目标"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive优化目标"}},[t._v("#")]),t._v(" "),a("strong",[t._v("hive优化目标")])]),t._v(" "),a("p",[t._v("在有限的资源下，运行效率高。")]),t._v(" "),a("p",[a("strong",[t._v("常见问题")]),t._v("\n数据倾斜、Map数设置、Reduce数设置等")]),t._v(" "),a("p",[a("strong",[t._v("hive运行")])]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/optstrategy-1.png"),alt:"wxmp"}}),t._v(" "),a("h3",{attrs:{id:"查看运行计划"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#查看运行计划"}},[t._v("#")]),t._v(" "),a("strong",[t._v("查看运行计划")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extended")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" hql\n")])])]),a("p",[a("strong",[t._v("例子")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("no")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" testudf "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("no")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("explain")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extended")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("no")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" testudf "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("no")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[a("strong",[t._v("运行阶段")]),t._v("\nSTAGE DEPENDENC1ES:")]),t._v(" "),a("p",[t._v("Stage-1 is a root stage")]),t._v(" "),a("p",[t._v("Stage-0 is a root stage")]),t._v(" "),a("p",[a("strong",[t._v("Map阶段")])]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("      Map Operator Tree:\n          TableScan\n            alias: testudf\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats:                               NONE\n            Select Operator\n              expressions: no "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n              outputColumnNames: no\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats                              "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" NONE\n              Group By Operator\n                aggregations: count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                keys: no "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                mode: "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("hash")]),t._v("\n                outputColumnNames: _col0, _col1\n                Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column sta                              ts: NONE\n                Reduce Output Operator\n                  key expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                  "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sort")]),t._v(" order: +\n                  Map-reduce partition columns: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                  Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column s                              tats: NONE\n                  value expressions: _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: bigint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[a("strong",[t._v("reduce阶段")])]),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("      Reduce Operator Tree:\n        Group By Operator\n          aggregations: count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VALUE._col0"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          keys: KEY._col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          mode: mergepartial\n          outputColumnNames: _col0, _col1\n          Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NONE\n          Select Operator\n            expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: bigint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            outputColumnNames: _col0, _col1\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NONE\n            File Output Operator\n              compressed: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NO                              NE\n              table:\n                  input format: org.apache.hadoop.mapred.TextInputFormat\n                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutput                              Format\n                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n")])])]),a("hr"),t._v(" "),a("div",{staticClass:"language-shell extra-class"},[a("pre",{pre:!0,attrs:{class:"language-shell"}},[a("code",[t._v("hive "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("liguodong"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" explain extended "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" no,count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("*"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" from testudf group by no"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nOK\nExplain\nABSTRACT SYNTAX TREE:\n\nTOK_QUERY\n   TOK_FROM\n      TOK_TABREF\n         TOK_TABNAME\n            testudf\n   TOK_INSERT\n      TOK_DESTINATION\n         TOK_DIR\n            TOK_TMP_FILE\n      TOK_SELECT\n         TOK_SELEXPR\n            TOK_TABLE_OR_COL\n               no\n         TOK_SELEXPR\n            TOK_FUNCTIONSTAR\n               count\n      TOK_GROUPBY\n         TOK_TABLE_OR_COL\n            no\n\n\nSTAGE DEPENDENCIES:\n  Stage-1 is a root stage\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-1\n    Map Reduce\n      Map Operator Tree:\n          TableScan\n            alias: testudf\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats: NONE\n            GatherStats: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n            Select Operator\n              expressions: no "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n              outputColumnNames: no\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats: NONE\n              Group By Operator\n                aggregations: count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                keys: no "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                mode: "),a("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("hash")]),t._v("\n                outputColumnNames: _col0, _col1\n                Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats: NONE\n                Reduce Output Operator\n                  key expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                  "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sort")]),t._v(" order: +\n                  Map-reduce partition columns: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                  Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" Basic stats: PARTIAL Column stats: NONE\n                  tag: -1\n                  value expressions: _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: bigint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      Path -"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Alias:\n        hdfs://nameservice1/user/hive/warehouse/liguodong.db/testudf "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("testudf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n      Path -"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Partition:\n        hdfs://nameservice1/user/hive/warehouse/liguodong.db/testudf\n          Partition\n            base "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("file")]),t._v(" name: testudf\n            input format: org.apache.hadoop.mapred.TextInputFormat\n            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n            properties:\n              COLUMN_STATS_ACCURATE "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n              bucket_count -1\n              columns no,num\n              columns.comments\n              columns.types string:string\n              field.delim\n              file.inputformat org.apache.hadoop.mapred.TextInputFormat\n              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n              line.delim\n\n              location hdfs://nameservice1/user/hive/warehouse/liguodong.db/testudf\n              name liguodong.testudf\n              numFiles "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n              numRows "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n              rawDataSize "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n              serialization.ddl struct testudf "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" string no, string num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n              serialization.format\n              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n              totalSize "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\n              transient_lastDdlTime "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1437374988")]),t._v("\n            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n\n              input format: org.apache.hadoop.mapred.TextInputFormat\n              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n              properties:\n                COLUMN_STATS_ACCURATE "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n                bucket_count -1\n                columns no,num\n                columns.comments\n                columns.types string:string\n                field.delim\n                file.inputformat org.apache.hadoop.mapred.TextInputFormat\n                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                line.delim\n\n                location hdfs://nameservice1/user/hive/warehouse/liguodong.db/testudf\n                name liguodong.testudf\n                numFiles "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                numRows "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n                rawDataSize "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n                serialization.ddl struct testudf "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" string no, string num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n                serialization.format\n                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                totalSize "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\n                transient_lastDdlTime "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1437374988")]),t._v("\n              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n              name: liguodong.testudf\n            name: liguodong.testudf\n      Truncated Path -"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" Alias:\n        /liguodong.db/testudf "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("testudf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n      Needs Tagging: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n      Reduce Operator Tree:\n        Group By Operator\n          aggregations: count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VALUE._col0"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          keys: KEY._col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n          mode: mergepartial\n          outputColumnNames: _col0, _col1\n          Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NONE\n          Select Operator\n            expressions: _col0 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(", _col1 "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type: bigint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            outputColumnNames: _col0, _col1\n            Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NONE\n            File Output Operator\n              compressed: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n              GlobalTableId: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n              directory: hdfs://nameservice1/tmp/hive-root/hive_2015-07-21_09-51-37_330_7990199479532530033-1/-mr-10000/.hive-staging_hive_2015-07-21_09-51-37_330_7990199479532530033-1/-ext-10001\n              NumFilesPerFileSink: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n              Statistics: Num rows: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Data size: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" Basic stats: NONE Column stats: NONE\n              Stats Publishing Key Prefix: hdfs://nameservice1/tmp/hive-root/hive_2015-07-21_09-51-37_330_7990199479532530033-1/-mr-10000/.hive-staging_hive_2015-07-21_09-51-37_330_7990199479532530033-1/-ext-10001/\n              table:\n                  input format: org.apache.hadoop.mapred.TextInputFormat\n                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                  properties:\n                    columns _col0,_col1\n                    columns.types string:bigint\n                    escape.delim "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("\\")]),t._v("\n                    hive.serialization.extend.nesting.levels "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n                    serialization.format "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n              TotalFiles: "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n              GatherStats: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n              MultiFileSpray: "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n\n  Stage: Stage-0\n    Fetch Operator\n      limit: -1\n")])])]),a("h3",{attrs:{id:"hive运行过程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive运行过程"}},[t._v("#")]),t._v(" "),a("strong",[t._v("HIVE运行过程")])]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/optstrategy-2.png"),alt:"wxmp"}}),t._v(" "),a("h3",{attrs:{id:"hive表优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive表优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("hive表优化")])]),t._v(" "),a("h4",{attrs:{id:"分区"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分区"}},[t._v("#")]),t._v(" "),a("strong",[t._v("分区")])]),t._v(" "),a("p",[t._v("静态分区\n动态分区")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.dynamic.partition"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\nset hive.exec.dynamic.partltlon.mode"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("nonstrict;")]),t._v("\n")])])]),a("h4",{attrs:{id:"分桶"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分桶"}},[t._v("#")]),t._v(" "),a("strong",[t._v("分桶")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.enforce.bucketing"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\nset hive.enforce.sorting"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\n")])])]),a("p",[t._v("**表优化数据目标：**同样数据尽量聚集在一起")]),t._v(" "),a("h3",{attrs:{id:"hive-job优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-job优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Hive job优化")])]),t._v(" "),a("h4",{attrs:{id:"并行化运行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#并行化运行"}},[t._v("#")]),t._v(" "),a("strong",[t._v("并行化运行")])]),t._v(" "),a("p",[t._v("每一个查询被hive转化成多个阶段，有些阶段关联性不大，则能够并行化运行，降低运行时问。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.parallel"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\nset hive.exec.parallel.thread.number"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("8;")]),t._v("\n")])])]),a("p",[t._v("eg:")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" num \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("city"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" city\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("all")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("province"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" num "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" province\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("tmp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h4",{attrs:{id:"本地化运行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#本地化运行"}},[t._v("#")]),t._v(" "),a("strong",[t._v("本地化运行")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.mode.local.auto"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\n")])])]),a("p",[t._v("当一个job满足例如以下条件才干真正使用本地模式：\n1.job的输入数据大小必须小于參数：")]),t._v(" "),a("p",[a("code",[t._v("hive.exec.mode.local.inputbytes.max")]),t._v("（默认128MB）")]),t._v(" "),a("p",[t._v("2.job的map数必须小于參数：")]),t._v(" "),a("p",[a("code",[t._v("hive.exec.mode.local.auto.tasks.max")]),t._v("（默认4)")]),t._v(" "),a("p",[t._v("3.job的reduce数必须为0或者1")]),t._v(" "),a("h4",{attrs:{id:"job合并输入小文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#job合并输入小文件"}},[t._v("#")]),t._v(" "),a("strong",[t._v("job合并输入小文件")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.input.format"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")])]),t._v("\norg.apache.hadoop.hive.ql.io.CombineHiveInputFormat\n")])])]),a("p",[t._v("合并文件数由"),a("code",[t._v("mapred.max.split.size")]),t._v("限制的大小决定。")]),t._v(" "),a("h4",{attrs:{id:"job合并输出小文件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#job合并输出小文件"}},[t._v("#")]),t._v(" "),a("strong",[t._v("job合并输出小文件")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.merge.smallfiles.avgsize"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("256000000;当输出文件平均大小小于该值。启动新job合并文件")]),t._v("\nset hive.merge.size.per.task"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("64000000;合并之后的文件大小")]),t._v("\n")])])]),a("h4",{attrs:{id:"jvm重利用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#jvm重利用"}},[t._v("#")]),t._v(" "),a("strong",[t._v("JVM重利用")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set mapred.job.reuse.jvm.num.tasks"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("20;")]),t._v("\n")])])]),a("p",[t._v("JVM重利用能够是job长时间保留slot，直到作业结束，这在对于有较多任务和较多小文件的任务是很有意义的，降低运行时间。当然这个值不能设置过大，由于有些作业会有reduce任务，假设reduce任务没有完毕，则map任务占用的slot不能释放。其它的作业可能就须要等待。")]),t._v(" "),a("h4",{attrs:{id:"压缩数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#压缩数据"}},[t._v("#")]),t._v(" "),a("strong",[t._v("压缩数据")])]),t._v(" "),a("p",[a("strong",[t._v("中间压缩")]),t._v("就是处理hive查询的多个job之间的数据。对中间压缩，\n最好选择一个节省CPU耗时的压缩方式。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.compress.intermediate"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true。")]),t._v("\nset hive.intermediate.compression.codec"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("org.apache.hadoop.io.compress.SnappyCodec;")]),t._v("\nset hive.intermediate.compression.type"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("BLOCK;")]),t._v("\n")])])]),a("p",[a("strong",[t._v("终于的输出也能够压缩")]),t._v(",选择一个压缩效果比較好的，节省了磁盘空间，可是cpu比較耗时。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.compress.output"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;")]),t._v("\nset mapred.output.compression.codec"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")])]),t._v("\norg.apache.hadoop.io.compress.GzipCodec;\nset mapred.output.compression.type"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("BLOCK:")]),t._v("\n")])])]),a("h3",{attrs:{id:"hive-sql语句优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-sql语句优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Hive SQL语句优化")])]),t._v(" "),a("h4",{attrs:{id:"join优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#join优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("join优化")])]),t._v(" "),a("p",[a("code",[t._v("hive.optimize.skewjoin=true;")]),t._v(" 假设是join过程出现倾斜应该设置为true")]),t._v(" "),a("p",[a("code",[t._v("set hive.skewjoin.key=100000;")]),t._v(" 这个是join的键相应的记录条数超过这个值则会进行优化。")]),t._v(" "),a("h4",{attrs:{id:"mapjoin"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mapjoin"}},[t._v("#")]),t._v(" "),a("strong",[t._v("mapjoin")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("自己主动运行\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auto"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("convert")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\nhive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mapjoin"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("smalltable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filesize默认值是"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v("mb   \n\n手动运行\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*+mapjoin(A)*/")]),t._v(" f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" A t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" B f "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("简单总结一下，mapjoin的使用场景：")]),t._v(" "),a("ul",[a("li",[t._v("1、关联操作中有一张表很小")]),t._v(" "),a("li",[t._v("2、(不等值)的链接操作时")])]),t._v(" "),a("p",[a("strong",[t._v("注")]),t._v("：小表尽量设置小一点或用手动方式。")]),t._v(" "),a("h4",{attrs:{id:"bucket-join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bucket-join"}},[t._v("#")]),t._v(" "),a("strong",[t._v("bucket join")])]),t._v(" "),a("p",[t._v("两个表以同样方式划分捅。")]),t._v(" "),a("p",[t._v("两个表的桶个数是倍数关系。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" ordertab"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cid "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("price"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("clustered")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("into")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v(" buckets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" customer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("first")]),t._v(" string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("clustered")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("into")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v(" buckets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" price "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ordertab t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" customer s "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id\n")])])]),a("h4",{attrs:{id:"改动where的位置进行优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#改动where的位置进行优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("改动where的位置进行优化")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v("优化前\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("order")]),t._v(" m "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" customer u "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2013-12-12\n\njoin优化后\nselect m.cid, u.id from\n(select cid from order where dt='")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2013")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v("'"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" m\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" customer u "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cid"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("u"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n这样就能降低"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v("连接的数据量。\n")])])]),a("h4",{attrs:{id:"group-by优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#group-by优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("group by优化")])]),t._v(" "),a("p",[a("code",[t._v("hive.groupby.skewindata=true;")])]),t._v(" "),a("p",[t._v("假设是group by过程出现倾斜应该设置为true。")]),t._v(" "),a("p",[a("code",[t._v("set hive.groupby.mapaggr.checkinterval=100000;")])]),t._v(" "),a("p",[t._v("这个是group的键相应的记录条数超过这个值则会进行优化。")]),t._v(" "),a("h4",{attrs:{id:"count-distinct优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#count-distinct优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("count distinct优化")])]),t._v(" "),a("p",[t._v("优化前（启动一个job，数据量大时，一个reduce负载过重）\n"),a("code",[t._v("select count(distinct id) from tablename;")])]),t._v(" "),a("p",[t._v("优化后（启动两个job）")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tablename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("tmp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tablename "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("tmp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h4",{attrs:{id:"union-all优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#union-all优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("union all优化")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("优化前\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" test "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n优化后\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" d\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" test "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("all")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" test "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("d\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("all")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" c"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" test\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("tmpl \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"hive-map-reduce优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive-map-reduce优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Hive Map/Reduce优化")])]),t._v(" "),a("h4",{attrs:{id:"map优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#map优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Map优化")])]),t._v(" "),a("p",[a("strong",[t._v("改动map个数进行优化")])]),t._v(" "),a("p",[a("strong",[t._v("直接设置mapred.map.tasks无效")]),t._v(" "),a("code",[t._v("set mapred.map.tasks=10。")])]),t._v(" "),a("p",[a("strong",[t._v("map个数的计算过程")]),t._v("\n（1）默认map个数")]),t._v(" "),a("p",[a("code",[t._v("default_num=total_size/block_size;")])]),t._v(" "),a("p",[t._v("（2）期望大小")]),t._v(" "),a("p",[a("code",[t._v("goal_num=mapred.map.tasks;")])]),t._v(" "),a("p",[t._v("（3）设置处理的文件大小")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("split_size")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("max(mapred.min.split.size,b1ock_size);")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("split_num")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("total_size/split_size;")]),t._v("\n")])])]),a("p",[t._v("(4)计算的map个数\n"),a("code",[t._v("compute_map_num=min(split_num,max(default_num,goal_num))")])]),t._v(" "),a("p",[t._v("经过以上的分析。在设置map个数的时候，能够简单的总结为下面几点：")]),t._v(" "),a("ul",[a("li",[t._v("1）假设想添加map个数，则设置mapred.map.tasks为一个较大的值。")]),t._v(" "),a("li",[t._v("2）假设想减小map个数。则设置mapred.min.split.size为一个较大的值。有例如以下两种情况：")])]),t._v(" "),a("p",[t._v("情况1：输入文件size巨大。但不是小文件增大"),a("code",[t._v("mapred.min.split.size")]),t._v("的值。")]),t._v(" "),a("p",[t._v("情况2：输入文件数量巨大，且都是小文件，就是单个文件的size小于blockSize。")]),t._v(" "),a("p",[t._v("这样的情况通过增大mapred.min.spllt.size不可行，")]),t._v(" "),a("p",[t._v("须要使用"),a("code",[t._v("CombineFileInputFormat")]),t._v("将多个input path合并成一个")]),t._v(" "),a("p",[t._v("InputSplit送给mapper处理，从而降低mapper的数量。")]),t._v(" "),a("p",[a("strong",[t._v("map端聚合")]),t._v("\nmap阶段进行combiner\n"),a("code",[t._v("set hive.map.aggr=true:")])]),t._v(" "),a("p",[a("strong",[t._v("猜測运行")]),t._v("\n启动多个同样的map，谁先运行完。用谁的。\n"),a("code",[t._v("set mapred.map.tasks.speculative.execution=true")])]),t._v(" "),a("hr"),t._v(" "),a("h4",{attrs:{id:"shuffle优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#shuffle优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("shuffle优化")])]),t._v(" "),a("p",[t._v("依据须要配置相应參数。\n"),a("strong",[t._v("Map端")])]),t._v(" "),a("ul",[a("li",[t._v("io.sort.mb")]),t._v(" "),a("li",[t._v("io.sort.spill.percent")]),t._v(" "),a("li",[t._v("min.num.spill.for.combine")]),t._v(" "),a("li",[t._v("io.sort.factor")]),t._v(" "),a("li",[t._v("io.sort.record.percent")])]),t._v(" "),a("p",[a("strong",[t._v("Reduce端")])]),t._v(" "),a("ul",[a("li",[t._v("mapred.reduce.parallel.copies")]),t._v(" "),a("li",[t._v("mapred.reduce.copy.backoff")]),t._v(" "),a("li",[t._v("io.sort.factor")]),t._v(" "),a("li",[t._v("mapred.job.shuffle.input.buffer.percent")]),t._v(" "),a("li",[t._v("mapred.job.reduce.input.buffer.percent")])]),t._v(" "),a("hr"),t._v(" "),a("h4",{attrs:{id:"reduce优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#reduce优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("Reduce优化")])]),t._v(" "),a("p",[t._v("须要reduce操作的查询\n聚合函数"),a("code",[t._v("sum,count,distinct")]),t._v("\n高级查询"),a("code",[t._v("group by,join,distribute by,cluster by…")])]),t._v(" "),a("p",[a("code",[t._v("order by")]),t._v("比較特殊，仅仅须要一个reduce，设置reduce个数无效。")]),t._v(" "),a("hr"),t._v(" "),a("p",[a("strong",[t._v("判断运行")]),t._v("\n设置"),a("code",[t._v("mapred.reduce.tasks.speculative.execution")]),t._v("或者"),a("code",[t._v("hive.mapred.reduce.tasks.speculative.execution")]),t._v("效果都一样。")]),t._v(" "),a("p",[a("strong",[t._v("设置Reduce")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set mapred.reduce.tasks"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("10; 直接设置")]),t._v("\nhive.exec.reducers.max 默认：999\nhive.exec.reducers.bytes.per.reducer 默认:1G\n")])])]),a("p",[a("strong",[t._v("计算公式")])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("maxReducers")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("`hive.exec.reducers.max")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("perReducer")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("`hive.exec.reducers.bytes.per.reducer")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token constant"}},[t._v("numRTasks")]),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("`min[maxReducers,input.size/perReducer]")]),t._v("\n")])])]),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),a("ul",[a("li",[t._v("https://www.cnblogs.com/claireyuancy/p/7224529.html")])])])}),[],!1,null,null,null);s.default=n.exports}}]);