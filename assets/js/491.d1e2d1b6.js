(window.webpackJsonp=window.webpackJsonp||[]).push([[491],{1006:function(t,a,s){"use strict";s.r(a);var e=s(53),r=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),s("p",[t._v("本文主要是介绍 Hive优化-常用优化方法 。")])]),t._v(" "),s("p"),s("div",{staticClass:"table-of-contents"},[s("ul",[s("li",[s("a",{attrs:{href:"#hive优化-常用优化方法"}},[t._v("Hive优化-常用优化方法")])]),s("li",[s("a",{attrs:{href:"#列裁剪和分区裁剪"}},[t._v("列裁剪和分区裁剪")])]),s("li",[s("a",{attrs:{href:"#谓词下推"}},[t._v("谓词下推")])]),s("li",[s("a",{attrs:{href:"#sort-by代替order-by"}},[t._v("sort by代替order by")])]),s("li",[s("a",{attrs:{href:"#group-by代替distinct"}},[t._v("group by代替distinct")])]),s("li",[s("a",{attrs:{href:"#group-by配置调整"}},[t._v("group by配置调整")])]),s("li",[s("a",{attrs:{href:"#join基础优化"}},[t._v("join基础优化")])]),s("li",[s("a",{attrs:{href:"#优化sql处理join数据倾斜"}},[t._v("优化SQL处理join数据倾斜")])]),s("li",[s("a",{attrs:{href:"#mapreduce优化"}},[t._v("MapReduce优化")])]),s("li",[s("a",{attrs:{href:"#并行执行与本地模式"}},[t._v("并行执行与本地模式")])]),s("li",[s("a",{attrs:{href:"#严格模式"}},[t._v("严格模式")])]),s("li",[s("a",{attrs:{href:"#采用合适的存储格式"}},[t._v("采用合适的存储格式")])]),s("li",[s("a",{attrs:{href:"#参考文章"}},[t._v("参考文章")])])])]),s("p"),t._v(" "),s("h2",{attrs:{id:"hive优化-常用优化方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hive优化-常用优化方法"}},[t._v("#")]),t._v(" Hive优化-常用优化方法")]),t._v(" "),s("p",[t._v("Hive作为"),s("a",{attrs:{href:"https://cloud.tencent.com/solution/bigdata?from=10680",target:"_blank",rel:"noopener noreferrer"}},[t._v("大数据"),s("OutboundLink")],1),t._v("领域常用的数据仓库组件，在平时设计和查询时要特别注意效率。影响Hive效率的几乎从不是数据量过大，而是数据倾斜、数据冗余、job或I/O过多、MapReduce分配不合理等等。对Hive的调优既包含对HiveSQL语句本身的优化，也包含Hive配置项和MR方面的调整。")]),t._v(" "),s("h2",{attrs:{id:"列裁剪和分区裁剪"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#列裁剪和分区裁剪"}},[t._v("#")]),t._v(" "),s("strong",[t._v("列裁剪和分区裁剪")])]),t._v(" "),s("p",[t._v("最基本的操作。所谓列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。以我们的日历记录表为例：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("record_data\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190201")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190224")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("status")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("当列很多或者数据量很大时，如果select *或者不指定分区，全列扫描和全表扫描效率都很低。 Hive中与列裁剪优化相关的配置项是"),s("code",[t._v("hive.optimize.cp")]),t._v("，与分区裁剪优化相关的则是"),s("code",[t._v("hive.optimize.pruner")]),t._v("，默认都是true。在HiveSQL解析阶段对应的则是ColumnPruner逻辑优化器。")]),t._v(" "),s("h2",{attrs:{id:"谓词下推"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#谓词下推"}},[t._v("#")]),t._v(" "),s("strong",[t._v("谓词下推")])]),t._v(" "),s("p",[t._v("在关系型"),s("a",{attrs:{href:"https://cloud.tencent.com/solution/database?from=10680",target:"_blank",rel:"noopener noreferrer"}},[t._v("数据库"),s("OutboundLink")],1),t._v("如"),s("a",{attrs:{href:"https://cloud.tencent.com/product/cdb?from=10680",target:"_blank",rel:"noopener noreferrer"}},[t._v("MySQL"),s("OutboundLink")],1),t._v("中，也有谓词下推（Predicate Pushdown，PPD）的概念。它就是将SQL语句中的where谓词逻辑都尽可能提前执行，减少下游处理的数据量。 例如以下HiveSQL语句：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("topic_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("title\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("outer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("topic_id"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("title "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" forum_topic\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190224")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" length"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("content"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190224")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("status")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("对forum_topic做过滤的where语句写在子查询内部，而不是外部。Hive中有谓词下推优化的配置项"),s("code",[t._v("hive.optimize.ppd")]),t._v("，默认值true，与它对应的逻辑优化器是PredicatePushDown。该优化器就是将OperatorTree中的FilterOperator向上提，见下图。")]),t._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/commonopt-1.png"),alt:"wxmp"}}),t._v(" "),s("p",[t._v("图来自https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html")]),t._v(" "),s("p",[t._v("上面的链接中是一篇讲解HiveSQL解析与执行过程的好文章，前文提到的优化器、OperatorTree等概念在其中也有详细的解释，非常推荐。")]),t._v(" "),s("h2",{attrs:{id:"sort-by代替order-by"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sort-by代替order-by"}},[t._v("#")]),t._v(" "),s("strong",[t._v("sort by代替order by")])]),t._v(" "),s("p",[t._v("HiveSQL中的order by与其他SQL方言中的功能一样，就是将结果按某字段全局排序，这会导致所有map端数据都进入一个reducer中，在数据量大时可能会长时间计算不完。")]),t._v(" "),s("p",[t._v("如果使用sort by，那么还是会视情况启动多个reducer进行排序，并且保证每个reducer内局部有序。为了控制map端数据分配到reducer的key，往往还要配合distribute by一同使用。如果不加distribute by的话，map端数据就会随机分配到reducer。 举个例子，假如要以UID为key，以上传时间倒序、记录类型倒序输出记录数据：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("upload_time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("record_data\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190201")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190224")]),t._v("\ndistribute "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" uid\nsort "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" upload_time "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("desc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("desc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h2",{attrs:{id:"group-by代替distinct"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#group-by代替distinct"}},[t._v("#")]),t._v(" "),s("strong",[t._v("group by代替distinct")])]),t._v(" "),s("p",[t._v("当要统计某一列的去重数时，如果数据量很大，count(distinct)就会非常慢，原因与order by类似，count(distinct)逻辑只会有很少的reducer来处理。这时可以用group by来改写：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190101")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" uid\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("但是这样写会启动两个MR job（单纯distinct只会启动一个），所以要确保数据量大到启动job的overhead远小于计算耗时，才考虑这种方法。当数据集很小或者key的倾斜比较明显时，group by还可能会比distinct慢。 那么如何用group by方式同时统计多个列？下面是解决方法：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" d "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" some_table\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("all")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" d "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" some_table "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("union")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("all")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("d "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" some_table "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("d\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h2",{attrs:{id:"group-by配置调整"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#group-by配置调整"}},[t._v("#")]),t._v(" "),s("strong",[t._v("group by配置调整")])]),t._v(" "),s("h4",{attrs:{id:"map端预聚合"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#map端预聚合"}},[t._v("#")]),t._v(" "),s("strong",[t._v("map端预聚合")])]),t._v(" "),s("p",[t._v("group by时，如果先起一个combiner在map端做部分预聚合，可以有效减少shuffle数据量。预聚合的配置项是"),s("code",[t._v("hive.map.aggr")]),t._v("，默认值true，对应的优化器为GroupByOptimizer，简单方便。 通过"),s("code",[t._v("hive.groupby.mapaggr.checkinterval")]),t._v("参数也可以设置map端预聚合的行数阈值，超过该值就会分拆job，默认值100000。")]),t._v(" "),s("h4",{attrs:{id:"倾斜均衡配置项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#倾斜均衡配置项"}},[t._v("#")]),t._v(" "),s("strong",[t._v("倾斜均衡配置项")])]),t._v(" "),s("p",[t._v("group by时如果某些key对应的数据量过大，就会发生数据倾斜。Hive自带了一个均衡数据倾斜的配置项"),s("code",[t._v("hive.groupby.skewindata")]),t._v("，默认值false。 其实现方法是在group by时启动两个MR job。第一个job会将map端数据随机输入reducer，每个reducer做部分聚合，相同的key就会分布在不同的reducer中。第二个job再将前面预处理过的数据按key聚合并输出结果，这样就起到了均衡的效果。 但是，配置项毕竟是死的，单纯靠它有时不能根本上解决问题，因此还是建议自行了解数据倾斜的细节，并优化查询语句。")]),t._v(" "),s("h2",{attrs:{id:"join基础优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#join基础优化"}},[t._v("#")]),t._v(" "),s("strong",[t._v("join基础优化")])]),t._v(" "),s("p",[t._v("join优化是一个复杂的话题，下面先说5点最基本的注意事项。")]),t._v(" "),s("h4",{attrs:{id:"build-table-小表-前置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#build-table-小表-前置"}},[t._v("#")]),t._v(" "),s("strong",[t._v("build table（小表）前置")])]),t._v(" "),s("p",[t._v("在最常见的hash join方法中，一般总有一张相对小的表和一张相对大的表，小表叫build table，大表叫probe table。如下图所示。")]),t._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/commonopt-2.png"),alt:"wxmp"}}),t._v(" "),s("p",[t._v("图来自http://hbasefly.com/2017/03/19/sparksql-basic-join/")]),t._v(" "),s("p",[t._v("Hive在解析带join的SQL语句时，会默认将最后一个表作为probe table，将前面的表作为build table并试图将它们读进内存。如果表顺序写反，probe table在前面，引发OOM的风险就高了。 在维度建模数据仓库中，事实表就是probe table，维度表就是build table。假设现在要将日历记录事实表和记录项编码维度表来join：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_code"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_desc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("upload_time\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_event_code a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("upload_time "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190225")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h4",{attrs:{id:"多表join时key相同"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多表join时key相同"}},[t._v("#")]),t._v(" "),s("strong",[t._v("多表join时key相同")])]),t._v(" "),s("p",[t._v("这种情况会将多个join合并为一个MR job来处理，例如：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_code"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_desc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("upload_time\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_event_code a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("upload_time "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190225")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("upload_time "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log_2\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190225")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" c "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("如果上面两个join的条件不相同，比如改成"),s("code",[t._v("a.event_code = c.event_code")]),t._v("，就会拆成两个MR job计算。 负责这个的是相关性优化器CorrelationOptimizer，它的功能除此之外还非常多，逻辑复杂，参考Hive官方的文档可以获得更多细节：https://cwiki.apache.org/confluence/display/Hive/Correlation+Optimizer")]),t._v(" "),s("h4",{attrs:{id:"利用map-join特性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#利用map-join特性"}},[t._v("#")]),t._v(" "),s("strong",[t._v("利用map join特性")])]),t._v(" "),s("p",[t._v("map join特别适合大小表join的情况。Hive会将build table和probe table在map端直接完成join过程，消灭了reduce，效率很高。")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("  a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("upload_time\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_event_code a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("upload_time "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190225")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("上面的语句中加了一条map join hint，以显式启用map join特性。早在Hive 0.8版本之后，就不需要写这条hint了。map join还支持不等值连接，应用更加灵活。 map join的配置项是"),s("code",[t._v("hive.auto.convert.join")]),t._v("，默认值true，对应逻辑优化器是MapJoinProcessor。 还有一些参数用来控制map join的行为，比如"),s("code",[t._v("hive.mapjoin.smalltable.filesize")]),t._v("，当build table大小小于该值就会启用map join，默认值25000000（25MB）。还有"),s("code",[t._v("hive.mapjoin.cache.numrows")]),t._v("，表示缓存build table的多少行数据到内存，默认值25000。")]),t._v(" "),s("h4",{attrs:{id:"分桶表map-join"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#分桶表map-join"}},[t._v("#")]),t._v(" "),s("strong",[t._v("分桶表map join")])]),t._v(" "),s("p",[t._v("map join对分桶表还有特别的优化。由于分桶表是基于一列进行hash存储的，因此非常适合抽样（按桶或按块抽样）。 它对应的配置项是"),s("code",[t._v("hive.optimize.bucketmapjoin")]),t._v("，优化器是BucketMapJoinOptimizer。但我们的业务中用分桶表较少，所以就不班门弄斧了，只是提一句。")]),t._v(" "),s("h4",{attrs:{id:"倾斜均衡配置项-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#倾斜均衡配置项-2"}},[t._v("#")]),t._v(" "),s("strong",[t._v("倾斜均衡配置项")])]),t._v(" "),s("p",[t._v("这个配置与上面group by的倾斜均衡配置项异曲同工，通过"),s("code",[t._v("hive.optimize.skewjoin")]),t._v("来配置，默认false。")]),t._v(" "),s("p",[t._v("如果开启了，在join过程中Hive会将计数超过阈值"),s("code",[t._v("hive.skewjoin.key")]),t._v("（默认100000）的倾斜key对应的行临时写进文件中，然后再启动另一个job做map join生成结果。通过"),s("code",[t._v("hive.skewjoin.mapjoin.map.tasks")]),t._v("参数还可以控制第二个job的mapper数量，默认10000。 再重复一遍，通过自带的配置项经常不能解决数据倾斜问题。join是数据倾斜的重灾区，后面还要介绍在SQL层面处理倾斜的各种方法。")]),t._v(" "),s("h2",{attrs:{id:"优化sql处理join数据倾斜"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#优化sql处理join数据倾斜"}},[t._v("#")]),t._v(" "),s("strong",[t._v("优化SQL处理join数据倾斜")])]),t._v(" "),s("h4",{attrs:{id:"空值或无意义值"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#空值或无意义值"}},[t._v("#")]),t._v(" "),s("strong",[t._v("空值或无意义值")])]),t._v(" "),s("p",[t._v("这种情况很常见，比如当事实表是日志类数据时，往往会有一些项没有记录到，我们视情况会将它置为null，或者空字符串、-1等。如果缺失的项很多，在做join时这些空值就会非常集中，拖累进度。")]),t._v(" "),s("p",[t._v("因此，若不需要空值数据，就提前写where语句过滤掉。需要保留的话，将空值key用随机方式打散，例如将用户ID为null的记录随机改为负值：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nickname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("age\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("when")]),t._v(" uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("is")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("then")]),t._v(" cast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rand"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10240")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" uid "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("end")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  event_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190201")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" a "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("outer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("nickname"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("age "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" user_info "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("status")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h4",{attrs:{id:"单独处理倾斜key"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#单独处理倾斜key"}},[t._v("#")]),t._v(" "),s("strong",[t._v("单独处理倾斜key")])]),t._v(" "),s("p",[t._v("这其实是上面处理空值方法的拓展，不过倾斜的key变成了有意义的。一般来讲倾斜的key都很少，我们可以将它们抽样出来，对应的行单独存入临时表中，然后打上一个较小的随机数前缀（比如0~9），最后再进行聚合。SQL语句与上面的相仿，不再赘述。")]),t._v(" "),s("h4",{attrs:{id:"不同数据类型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#不同数据类型"}},[t._v("#")]),t._v(" "),s("strong",[t._v("不同数据类型")])]),t._v(" "),s("p",[t._v("这种情况不太常见，主要出现在相同业务含义的列发生过逻辑上的变化时。 举个例子，假如我们有一旧一新两张日历记录表，旧表的记录类型字段是(event_type int)，新表的是(event_type string)。为了兼容旧版记录，新表的event_type也会以字符串形式存储旧版的值，比如'17'。当这两张表join时，经常要耗费很长时间。其原因就是如果不转换类型，计算key的hash值时默认是以int型做的，这就导致所有“真正的”string型key都分配到一个reducer上。所以要注意类型转换：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("record_data\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("outer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log_2\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190228")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("and")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cast"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" string"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190228")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h4",{attrs:{id:"build-table过大"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#build-table过大"}},[t._v("#")]),t._v(" "),s("strong",[t._v("build table过大")])]),t._v(" "),s("p",[t._v("有时，build table会大到无法直接使用map join的地步，比如全量用户维度表，而使用普通join又有数据分布不均的问题。这时就要充分利用probe table的限制条件，削减build table的数据量，再使用map join解决。代价就是需要进行两次join。举个例子：")]),t._v(" "),s("div",{staticClass:"language-sql extra-class"},[s("pre",{pre:!0,attrs:{class:"language-sql"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*+mapjoin(b)*/")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("event_type"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("status")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extra_info\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log a\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("outer")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/*+mapjoin(s)*/")]),t._v(" t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("status")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("extra_info\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" uid "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" calendar_record_log "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190228")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" s\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" user_info t "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pt_date "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20190228")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h2",{attrs:{id:"mapreduce优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce优化"}},[t._v("#")]),t._v(" "),s("strong",[t._v("MapReduce优化")])]),t._v(" "),s("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/commonopt-3.png"),alt:"wxmp"}}),t._v(" "),s("h4",{attrs:{id:"调整mapper数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#调整mapper数"}},[t._v("#")]),t._v(" "),s("strong",[t._v("调整mapper数")])]),t._v(" "),s("p",[t._v("mapper数量与输入文件的split数息息相关，在Hadoop源码"),s("code",[t._v("org.apache.hadoop.mapreduce.lib.input.FileInputFormat")]),t._v("类中可以看到split划分的具体逻辑。这里不贴代码，直接叙述mapper数是如何确定的。")]),t._v(" "),s("ul",[s("li",[t._v("可以直接通过参数"),s("code",[t._v("mapred.map.tasks")]),t._v("（默认值2）来设定mapper数的期望值，但它不一定会生效，下面会提到。")]),t._v(" "),s("li",[t._v("设输入文件的总大小为"),s("code",[t._v("total_input_size")]),t._v("。HDFS中，一个块的大小由参数"),s("code",[t._v("dfs.block.size")]),t._v("指定，默认值64MB或128MB。在默认情况下，mapper数就是： "),s("code",[t._v("default_mapper_num = total_input_size / dfs.block.size")]),t._v("。")]),t._v(" "),s("li",[t._v("参数"),s("code",[t._v("mapred.min.split.size")]),t._v("（默认值1B）和"),s("code",[t._v("mapred.max.split.size")]),t._v("（默认值64MB）分别用来指定split的最小和最大大小。split大小和split数计算规则是： "),s("code",[t._v("split_size = MAX(mapred.min.split.size, MIN(mapred.max.split.size, dfs.block.size))")]),t._v("； "),s("code",[t._v("split_num = total_input_size / split_size")]),t._v("。")]),t._v(" "),s("li",[t._v("得出mapper数： "),s("code",[t._v("mapper_num = MIN(split_num, MAX(default_num, mapred.map.tasks))")]),t._v("。")])]),t._v(" "),s("p",[t._v("可见，如果想减少mapper数，就适当调高"),s("code",[t._v("mapred.min.split.size")]),t._v("，split数就减少了。如果想增大mapper数，除了降低"),s("code",[t._v("mapred.min.split.size")]),t._v("之外，也可以调高"),s("code",[t._v("mapred.map.tasks")]),t._v("。")]),t._v(" "),s("p",[t._v("一般来讲，如果输入文件是少量大文件，就减少mapper数；如果输入文件是大量非小文件，就增大mapper数；至于大量小文件的情况，得参考下面“合并小文件”一节的方法处理。")]),t._v(" "),s("h4",{attrs:{id:"调整reducer数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#调整reducer数"}},[t._v("#")]),t._v(" "),s("strong",[t._v("调整reducer数")])]),t._v(" "),s("p",[t._v("reducer数量的确定方法比mapper简单得多。使用参数"),s("code",[t._v("mapred.reduce.tasks")]),t._v("可以直接设定reducer数量，不像mapper一样是期望值。但如果不设这个参数的话，Hive就会自行推测，逻辑如下：")]),t._v(" "),s("ul",[s("li",[t._v("参数"),s("code",[t._v("hive.exec.reducers.bytes.per.reducer")]),t._v("用来设定每个reducer能够处理的最大数据量，默认值1G（1.2版本之前）或256M（1.2版本之后）。")]),t._v(" "),s("li",[t._v("参数"),s("code",[t._v("hive.exec.reducers.max")]),t._v("用来设定每个job的最大reducer数量，默认值999（1.2版本之前）或1009（1.2版本之后）。")]),t._v(" "),s("li",[t._v("得出reducer数： "),s("code",[t._v("reducer_num = MIN(total_input_size / reducers.bytes.per.reducer, reducers.max)")]),t._v("。")])]),t._v(" "),s("p",[t._v("reducer数量与输出文件的数量相关。如果reducer数太多，会产生大量小文件，对HDFS造成压力。如果reducer数太少，每个reducer要处理很多数据，容易拖慢运行时间或者造成OOM。")]),t._v(" "),s("h4",{attrs:{id:"合并小文件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#合并小文件"}},[t._v("#")]),t._v(" "),s("strong",[t._v("合并小文件")])]),t._v(" "),s("ul",[s("li",[t._v("输入阶段合并 需要更改Hive的输入文件格式，即参数"),s("code",[t._v("hive.input.format")]),t._v("，默认值是"),s("code",[t._v("org.apache.hadoop.hive.ql.io.HiveInputFormat")]),t._v("，我们改成"),s("code",[t._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat")]),t._v("。 这样比起上面调整mapper数时，又会多出两个参数，分别是"),s("code",[t._v("mapred.min.split.size.per.node")]),t._v("和"),s("code",[t._v("mapred.min.split.size.per.rack")]),t._v("，含义是单节点和单机架上的最小split大小。如果发现有split大小小于这两个值（默认都是100MB），则会进行合并。具体逻辑可以参看Hive源码中的对应类。")]),t._v(" "),s("li",[t._v("输出阶段合并 直接将"),s("code",[t._v("hive.merge.mapfiles")]),t._v("和"),s("code",[t._v("hive.merge.mapredfiles")]),t._v("都设为true即可，前者表示将map-only任务的输出合并，后者表示将map-reduce任务的输出合并。 另外，"),s("code",[t._v("hive.merge.size.per.task")]),t._v("可以指定每个task输出后合并文件大小的期望值，"),s("code",[t._v("hive.merge.size.smallfiles.avgsize")]),t._v("可以指定所有输出文件大小的均值阈值，默认值都是1GB。如果平均大小不足的话，就会另外启动一个任务来进行合并。")])]),t._v(" "),s("h4",{attrs:{id:"启用压缩"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#启用压缩"}},[t._v("#")]),t._v(" "),s("strong",[t._v("启用压缩")])]),t._v(" "),s("p",[t._v("压缩job的中间结果数据和输出数据，可以用少量CPU时间节省很多空间。压缩方式一般选择Snappy，效率最高。 要启用中间压缩，需要设定"),s("code",[t._v("hive.exec.compress.intermediate")]),t._v("为true，同时指定压缩方式"),s("code",[t._v("hive.intermediate.compression.codec")]),t._v("为"),s("code",[t._v("org.apache.hadoop.io.compress.SnappyCodec")]),t._v("。另外，参数"),s("code",[t._v("hive.intermediate.compression.type")]),t._v("可以选择对块（BLOCK）还是记录（RECORD）压缩，BLOCK的压缩率比较高。 输出压缩的配置基本相同，打开"),s("code",[t._v("hive.exec.compress.output")]),t._v("即可。")]),t._v(" "),s("h4",{attrs:{id:"jvm重用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#jvm重用"}},[t._v("#")]),t._v(" "),s("strong",[t._v("JVM重用")])]),t._v(" "),s("p",[t._v("在MR job中，默认是每执行一个task就启动一个JVM。如果task非常小而碎，那么JVM启动和关闭的耗时就会很长。可以通过调节参数"),s("code",[t._v("mapred.job.reuse.jvm.num.tasks")]),t._v("来重用。例如将这个参数设成5，那么就代表同一个MR job中顺序执行的5个task可以重复使用一个JVM，减少启动和关闭的开销。但它对不同MR job中的task无效。")]),t._v(" "),s("h2",{attrs:{id:"并行执行与本地模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#并行执行与本地模式"}},[t._v("#")]),t._v(" "),s("strong",[t._v("并行执行与本地模式")])]),t._v(" "),s("ul",[s("li",[t._v("并行执行 Hive中互相没有依赖关系的job间是可以并行执行的，最典型的就是多个子查询union all。在集群资源相对充足的情况下，可以开启并行执行，即将参数"),s("code",[t._v("hive.exec.parallel")]),t._v("设为true。另外"),s("code",[t._v("hive.exec.parallel.thread.number")]),t._v("可以设定并行执行的线程数，默认为8，一般都够用。")]),t._v(" "),s("li",[t._v("本地模式 Hive也可以不将任务提交到集群进行运算，而是直接在一台节点上处理。因为消除了提交到集群的overhead，所以比较适合数据量很小，且逻辑不复杂的任务。 设置"),s("code",[t._v("hive.exec.mode.local.auto")]),t._v("为true可以开启本地模式。但任务的输入数据总量必须小于"),s("code",[t._v("hive.exec.mode.local.auto.inputbytes.max")]),t._v("（默认值128MB），且mapper数必须小于"),s("code",[t._v("hive.exec.mode.local.auto.tasks.max")]),t._v("（默认值4），reducer数必须为0或1，才会真正用本地模式执行。")])]),t._v(" "),s("h2",{attrs:{id:"严格模式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#严格模式"}},[t._v("#")]),t._v(" "),s("strong",[t._v("严格模式")])]),t._v(" "),s("p",[t._v("所谓严格模式，就是强制不允许用户执行3种有风险的HiveSQL语句，一旦执行会直接失败。这3种语句是：")]),t._v(" "),s("ul",[s("li",[t._v("查询分区表时不限定分区列的语句；")]),t._v(" "),s("li",[t._v("两表join产生了笛卡尔积的语句；")]),t._v(" "),s("li",[t._v("用order by来排序但没有指定limit的语句。")])]),t._v(" "),s("p",[t._v("要开启严格模式，需要将参数"),s("code",[t._v("hive.mapred.mode")]),t._v("设为strict。")]),t._v(" "),s("h2",{attrs:{id:"采用合适的存储格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#采用合适的存储格式"}},[t._v("#")]),t._v(" "),s("strong",[t._v("采用合适的存储格式")])]),t._v(" "),s("p",[t._v("在HiveSQL的create table语句中，可以使用"),s("code",[t._v("stored as ...")]),t._v("指定表的存储格式。Hive表支持的存储格式有TextFile、SequenceFile、RCFile、Avro、ORC、Parquet等。")]),t._v(" "),s("p",[t._v("存储格式一般需要根据业务进行选择，在我们的实操中，绝大多数表都采用TextFile与Parquet两种存储格式之一。")]),t._v(" "),s("p",[t._v("TextFile是最简单的存储格式，它是纯文本记录，也是Hive的默认格式。虽然它的磁盘开销比较大，查询效率也低，但它更多地是作为跳板来使用。RCFile、ORC、Parquet等格式的表都不能由文件直接导入数据，必须由TextFile来做中转。")]),t._v(" "),s("p",[t._v("Parquet和ORC都是Apache旗下的开源列式存储格式。列式存储比起传统的行式存储更适合批量OLAP查询，并且也支持更好的压缩和编码。我们选择Parquet的原因主要是它支持Impala查询引擎，并且我们对update、delete和事务性操作需求很低。")]),t._v(" "),s("p",[t._v("这里就不展开讲它们的细节，可以参考各自的官网：")]),t._v(" "),s("p",[t._v("https://parquet.apache.org/")]),t._v(" "),s("p",[t._v("https://orc.apache.org/")]),t._v(" "),s("h2",{attrs:{id:"参考文章"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),s("ul",[s("li",[t._v("https://cloud.tencent.com/developer/article/1453464")])])])}),[],!1,null,null,null);a.default=r.exports}}]);