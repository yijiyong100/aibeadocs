(window.webpackJsonp=window.webpackJsonp||[]).push([[492],{1007:function(t,s,a){"use strict";a.r(s);var e=a(53),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),a("p",[t._v("本文主要是介绍 Hive优化-汇总精华总结(一) 。")])]),t._v(" "),a("p"),a("div",{staticClass:"table-of-contents"},[a("ul",[a("li",[a("a",{attrs:{href:"#hive优化-整理版"}},[t._v("Hive优化(整理版)")])]),a("li",[a("a",{attrs:{href:"#_1-1-hive的特征"}},[t._v("1.1 hive的特征：")])]),a("li",[a("a",{attrs:{href:"#_1-2-hive的优势"}},[t._v("1.2 hive的优势：")])]),a("li",[a("a",{attrs:{href:"#_3-1-慎用api"}},[t._v("3.1 慎用api")])]),a("li",[a("a",{attrs:{href:"#_3-2-自定义udaf函数优化"}},[t._v("3.2 自定义UDAF函数优化")])]),a("li",[a("a",{attrs:{href:"#_3-3-设置合理的map-reduce的task数量"}},[t._v("3.3 设置合理的map reduce的task数量")]),a("ul",[a("li",[a("a",{attrs:{href:"#_3-3-1-map阶段优化"}},[t._v("3.3.1 map阶段优化")])])])]),a("li",[a("a",{attrs:{href:"#_3-4-小文件合并优化"}},[t._v("3.4 小文件合并优化")]),a("ul",[a("li",[a("a",{attrs:{href:"#_3-4-1-hive优化之小文件问题及其解决方案"}},[t._v("3.4.1 Hive优化之小文件问题及其解决方案：")])])])]),a("li",[a("a",{attrs:{href:"#_3-5-sql优化"}},[t._v("3.5 SQL优化")]),a("ul",[a("li",[a("a",{attrs:{href:"#_3-5-1-列裁剪"}},[t._v("3.5.1 列裁剪")])]),a("li",[a("a",{attrs:{href:"#_3-5-2-分区裁剪"}},[t._v("3.5.2 分区裁剪")])]),a("li",[a("a",{attrs:{href:"#_3-5-3-熟练使用sql提高查询"}},[t._v("3.5.3 熟练使用SQL提高查询")])]),a("li",[a("a",{attrs:{href:"#_3-5-5-不同数据类型关联产生的倾斜问题"}},[t._v("3.5.5 不同数据类型关联产生的倾斜问题")])]),a("li",[a("a",{attrs:{href:"#_3-5-6-利用hive对union-all优化的特性"}},[t._v("3.5.6 利用Hive对UNION ALL优化的特性")])]),a("li",[a("a",{attrs:{href:"#_3-5-7-解决hive对union-all优化的短板"}},[t._v("3.5.7 解决Hive对UNION ALL优化的短板")])]),a("li",[a("a",{attrs:{href:"#_3-5-8-count-distinct"}},[t._v("3.5.8 COUNT(DISTINCT)")])]),a("li",[a("a",{attrs:{href:"#_3-5-9-join操作"}},[t._v("3.5.9 JOIN操作")])]),a("li",[a("a",{attrs:{href:"#_3-5-10-优化in-exists语句"}},[t._v("3.5.10 优化in/exists语句")])]),a("li",[a("a",{attrs:{href:"#_3-5-11-排序选择"}},[t._v("3.5.11 排序选择")])])])]),a("li",[a("a",{attrs:{href:"#_3-6-存储格式"}},[t._v("3.6 存储格式")])]),a("li",[a("a",{attrs:{href:"#_3-7-压缩格式"}},[t._v("3.7 压缩格式")]),a("ul",[a("li",[a("a",{attrs:{href:"#_3-7-1-压缩的原因"}},[t._v("3.7.1 压缩的原因")])]),a("li",[a("a",{attrs:{href:"#_3-7-2-常用压缩方法对比"}},[t._v("3.7.2 常用压缩方法对比")])]),a("li",[a("a",{attrs:{href:"#_3-7-3-压缩方式的选择"}},[t._v("3.7.3 压缩方式的选择")])]),a("li",[a("a",{attrs:{href:"#_3-7-4-压缩使用"}},[t._v("3.7.4 压缩使用")])])])]),a("li",[a("a",{attrs:{href:"#_3-8-引擎的选择"}},[t._v("3.8 引擎的选择")])]),a("li",[a("a",{attrs:{href:"#_3-9-使用向量化查询"}},[t._v("3.9 使用向量化查询")])]),a("li",[a("a",{attrs:{href:"#_3-10-设置cost-based-query-optimization"}},[t._v("3.10 设置cost based query optimization")])]),a("li",[a("a",{attrs:{href:"#_3-11-模式选择"}},[t._v("3.11 模式选择")]),a("ul",[a("li",[a("a",{attrs:{href:"#本地模式"}},[t._v("本地模式")])]),a("li",[a("a",{attrs:{href:"#并行模式"}},[t._v("并行模式")])]),a("li",[a("a",{attrs:{href:"#严格模式"}},[t._v("严格模式")])])])]),a("li",[a("a",{attrs:{href:"#_3-12-jvm重用"}},[t._v("3.12 JVM重用")])]),a("li",[a("a",{attrs:{href:"#_3-13-推测执行"}},[t._v("3.13 推测执行")])]),a("li",[a("a",{attrs:{href:"#参考文章"}},[t._v("参考文章")])])])]),a("p"),t._v(" "),a("h2",{attrs:{id:"hive优化-整理版"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hive优化-整理版"}},[t._v("#")]),t._v(" Hive优化(整理版)")]),t._v(" "),a("h2",{attrs:{id:"_1-1-hive的特征"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-hive的特征"}},[t._v("#")]),t._v(" "),a("strong",[t._v("1.1 hive的特征")]),t._v("：")]),t._v(" "),a("ul",[a("li",[a("ol",[a("li",[t._v("可以通过SQL轻松访问数据的工具，从而实现数据仓库任务，如提取/转换/加载（ETL），报告和数据分析；")])])]),t._v(" "),a("li",[a("ol",{attrs:{start:"2"}},[a("li",[t._v("它可以使已经存储的数据结构化；")])])]),t._v(" "),a("li",[a("ol",{attrs:{start:"3"}},[a("li",[t._v("可以直接访问存储在Apache HDFS或其他数据存储系统（如Apache HBase）中的文件；")])])]),t._v(" "),a("li",[a("ol",{attrs:{start:"4"}},[a("li",[t._v("Hive除了支持MapReduce计算引擎，还支持Spark和Tez这两种分布式计算引擎；")])])]),t._v(" "),a("li",[a("ol",{attrs:{start:"5"}},[a("li",[t._v("它提供类似sql的查询语句HiveQL对数据进行分析处理；")])])]),t._v(" "),a("li",[a("ol",{attrs:{start:"6"}},[a("li",[t._v("数据的存储格式有多种，比如数据源是二进制格式，普通文本格式等等；")])])])]),t._v(" "),a("h2",{attrs:{id:"_1-2-hive的优势"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-hive的优势"}},[t._v("#")]),t._v(" "),a("strong",[t._v("1.2 hive的优势")]),t._v("：")]),t._v(" "),a("p",[t._v("hive强大之处不要求数据转换成特定的格式，而是利用hadoop本身InputFormat API来从不同的数据源读取数据，同样地使用OutputFormat API将数据写成不同的格式。所以对于不同的数据源，或者写出不同的格式就需要不同的对应的InputFormat和OutputFormat类的实现。以stored as textFile为例，其在底层java API中表现是输入InputFormat格式：TextInputFormat以及输出OutputFormat格式：HiveIgnoreKeyTextOutputFormat。这里InputFormat中定义了如何对数据源文本进行读取划分，以及如何将切片分割成记录存入表中。而OutputFormat定义了如何将这些切片写回到文件里或者直接在控制台输出。")]),t._v(" "),a("p",[t._v("Hive拥有统一的元数据管理，所以和Spark、Impala等SQL引擎是通用的。通用是指，在拥有了统一的metastore之后，在Hive中创建一张表，在Spark/Impala中是能用的；反之在Spark中创建一张表，在Hive中也是能用的，只需要共用元数据，就可以切换SQL引擎，涉及到了Spark sql和Hive On Spark。")]),t._v(" "),a("p",[t._v("不仅如此Hive使用SQL语法，提供快速开发的能力，还可以通过用户定义的函数（UDF），用户定义的聚合（UDAF）和用户定义的表函数（UDTF）进行扩展，避免了去写mapreducce，减少开发人员的学习成本。Hive中不仅可以使用逗号和制表符分隔值（CSV/TSV）文本文件，还可以使用Sequence File、RC、ORC、Parquet（知道这几种存储格式的区别）。当然Hive还可以通过用户来自定义自己的存储格式，基本上前面说到几种格式完全够了。Hive旨在最大限度地提高可伸缩性（通过向Hadoop集群动态田间更多机器扩展），性能，可扩展性，容错性以及与其输入格式的松散耦合。")]),t._v(" "),a("p",[t._v("数据离线处理，比如日志分析，海量数据结构化分析。")]),t._v(" "),a("h1",{attrs:{id:"_2-hive函数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-hive函数"}},[t._v("#")]),t._v(" 2. Hive函数")]),t._v(" "),a("p",[t._v("Hive的SQL还可以通过用户定义的函数（UDF），用户定义的聚合（UDAF）和用户定义的表函数（UDTF）进行扩展。")]),t._v(" "),a("p",[t._v("当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF）。")]),t._v(" "),a("p",[a("strong",[t._v("UDF、UDAF、UDTF")]),t._v("的区别：")]),t._v(" "),a("ul",[a("li",[t._v("UDF（User-Defined-Function）一进一出")]),t._v(" "),a("li",[t._v("UDAF（User-Defined Aggregation Funcation）聚集函数，多进一出")]),t._v(" "),a("li",[t._v("UDTF（User-Defined Table-Generating Functions）一进多出，如lateral view explore()")])]),t._v(" "),a("h1",{attrs:{id:"_3-hive优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-hive优化"}},[t._v("#")]),t._v(" 3. Hive优化")]),t._v(" "),a("h2",{attrs:{id:"_3-1-慎用api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-慎用api"}},[t._v("#")]),t._v(" 3.1 慎用api")]),t._v(" "),a("p",[t._v("我们知道大数据场景下不害怕数据量大，害怕的是数据倾斜，怎样避免数据倾斜，找到可能产生数据倾斜的函数尤为关键，数据量较大的情况下，慎用count(distinct)，count(distinct)容易产生倾斜问题。")]),t._v(" "),a("h2",{attrs:{id:"_3-2-自定义udaf函数优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-自定义udaf函数优化"}},[t._v("#")]),t._v(" 3.2 自定义UDAF函数优化")]),t._v(" "),a("p",[t._v("sum，count，max，min等UDAF，不怕数据倾斜问题，hadoop在map端汇总合并优化，是数据倾斜不成问题。")]),t._v(" "),a("h2",{attrs:{id:"_3-3-设置合理的map-reduce的task数量"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-设置合理的map-reduce的task数量"}},[t._v("#")]),t._v(" 3.3 设置合理的map reduce的task数量")]),t._v(" "),a("h3",{attrs:{id:"_3-3-1-map阶段优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-1-map阶段优化"}},[t._v("#")]),t._v(" "),a("strong",[t._v("3.3.1 map阶段优化")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size: 指的是数据的最小分割单元大小；min的默认值是"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("B\nmapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size: 指的是数据的最大分割单元大小；max的默认值是"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),t._v("MB\n通过调整max可以起到调整map数的作用，减小max可以增加map数，增大max可以减少map数。\n需要提醒的是，直接调整mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tasks这个参数是没有效果的。\n")])])]),a("p",[t._v("举例：")]),t._v(" "),a("p",[t._v("a) 假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128M的块和1个12M的块），从而产生7个map书；")]),t._v(" "),a("p",[t._v("b) 假设input目录下有3个文件a,b,c，大小分别为10M，20M，130M，那么hadoop会分隔成4个块（10M，20M，128M，2M），从而产生4个map数；")]),t._v(" "),a("p",[t._v("注意：如果文件大于块大小（128M），那么会拆分，如果小于块大小，则把该文件当成一个块。")]),t._v(" "),a("p",[t._v("其实这就涉及到小文件的问题：如果一个任务有很多小文件（远远小于块大小128M），则每个小文件也会当做一个块，用一个map任务来完成。")]),t._v(" "),a("p",[t._v("而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。那么，是不是保证每个map处理接近128M的文件块，就高枕无忧了？答案也是不一定。比如有一个127M的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。")]),t._v(" "),a("p",[t._v("我们该如何去解决呢？？？")]),t._v(" "),a("p",[t._v("我们需要采取两种方式来解决：即减少map数和增加map数")]),t._v(" "),a("ul",[a("li",[t._v("减少map数量")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("假设一个"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SQL")]),t._v("任务：\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("Select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" popt_tbaccountcopy_mes "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2012-07-04'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n该任务的inputdir :  "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("p_sdo_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("p_sdo_data_etl"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("pt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("popt_tbaccountcopy_mes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("pt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2012")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("07")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("04")]),t._v("\n共有"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("194")]),t._v("个文件，其中很多是远远小于"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),t._v("M的小文件，总大小"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v("G，正常执行会用"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("194")]),t._v("个map任务。\nMap总共消耗的计算资源：SLOTS_MILLIS_MAPS"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("623")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("020")]),t._v("\n\n通过以下方法来在map执行前合并小文件，减少map数：\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("约"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("95")]),t._v("M"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("per"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("node"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("约"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("95")]),t._v("M"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("min"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("per"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rack"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("约"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("95")]),t._v("M"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("format"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CombineHiveInputFormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n再执行上面的语句，用了"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("74")]),t._v("个map任务，map消耗的计算资源：SLOTS_MILLIS_MAPS"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("333")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),t._v("\n对于这个简单"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SQL")]),t._v("任务，执行时间上可能差不多，但节省了一半的计算资源。\n大概解释一下，"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100000000")]),t._v("表示"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("M"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("format"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hadoop"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hive"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ql"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("io"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CombineHiveInputFormat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("这个参数表示执行前进行小文件合并，\n前面三个参数确定合并文件块的大小，大于文件块大小"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),t._v("m的，按照"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),t._v("m来分隔，\n小于"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("大于"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("m的，按照"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("m来分隔，把那些小于"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v("m的（包括小文件和分隔大文件剩下的），\n进行合并"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("最终生成了"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("74")]),t._v("个块。\n")])])]),a("ul",[a("li",[t._v("增大map数量")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("如何适当的增加map数？\n当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，\n来使得每个map处理的数据量减少，从而提高任务的执行效率。\n\n 假设有这样一个任务：\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("Select")]),t._v(" data_desc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("when")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("when")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n               "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" data_desc\n\n如果表a只有一个文件，大小为"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("120")]),t._v("M，但包含几千万的记录，如果用"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("个map去完成这个任务，肯定是比较耗时的，\n这种情况下，我们要考虑将这一个文件合理的拆分成多个，\n这样就可以用多个map任务去完成。\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("set")]),t._v(" mapred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reduce"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tasks"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" a_1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" \n      "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" a \n      distribute "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" rand"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("123")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n这样会将a表的记录，随机的分散到包含"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("个文件的a_1表中，再用a_1代替上面"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("sql")]),t._v("中的a表，则会用"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("个map任务去完成。\n每个map任务处理大于"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v("M（几百万记录）的数据，效率肯定会好很多。\n")])])]),a("p",[t._v("注意：看上去，貌似这两种有些矛盾，一个是要合并小文件，一个是要把大文件拆成小文件，这点正是重点需要关注的地方，使单个map任务处理合适的数据量；")]),t._v(" "),a("p",[a("strong",[t._v("3.3.2 reduce阶段优化")])]),t._v(" "),a("p",[t._v("Reduce的个数对整个作业的运行性能有很大影响。如果Reduce设置的过大，那么将会产生很多小文件，对NameNode会产生一定的影响，而且整个作业的运行时间未必会减少；如果Reduce设置的过小，那么单个Reduce处理的数据将会加大，很可能会引起OOM异常。")]),t._v(" "),a("p",[t._v("如果设置了mapred.reduce.tasks/mapreduce.job.reduces参数，那么Hive会直接使用它的值作为Reduce的个数；如果mapred.reduce.tasks/mapreduce.job.reduces的值没有设置（也就是-1），那么Hive会根据输入文件的大小估算出Reduce的个数。根据输入文件估算Reduce的个数可能未必很准确，因为Reduce的输入是Map的输出，而Map的输出可能会比输入要小，所以最准确的数根据Map的输出估算Reduce的个数。")]),t._v(" "),a("p",[t._v("\\1. Hive自己如何确定reduce数：")]),t._v(" "),a("p",[t._v("reduce个数的设定极大影响任务执行效率，不指定reduce个数的情况下，Hive会猜测确定一个reduce个数，基于以下两个设定：")]),t._v(" "),a("p",[t._v("hive.exec.reducers.bytes.per.reducer（每个reduce任务处理的数据量，默认为1000^3=1G）")]),t._v(" "),a("p",[t._v("hive.exec.reducers.max（每个任务最大的reduce数，默认为999）")]),t._v(" "),a("p",[t._v("计算reducer数的公式很简单N=min（参数2，总输入数据量/参数1）")]),t._v(" "),a("p",[t._v("即，如果reduce的输入（map的输出）总大小不超过1G，那么只会有一个reduce任务；")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("如："),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" pt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" popt_tbaccountcopy_mes "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" pt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2012-07-04'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" pt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n            "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("p_sdo_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("p_sdo_data_etl"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("pt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("popt_tbaccountcopy_mes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("pt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2012")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("07")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("04")]),t._v(" 总大小为"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v("G多，\n  因此这句有"),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("个reduce\n")])])]),a("p",[t._v("\\2. 调整reduce个数方法一：")]),t._v(" "),a("p",[t._v("调整hive.exec.reducers.bytes.per.reducer参数的值；")]),t._v(" "),a("p",[t._v("set hive.exec.reducers.bytes.per.reducer=500000000; （500M）")]),t._v(" "),a("p",[t._v("select pt, count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt;")]),t._v(" "),a("p",[t._v("这次有20个reduce")]),t._v(" "),a("p",[t._v("\\3. 调整reduce个数方法二：")]),t._v(" "),a("p",[t._v("set mapred.reduce.tasks=15;")]),t._v(" "),a("p",[t._v("select pt,count(1) from popt_tbaccountcopy_mes where pt = '2012-07-04' group by pt;")]),t._v(" "),a("p",[t._v("这次有15个reduce")]),t._v(" "),a("p",[t._v("\\4. reduce个数并不是越多越好；")]),t._v(" "),a("p",[t._v("同map一样，启动和初始化reduce也会消耗时间和资源；")]),t._v(" "),a("p",[t._v("另外，有多少个reduce，就会有个多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；")]),t._v(" "),a("p",[t._v("\\5. 什么情况下只有一个reduce；")]),t._v(" "),a("p",[t._v("很多时候你会发现任务中不管数据量多大，不管你有没有调整reduce个数的参数，任务中一直都只有一个reduce任务；其实只有一个reduce任务的情况，除了数据量小于hive.exec.reducers.bytes.per.reducer参数值的情况外，还有以下原因：")]),t._v(" "),a("ul",[a("li",[t._v("没有group by的汇总，比如把select pt,count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’ group by pt; 写成select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’; 这点非常常见，希望大家尽量改写。")]),t._v(" "),a("li",[t._v("用了Order by")]),t._v(" "),a("li",[t._v("有笛卡尔积。")])]),t._v(" "),a("p",[t._v("注意：在设置reduce个数的时候也需要考虑这两个原则：使大数据量利用合适的reduce数；是单个reduce任务处理合适的数据量；")]),t._v(" "),a("h2",{attrs:{id:"_3-4-小文件合并优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-小文件合并优化"}},[t._v("#")]),t._v(" 3.4 小文件合并优化")]),t._v(" "),a("p",[t._v("我们知道文件数目小，容易在文件存储端造成瓶颈，给HDFS带来压力，影响处理效率。对此，可以通过合并Map和Reduce的结果文件来消除这样的影响。")]),t._v(" "),a("p",[t._v("用于设置合并的参数有：")]),t._v(" "),a("ul",[a("li",[a("ul",[a("li",[t._v("是否合并Map输出文件：hive.merge.mapfiles=true（默认值为true）")]),t._v(" "),a("li",[t._v("是否合并Reduce端输出文件：hive.merge.mapredfiles=false（默认值为false）")]),t._v(" "),a("li",[t._v("合并文件的大小：hive.merge.size.per.task=256"),a("em",[t._v("1000")]),t._v("1000（默认值为256000000）")])])])]),t._v(" "),a("h3",{attrs:{id:"_3-4-1-hive优化之小文件问题及其解决方案"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-1-hive优化之小文件问题及其解决方案"}},[t._v("#")]),t._v(" 3.4.1 Hive优化之小文件问题及其解决方案：")]),t._v(" "),a("p",[a("strong",[t._v("小文件是如何产生的：")])]),t._v(" "),a("ul",[a("li",[a("ul",[a("li",[t._v("动态分区插入数据，产生大量的小文件，从而导致map数量剧增；")]),t._v(" "),a("li",[t._v("reduce数量越多，小文件也越多（reduce的个数和输出文件是对应的）；")]),t._v(" "),a("li",[t._v("数据源本身就包含大量的小文件。")])])])]),t._v(" "),a("p",[a("strong",[t._v("小文件问题的影响：")])]),t._v(" "),a("ul",[a("li",[a("ul",[a("li",[t._v("从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。")]),t._v(" "),a("li",[t._v("在HDFS中，每个小文件对象约占150byte，如果小文件过多会占用大量内存。这样NameNode内存容量严重制约了集群的扩展。")])])])]),t._v(" "),a("p",[a("strong",[t._v("小文件问题的解决方案：")])]),t._v(" "),a("p",[t._v("从小文件产生的途径就可以从源头上控制小文件数量，方法如下：")]),t._v(" "),a("ul",[a("li",[a("ul",[a("li",[t._v("使用Sequencefile作为表存储格式，不要用textfile，在一定程度上可以减少小文件；")]),t._v(" "),a("li",[t._v("减少reduce的数量（可以使用参数进行控制）；")]),t._v(" "),a("li",[t._v("少用动态分区，用时记得按distribute by分区；")])])])]),t._v(" "),a("p",[t._v("对于已有的小文件，我们可以通过以下几种方案解决：")]),t._v(" "),a("ul",[a("li",[a("ul",[a("li",[t._v("使用hadoop archive命令把小文件进行归档；")]),t._v(" "),a("li",[t._v("重建表，建表时减少reduce数量；")]),t._v(" "),a("li",[t._v("通过参数进行调节，设置map/reduce端的相关参数，如下：")])])])]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("//每个Map最大输入大小(这个值决定了合并后文件的数量)  \nset mapred.max.split.size"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("256000000;    ")]),t._v("\n//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)  \nset mapred.min.split.size.per.node"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("100000000;  ")]),t._v("\n//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)    \nset mapred.min.split.size.per.rack"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("100000000;  ")]),t._v("\n//执行Map前进行小文件合并  \nset hive.input.format"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;   ")]),t._v("\n\n设置map输出和reduce输出进行合并的相关参数：\n"),a("span",{pre:!0,attrs:{class:"token selector"}},[t._v("[java]")]),t._v(" view plain copy\n//设置map端输出进行合并，默认为true  \nset hive.merge.mapfiles "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true  ")]),t._v("\n//设置reduce端输出进行合并，默认为false  \nset hive.merge.mapredfiles "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true  ")]),t._v("\n//设置合并文件的大小  \nset hive.merge.size.per.task "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" 256*1000*1000  ")]),t._v("\n//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。  \nset hive.merge.smallfiles.avgsize"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("16000000 ")]),t._v("\n")])])]),a("h2",{attrs:{id:"_3-5-sql优化"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-sql优化"}},[t._v("#")]),t._v(" 3.5 SQL优化")]),t._v(" "),a("h3",{attrs:{id:"_3-5-1-列裁剪"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-1-列裁剪"}},[t._v("#")]),t._v(" 3.5.1 列裁剪")]),t._v(" "),a("p",[t._v("Hive在读数据的时候，可以只读取查询中所需要用到的列，而忽略其他列。例如，若有以下查询：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" q "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" e"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("在实施此项查询中，Q表有5列（a，b，c，d，e），Hive只读取查询逻辑中真实需要的3列a、b、e， 而忽略列c，d；这样做节省了读取开销，中间表存储开销和数据整合开销。")]),t._v(" "),a("p",[t._v("裁剪对应的参数项为：hive.optimize.cp=true（默认值为真）")]),t._v(" "),a("h3",{attrs:{id:"_3-5-2-分区裁剪"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-2-分区裁剪"}},[t._v("#")]),t._v(" 3.5.2 分区裁剪")]),t._v(" "),a("p",[t._v("可以在查询的过程中减少不必要的分区。例如，若有以下查询：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" a1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" T "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" a1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" subq "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" subq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prtn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# （多余分区）")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" T1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" T2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" subq "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("T1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a1"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("subq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("a2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" subq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("prtn"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v('查询语句若将"subq.prtn=100"条件放入子查询中更为高效，可以减少读入的分区数目。Hive自动执行这种裁剪优化。')]),t._v(" "),a("p",[t._v("分区参数为：hive.optimize.pruner=true（默认值为真）")]),t._v(" "),a("h3",{attrs:{id:"_3-5-3-熟练使用sql提高查询"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-3-熟练使用sql提高查询"}},[t._v("#")]),t._v(" 3.5.3 熟练使用SQL提高查询")]),t._v(" "),a("p",[t._v("熟练地使用SQL，能写出高效率的查询语句。")]),t._v(" "),a("p",[t._v("场景：有一张user表，为卖家每天收到表，user_id，ds（日期）为key，属性有主营类目，指标有交易金额，交易笔数。每天要取前10天的总收入，总笔数，和最近一天的主营类目。")]),t._v(" "),a("p",[a("strong",[t._v("解决方法 1")]),t._v(" 如下所示：常用方法")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" t1\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" substr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("MAX")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CONCAT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("cat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" main_cat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" users\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 20120329 为日期列的值，实际代码中可以用函数表示当天日期GROUP BY user_id;")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" t2\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("qty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" qty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("amt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" amt "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" users\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("BETWEEN")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120301")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("AND")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("main_cat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("qty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("amt "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" t1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id\n")])])]),a("p",[t._v("下面给出方法1的思路，实现步骤如下：")]),t._v(" "),a("p",[t._v("第一步：利用分析函数，取每个user_id最近一天的主营类目，存入临时表t1；")]),t._v(" "),a("p",[t._v("第二步：汇总10天的总交易金额，交易笔数，存入临时表t2；")]),t._v(" "),a("p",[t._v("第三步：关联t1、t2，得到最终的结果。")]),t._v(" "),a("p",[a("strong",[t._v("解决方法 2")]),t._v(" 如下所示：优化方法")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" substr"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("MAX")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("CONCAT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" main_cat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("qty"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("amt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" users\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" ds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("BETWEEN")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120301")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("AND")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" user_id\n")])])]),a("p",[t._v("在工作中我们总结出：方案2的开销等于方案1的第二步开销，性能提升，由原有的25分钟完成，缩短为10分钟以内完成。节省了两个临时表的读写是一个关键原因，这种方式也适用于Oracle中的数据查找工作。")]),t._v(" "),a("p",[t._v("SQL具有普适性，很多SQL通用的优化方案在Hadoop分布式计算方式中也可以达到效果。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-5-不同数据类型关联产生的倾斜问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-5-不同数据类型关联产生的倾斜问题"}},[t._v("#")]),t._v(" 3.5.5 不同数据类型关联产生的倾斜问题")]),t._v(" "),a("p",[t._v("问题：不同数据类型id的关联会产生数据倾斜问题。")]),t._v(" "),a("p",[t._v("一张表的s8的日志，每个商品一条记录，要和商品表关联。但关联却碰到倾斜的问题。s8的日志中有32位字符串商品id，也有数值商品id，日志中类型是string的，但商品中的数值id是bigint的。猜想问题的原因是把s8的商品id转成数值id做hash来分配Reduce，所以字符串id的s8日志，都到一个Reduce上了，解决的方法验证了这个猜测。")]),t._v(" "),a("p",[t._v("解决方法：把数据类型转换成字符串类型")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" s8_log a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("LEFT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("OUTER")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" r_auction_auctions b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auction_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("CAST"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auction_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" STRING"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("调优结果显示：数据表处理由1小时30分钟经代码调整后可以在20分钟内完成。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-6-利用hive对union-all优化的特性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-6-利用hive对union-all优化的特性"}},[t._v("#")]),t._v(" 3.5.6 利用Hive对UNION ALL优化的特性")]),t._v(" "),a("p",[t._v("多表union all会优化成一个job。")]),t._v(" "),a("p",[t._v("问题：比如推广效果表要和商品表关联，效果表中的auction_id列既有32位字符串商品id，也有数字id，和商品表关联得到商品的信息。")]),t._v(" "),a("p",[t._v("解决方法：Hive SQL性能会比较好")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" effect a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" auction_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" auction_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" auctions\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" auction_string_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" auction_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" auctions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auction_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("auction_id\n")])])]),a("p",[t._v("比分别过滤数字id，字符串id然后分别和商品表关联性能要好。")]),t._v(" "),a("p",[t._v("这样写的好处：1个MapReduce作业，商品表只读一次，推广效果表只读取一次。把这个SQL换成Map/Reduce代码的话，Map的时候，把a表的记录打上标签a，商品表记录每读取一条，打上标签b，变成两个<key, value>对，<(b，数字id)，value>，<(b，字符串id)，value>。")]),t._v(" "),a("p",[t._v("所以商品表的HDFS读取只会是一次。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-7-解决hive对union-all优化的短板"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-7-解决hive对union-all优化的短板"}},[t._v("#")]),t._v(" 3.5.7 解决Hive对UNION ALL优化的短板")]),t._v(" "),a("p",[t._v("Hive对union all的优化的特性：对union all优化只局限于非嵌套查询")]),t._v(" "),a("ul",[a("li",[t._v("消灭子查询内的group by")])]),t._v(" "),a("p",[t._v("示例1：子查询内有group by")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t3GROUP "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3\n")])])]),a("p",[t._v("从业务逻辑上说，子查询内的GROUP BY怎么看都是多余（功能上的多余，除非有COUNT(DISTINCT)），如果不是因为Hive Bug或者性能上的考量（曾经出现如果不执行子查询GROUP BY，数据得不到正确的结果的Hive Bug）。所以这个Hive按经验转换成如下所示：")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t3 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3\n")])])]),a("p",[t._v("调优结果：经过测试，并未出现union all的Hive Bug，数据是一致的。MapReduce的作业数由3减少到1。")]),t._v(" "),a("p",[t._v("t1相当于一个目录，t2相当于一个目录，对Map/Reduce程序来说，t1、t2可以作为Map/Reduce作业的mutli inputs。这可以通过一个Map/Reduce来解决这个问题。Hadoop的计算框架，不怕数据多，就怕作业数多。")]),t._v(" "),a("p",[t._v("但如果换成是其他计算平台如Oracle，那就不一定了，因为把大输入拆成两个输入，分别排序汇总成merge（假如两个子排序是并行的话），是有可能性能更优的（比如希尔排序比冒泡排序的性能更优）。")]),t._v(" "),a("ul",[a("li",[t._v("消灭子查询内的COUNT(DISTINCT)，MAX，MIN")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3 "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DISTINCT")]),t._v(" c4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t3\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3\n")])])]),a("p",[t._v("由于子查询里头有COUNT(DISTINCT)操作，直接去GROUP BY将达不到业务目标。这时采用临时表消灭COUNT(DISTINCT)作业不但能解决倾斜问题，还能有效减少jobs。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" t4 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c4 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("income"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("SUM")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("uv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("income"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" uv "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" income"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" uv "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" t3\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("job数是2，减少一半，而且两次Map/Reduce比COUNT(DISTINCT)效率更高。")]),t._v(" "),a("p",[t._v("调优结果：千万级别的类目表，member表，与10亿级的商品表关联。原先1963s的任务经过调整，1152s即完成。")]),t._v(" "),a("ul",[a("li",[t._v("消灭子查询内的JOIN")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t4 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" t3 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("t3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" x\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" c1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("上面代码运行会有5个jobs。加入先JOIN生存临时表的话t5，然后UNION ALL，会变成2个jobs。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" t5\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" t2 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" t3 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" t2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("t3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t1 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" t4 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" t5"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("调优结果显示：针对千万级别的广告位表，由原先5个Job共15分钟，分解为2个job，一个8-10分钟，一个3分钟。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-8-count-distinct"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-8-count-distinct"}},[t._v("#")]),t._v(" 3.5.8 COUNT(DISTINCT)")]),t._v(" "),a("p",[t._v("计算uv的时候，经常会用到COUNT(DISTINCT)，但在数据比较倾斜的时候COUNT(DISTINCT)会比较慢。这时可以尝试用GROUP BY改写代码计算uv。数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：")]),t._v(" "),a("ul",[a("li",[t._v("原有代码")])]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("①"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" OVERWRITE "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" s_dw_tanx_adzone_uv "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("PARTITION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" thedate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("adzoneid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DISTINCT")]),t._v(" acookie"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" uv \n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" s_ods_log_tanx_pv t "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" t"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20120329")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("GROUP")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("BY")]),t._v(" adzoneid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n②"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("distinct")]),t._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" bigtable"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("关于COUNT(DISTINCT)的数据倾斜问题不能一概而论，要依情况而定，下面是我测试的一组数据：")]),t._v(" "),a("p",[t._v("测试数据：169857条")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("①\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#统计每日IP")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" ip_2014_12_29 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DISTINCT")]),t._v(" ip"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" logdfs "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" logdate"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2014_12_29'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n耗时："),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("24.805")]),t._v(" seconds\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#统计每日IP(改造)")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" ip_2014_12_29 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("COUNT")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" IP "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("DISTINCT")]),t._v(" ip "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" logdfs "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" logdate"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2014_12_29'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" tmp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n耗时："),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("46.833")]),t._v(" seconds\n\n②\n "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("count")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" bigtable "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("group")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("by")]),t._v(" id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("测试结果表明：明显改造后的语句比之前耗时，这时因为改造后的语句有2个SELECT，多了一个job，这样在数据量小的时候，数据不会存在倾斜问题。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-9-join操作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-9-join操作"}},[t._v("#")]),t._v(" 3.5.9 JOIN操作")]),t._v(" "),a("h4",{attrs:{id:"_3-5-9-1-小表、大表join"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-9-1-小表、大表join"}},[t._v("#")]),t._v(" 3.5.9.1 小表、大表JOIN")]),t._v(" "),a("p",[t._v("在使用写有Join操作的查询语句时有一条原则：应该将条目少的表/子查询放在Join操作符的左边。原因是在Join操作的Reduce阶段，位于Join操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生OOM错误的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。")]),t._v(" "),a("p",[t._v("实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。")]),t._v(" "),a("p",[t._v("案例实操：")]),t._v(" "),a("p",[t._v("（1）关闭mapjoin功能（默认是打开的）")]),t._v(" "),a("p",[t._v("set hive.auto.convert.join=false;")]),t._v(" "),a("p",[t._v("（2）执行小表JOIN大表语句")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" jointable\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyword"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("url_rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_url\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" smalltable s\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" bigtable  b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("Time taken: 35.921 seconds")]),t._v(" "),a("p",[t._v("（3）执行大表JOIN大表语句")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" jointable\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyword"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("url_rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_url\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" bigtable  b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" smalltable  s\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("Time taken: 34.196 seconds；")]),t._v(" "),a("h4",{attrs:{id:"_3-5-9-2-大表join大表"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-9-2-大表join大表"}},[t._v("#")]),t._v(" 3.5.9.2 大表JOIN大表")]),t._v(" "),a("h5",{attrs:{id:"_1-空key过滤"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-空key过滤"}},[t._v("#")]),t._v(" 1）空Key过滤")]),t._v(" "),a("p",[t._v("问题：日志中常会出现信息丢失，比如每日约为20亿的全网日志，其中的user_id为主键，在日志收集过程中会丢失，出现主键为null的情况，如果取其中的user_id和bmw_users关联，就会碰到数据倾斜的问题。原因是Hive中，主键为null值的项会被当做相同的Key而分配进同一个计算Map。")]),t._v(" "),a("p",[t._v("解决方法1：user_id为空的不参与关联，子查询过滤null")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" log a\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" bmw_users b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("IS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("NOT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("AND")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("UNION")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ALL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" log a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("IS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v("\n")])])]),a("p",[t._v("解决方法2 如下所示：函数过滤null")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" log a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("LEFT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("OUTER")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("JOIN")]),t._v(" bmw_users b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CASE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("WHEN")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("IS")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("NULL")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("THEN")]),t._v(" CONCAT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'dp_hive'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" RAND"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ELSE")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("END")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("user_id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("调优结果：原先由于数据倾斜导致运行时长超过1小时，解决方法1运行每日平均时长25分钟，解决方法2运行的每日平均时长在20分钟左右。优化效果很明显。")]),t._v(" "),a("p",[t._v("我们在工作中总结出：解决方法2比解决方法1效果更好，不但IO少了，而且作业数也少了。解决方法1中log读取两次，job数为2。解决方法2中job数是1。这个优化适合无效id（比如-99，‘’，null等）产生的倾斜问题。把空值的key变成一个字符串加上随机数，就能把倾斜的数据分到不同的Reduce上，从而解决数据倾斜问题。因为空值不参与关联，即使分到不同的Reduce上，也不会影响最终的结果。附上Hadoop通用关联的实现方法是：关联通过二次排序实现的，关联的列为partition key，关联的列和表的tag组成排序的group key，根据partition key分配Reduce。同一Reduce内根据group key排序。")]),t._v(" "),a("h4",{attrs:{id:"_3-5-9-3-map-join操作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-9-3-map-join操作"}},[t._v("#")]),t._v(" 3.5.9.3 MAP JOIN操作")]),t._v(" "),a("p",[t._v("如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。")]),t._v(" "),a("ul",[a("li",[t._v("开启MapJoin参数设置：")])]),t._v(" "),a("p",[t._v("1) 设置自动选择MapJoin")]),t._v(" "),a("p",[t._v("set hive.auto.convert.join = true;默认为true")]),t._v(" "),a("p",[t._v("2) 大表小表的阀值设置（默认25M一下认为是小表）：")]),t._v(" "),a("p",[t._v("set hive.mapjoin.smalltable.filesize=25000000;")]),t._v(" "),a("ul",[a("li",[t._v("MapJoin工作机制")])]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/sumopt11-1.png"),alt:"wxmp"}}),t._v(" "),a("p",[t._v("上图是Hive MapJoin的原理图，从图中可以看出MapJoin分为两个阶段：")]),t._v(" "),a("p",[t._v("（1）通过MapReduce Local Task，将小表读入内存，生成内存HashTableFiles上传至Distributed Cache中，这里会对HashTableFiles进行压缩。")]),t._v(" "),a("p",[t._v("（2）MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务。也就是在map端进行join避免了shuffle。")]),t._v(" "),a("p",[t._v("Join操作在Map阶段完成，不再需要Reduce，有多少个Map Task，就有多少个结果文件。")]),t._v(" "),a("p",[t._v("实例：")]),t._v(" "),a("p",[t._v("（1）开启MapJoin功能")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　　　set hive.auto.convert.join "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true; 默认为true")]),t._v("\n")])])]),a("p",[t._v("（2）执行小表JOIN大表语句")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" jointable\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyword"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("url_rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_url\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" smalltable s\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" bigtable  b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("Time taken: 24.594 seconds")]),t._v(" "),a("p",[t._v("（3）执行大表JOIN小表语句")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("insert")]),t._v(" overwrite "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" jointable\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("time")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keyword"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("url_rank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_num"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("click_url\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" bigtable  b\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" smalltable  s\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" s"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("Time taken: 24.315 seconds")]),t._v(" "),a("h4",{attrs:{id:"_3-5-9-3-group-by操作"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-9-3-group-by操作"}},[t._v("#")]),t._v(" 3.5.9.3 GROUP BY操作")]),t._v(" "),a("p",[t._v("默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。进行GROUP BY操作时需要注意以下几点：")]),t._v(" "),a("ul",[a("li",[t._v("Map端部分聚合")])]),t._v(" "),a("p",[t._v("事实上并不是所有的聚合操作都需要在reduce部分进行，很多聚合操作都可以先在Map端进行部分聚合，然后reduce端得出最终结果。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　（1）开启Map端聚合参数设置\n\n　　　　set hive.map.aggr"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true")]),t._v("\n\n　　（2）在Map端进行聚合操作的条目数目\n\n　　　　set hive.grouby.mapaggr.checkinterval"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("100000")]),t._v("\n\n　　（3）有数据倾斜的时候进行负载均衡（默认是false）\n\n　　　　set hive.groupby.skewindata "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true")]),t._v("\n")])])]),a("ul",[a("li",[t._v("有数据倾斜时进行负载均衡")])]),t._v(" "),a("p",[t._v("此处需要设定hive.groupby.skewindata，当选项设定为true时，生成的查询计划有两个MapReduce任务。在第一个MapReduce中，map的输出结果集合会随机分布到reduce中，每个reduce做部分聚合操作，并输出结果。这样处理的结果是，相同的Group By Key有可能分发到不同的reduce中，从而达到负载均衡的目的；第二个MapReduce任务再根据预处理的数据结果按照Group By Key分布到reduce中（这个过程可以保证相同的Group By Key分布到同一个reduce中），最后完成最终的聚合操作。")]),t._v(" "),a("h3",{attrs:{id:"_3-5-10-优化in-exists语句"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-10-优化in-exists语句"}},[t._v("#")]),t._v(" 3.5.10 优化in/exists语句")]),t._v(" "),a("p",[t._v("虽然经过测验，hive1.2.1也支持in/exists操作，但还是推荐使用hive的一个高效替代方案：left semi join")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[t._v("　　比如说：　　　　\n\n　　　　　　"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n　　　　　　"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("exists")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" id "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("where")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n　　应该转换成：\n\n　　　　　　"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("select")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" a "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("left")]),t._v(" semi "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" b "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("on")]),t._v(" a"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" b"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h3",{attrs:{id:"_3-5-11-排序选择"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-11-排序选择"}},[t._v("#")]),t._v(" 3.5.11 排序选择")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("cluster by")]),t._v(": 对同一字段分桶并排序，不能和sort by连用；")]),t._v(" "),a("li",[a("strong",[t._v("distribute by + sort by")]),t._v(": 分桶，保证同一字段值只存在一个结果文件当中，结合sort by 保证每个reduceTask结果有序；")]),t._v(" "),a("li",[a("strong",[t._v("sort by")]),t._v(": 单机排序，单个reduce结果有序")]),t._v(" "),a("li",[a("strong",[t._v("order by")]),t._v("：全局排序，缺陷是只能使用一个reduce")])]),t._v(" "),a("h2",{attrs:{id:"_3-6-存储格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-存储格式"}},[t._v("#")]),t._v(" 3.6 存储格式")]),t._v(" "),a("p",[t._v("可以使用列裁剪，分区裁剪，orc，parquet等这些列式存储格式，因为列式存储的表，每一列的数据在物理上是存储在一起的，Hive查询时会只遍历需要列数据，大大减少处理的数据量。")]),t._v(" "),a("p",[t._v("Hive支持ORCfile，这是一种新的表格存储格式，通过诸如谓词下推，压缩等技术来提高执行速度提升。对于每个HIVE表使用ORCfile应该是一件容易的事情，并且对于获得HIVE查询的快速响应时间非常有益。")]),t._v(" "),a("p",[t._v("作为一个例子，考虑两个大表A和B（作为文本存储，其中一些列未在此处指定，即行式存储的缺点）以及一个简单的查询，如：")]),t._v(" "),a("p",[t._v("SELECT A.customerID，A.name，A.age，A.address join B.role，B.department，B.salary ON A.customerID=B.customerID；")]),t._v(" "),a("p",[t._v("此查询可能需要很长时间才能执行，因为表A和B都以TEXT形式存储，进行全表扫描。")]),t._v(" "),a("p",[t._v("将这些表格转换为ORCFile格式通常会显着减少查询时间；")]),t._v(" "),a("p",[t._v("ORC支持压缩存储（使用ZLIB或如上所示使用SNAPPY），但也支持未压缩的存储。")]),t._v(" "),a("div",{staticClass:"language-sql extra-class"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" A_ORC "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n　　customerID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v("，name string，age "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" address string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" STORED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" ORC tblproperties "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"orc.compress"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"SNAPPY"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("；\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INTO")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" A_ORC "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" A"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("CREATE")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" B_ORC "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n       customerID "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" role string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" salary "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" department string\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" STORED "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("AS")]),t._v(" ORC tblproperties "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"orc.compress"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"SNAPPY"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INSERT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("INTO")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("TABLE")]),t._v(" B_ORC "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" B"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" A_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("customerID"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" A_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" A_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("age"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" A_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("address "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("join")]),t._v(" B_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("role，B_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("department"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" B_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("salary\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("ON")]),t._v(" A_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("customerID"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("B_ORC"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("customerID"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("h2",{attrs:{id:"_3-7-压缩格式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-压缩格式"}},[t._v("#")]),t._v(" 3.7 压缩格式")]),t._v(" "),a("p",[t._v("大数据场景下存储格式压缩格式尤为关键，可以提升计算速度，减少存储空间，降低网络io，磁盘io，所以要选择合适的压缩格式和存储格式，那么首先就了解这些东西。"),a("a",{attrs:{href:"https://blog.csdn.net/yu0_zhang0/article/details/79524842",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考该博客"),a("OutboundLink")],1)]),t._v(" "),a("h3",{attrs:{id:"_3-7-1-压缩的原因"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-1-压缩的原因"}},[t._v("#")]),t._v(" 3.7.1 压缩的原因")]),t._v(" "),a("p",[t._v("Hive最终是转为MapReduce程序来执行的，而MapReduce的性能瓶颈在于网络IO和磁盘IO，要解决性能瓶颈，最主要的是减少数据量，对数据进行压缩是个好的方式。压缩虽然是减少了数据量，但是压缩过程要消耗CPU的，但是在Hadoop中，往往性能瓶颈不在于CPU，CPU压力并不大，所以压缩充分利用了比较空闲的CPU。")]),t._v(" "),a("h3",{attrs:{id:"_3-7-2-常用压缩方法对比"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-2-常用压缩方法对比"}},[t._v("#")]),t._v(" 3.7.2 常用压缩方法对比")]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/sumopt11-2.png"),alt:"wxmp"}}),t._v(" "),a("p",[t._v("各个压缩方式所对应的Class类：")]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/sumopt11-3.png"),alt:"wxmp"}}),t._v(" "),a("h3",{attrs:{id:"_3-7-3-压缩方式的选择"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-3-压缩方式的选择"}},[t._v("#")]),t._v(" 3.7.3 压缩方式的选择")]),t._v(" "),a("p",[t._v("压缩比率，压缩解压缩速度，是否支持Split")]),t._v(" "),a("h3",{attrs:{id:"_3-7-4-压缩使用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-4-压缩使用"}},[t._v("#")]),t._v(" 3.7.4 压缩使用")]),t._v(" "),a("p",[t._v("Job输出文件按照block以Gzip的方式进行压缩：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set mapreduce.output.fileoutputformat.compress"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true // 默认值是 false")]),t._v("\nset mapreduce.output.fileoutputformat.compress.type"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("BLOCK // 默认值是 Record")]),t._v("\nset mapreduce.output.fileoutputformat.compress.codec"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec")]),t._v("\n")])])]),a("p",[t._v("Map输出结果也以Gzip进行压缩：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set mapred.map.output.compress"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true")]),t._v("\nset mapreduce.map.output.compress.codec"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("org.apache.hadoop.io.compress.GzipCodec // 默认值是 org.apache.hadoop.io.compress.DefaultCodec ")]),t._v("\n")])])]),a("p",[t._v("对Hive输出结果和中间都进行压缩：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.compress.output"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true // 默认值是 false，不压缩")]),t._v("\nset hive.exec.compress.intermediate"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true // 默认值是 false，为 true 时 MR 设置的压缩才启用")]),t._v("\n")])])]),a("h2",{attrs:{id:"_3-8-引擎的选择"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-8-引擎的选择"}},[t._v("#")]),t._v(" 3.8 引擎的选择")]),t._v(" "),a("p",[t._v("Hive可以使用Apache Tez执行引擎而不是古老的Map-Reduce引擎。没有在环境中没有默认打开，在Hive查询开头将以下内容设置为‘true’来使用Tez：“设置hive.execution.engine = tez; ”，通过上述设置，你执行的每个HIVE查询都将利用Tez。目前Hive On Spark还处于试验阶段，慎用。")]),t._v(" "),a("h2",{attrs:{id:"_3-9-使用向量化查询"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-9-使用向量化查询"}},[t._v("#")]),t._v(" 3.9 使用向量化查询")]),t._v(" "),a("p",[t._v("向量化查询执行通过一次性批量执行1024行而不是每次单行执行，从而提供扫描、聚合、筛选器和连接等操作的性能。在Hive 0.13中引入，此功能显着提高了查询执行时间，并可通过两个参数设置轻松启用：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　设置hive.vectorized.execution.enabled "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n　　设置hive.vectorized.execution.reduce.enabled "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n")])])]),a("h2",{attrs:{id:"_3-10-设置cost-based-query-optimization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-10-设置cost-based-query-optimization"}},[t._v("#")]),t._v(" 3.10 设置cost based query optimization")]),t._v(" "),a("p",[t._v("Hive自0.14.0开始，加入了一项“Cost based Optimizer”来对HQL执行计划进行优化，这个功能通过“hive.cbo.enable”来开启。在Hive 1.1.0之后，这个feature是默认开启的，它可以自动优化HQL中多个JOIN的顺序，并选择合适的JOIN算法。")]),t._v(" "),a("p",[t._v("Hive在提供最终执行前，优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等。要使用基于成本的优化（也称为CBO），请在查询开始设置以下参数：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　设置hive.cbo.enable "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n　　设置hive.compute.query.using.stats "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n　　设置hive.stats.fetch.column.stats "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n　　设置hive.stats.fetch.partition.stats "),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v(" true;")]),t._v("\n")])])]),a("h2",{attrs:{id:"_3-11-模式选择"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-11-模式选择"}},[t._v("#")]),t._v(" 3.11 模式选择")]),t._v(" "),a("h3",{attrs:{id:"本地模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#本地模式"}},[t._v("#")]),t._v(" 本地模式")]),t._v(" "),a("p",[t._v("对于大多数情况，Hive可以通过本地模式在单台机器上处理所有任务。对于小数据，执行时间可以明显被缩短。通过set hive.exec.mode.local.auto = true（默认为false）设置为本地模式，本地模式涉及到三个参数：")]),t._v(" "),a("img",{staticClass:"zoom-custom-imgs",attrs:{src:t.$withBase("/assets/img/dw/hiveopt/sumopt11-4.png"),alt:"wxmp"}}),t._v(" "),a("p",[t._v("set hive.exec.mode.local.auto=true; 是打开hive自动判断是否启动本地模式的开关，但是只是打开这个参数不能保证启动本地模式，要当map任务数不超过hive.exec.mode.local.auto.input.files.max的个数并且map输入文件大小不超过hive.exec.mode.local.auto.inputbytes.max所指定的大小时，才能启动本地模式。")]),t._v(" "),a("p",[t._v("如下：用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("set hive.exec.mode.local.auto"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true;  //开启本地mr")]),t._v("\n//设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M\nset hive.exec.mode.local.auto.inputbytes.max"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("50000000;")]),t._v("\n//设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4\nset hive.exec.mode.local.auto.input.files.max"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("10;")]),t._v("\n")])])]),a("h3",{attrs:{id:"并行模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#并行模式"}},[t._v("#")]),t._v(" 并行模式")]),t._v(" "),a("p",[t._v("Hive会将一个查询转化成一个或多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。默认情况下，Hive一次只会执行一个阶段，由于job包含多个阶段，而这些阶段并非完全相互依赖，即：这些阶段可以并行执行，可以缩短整个job的执行时间。设置参数，set hive.exec.parallel=true,或者通过配置文件来完成：")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　hive> set hive.exec.parallel;\n")])])]),a("h3",{attrs:{id:"严格模式"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#严格模式"}},[t._v("#")]),t._v(" 严格模式")]),t._v(" "),a("p",[t._v("Hive提供一个严格模式，可以防止用户执行那些可能产生意想不到的影响查询，通过设置Hive.mapred.modestrict来完成。")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　set Hive.mapred.modestrict;\n")])])]),a("h2",{attrs:{id:"_3-12-jvm重用"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-12-jvm重用"}},[t._v("#")]),t._v(" 3.12 JVM重用")]),t._v(" "),a("p",[t._v("Hadoop通常是使用派生JVM来执行map和reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含成百上千的task任务的情况。JVM重用可以使得JVM示例在同一个job中时候，通过参数mapred.job.reuse.jvm.num.tasks来设置。")]),t._v(" "),a("h2",{attrs:{id:"_3-13-推测执行"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-13-推测执行"}},[t._v("#")]),t._v(" 3.13 推测执行")]),t._v(" "),a("p",[t._v("Hadoop推测执行可以触发执行一些重复的任务，尽管因对重复的数据进行计算而导致消耗更多的计算资源，不过这个功能的目标是通过加快获取单个task的结果以侦测执行慢的TaskTracker加入到没名单的方式来提高整体的任务执行效率。Hadoop的推测执行功能由2个配置控制着，通过mapred-site.xml中配置")]),t._v(" "),a("div",{staticClass:"language-ini extra-class"},[a("pre",{pre:!0,attrs:{class:"language-ini"}},[a("code",[t._v("　　mapred.map.tasks.speculative.execution"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true")]),t._v("\n　　mapred.reduce.tasks.speculative.execution"),a("span",{pre:!0,attrs:{class:"token attr-value"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("=")]),t._v("true")]),t._v("\n")])])]),a("h1",{attrs:{id:"_4-总结"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-总结"}},[t._v("#")]),t._v(" 4. 总结")]),t._v(" "),a("p",[t._v("【参考资料】")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/yu0_zhang0/article/details/81776459")]),t._v(" "),a("p",[t._v("https://www.cnblogs.com/smartloli/p/4356660.html")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/zdy0_2004/article/details/81613230")]),t._v(" "),a("p",[t._v("https://blog.51cto.com/12445535/2352789")]),t._v(" "),a("p",[t._v("https://blog.csdn.net/BabyFish13/article/details/52055927")]),t._v(" "),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),a("ul",[a("li",[t._v("https://www.cnblogs.com/swordfall/p/11037539.html")])])])}),[],!1,null,null,null);s.default=n.exports}}]);